[2025-04-03T02:55:45.555+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-03T02:55:45.601+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_dbt_spark_oracle.dbt_run scheduled__2025-04-02T00:00:00+00:00 [queued]>
[2025-04-03T02:55:45.620+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_dbt_spark_oracle.dbt_run scheduled__2025-04-02T00:00:00+00:00 [queued]>
[2025-04-03T02:55:45.621+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 1
[2025-04-03T02:55:45.835+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): dbt_run> on 2025-04-02 00:00:00+00:00
[2025-04-03T02:55:45.853+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-04-03T02:55:45.857+0000] {standard_task_runner.py:72} INFO - Started process 69 to run task
[2025-04-03T02:55:45.855+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'etl_dbt_spark_oracle', 'dbt_run', 'scheduled__2025-04-02T00:00:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/dbt-spark-oracle.py', '--cfg-path', '/tmp/tmpv78j210n']
[2025-04-03T02:55:45.861+0000] {standard_task_runner.py:105} INFO - Job 29: Subtask dbt_run
[2025-04-03T02:55:45.950+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_dbt_spark_oracle.dbt_run scheduled__2025-04-02T00:00:00+00:00 [running]> on host a72b9165479b
[2025-04-03T02:55:46.134+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_dbt_spark_oracle' AIRFLOW_CTX_TASK_ID='dbt_run' AIRFLOW_CTX_EXECUTION_DATE='2025-04-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-02T00:00:00+00:00'
[2025-04-03T02:55:46.138+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2025-04-03T02:55:46.139+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2025-04-03T02:55:46.140+0000] {logging_mixin.py:190} INFO - Current task name:dbt_run state:running start_date:2025-04-03 02:55:45.603208+00:00
[2025-04-03T02:55:46.141+0000] {logging_mixin.py:190} INFO - Dag name:etl_dbt_spark_oracle and current dag run status:running
[2025-04-03T02:55:46.142+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-04-03T02:55:46.145+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-04-03T02:55:46.148+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'cd /dbt/dbt-spark-project && /opt/spark/sbin/start-thriftserver.sh --master local[*] --hiveconf hive.metastore.warehouse.dir=/opt/spark/spark-warehouse --hiveconf javax.jdo.option.ConnectionURL="jdbc:derby:;databaseName=/opt/spark/metastore_db;create=true" && sleep 10 && dbt run']
[2025-04-03T02:55:46.175+0000] {subprocess.py:99} INFO - Output:
[2025-04-03T02:55:46.246+0000] {subprocess.py:106} INFO - /opt/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2025-04-03T02:55:46.281+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 169: ps: command not found
[2025-04-03T02:55:46.326+0000] {subprocess.py:106} INFO - starting org.apache.spark.sql.hive.thriftserver.HiveThriftServer2, logging to /opt/spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-a72b9165479b.out
[2025-04-03T02:55:46.329+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:46.834+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:47.340+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:47.849+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:48.355+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:48.862+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:49.372+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:49.891+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:50.405+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:50.910+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 143: ps: command not found
[2025-04-03T02:55:53.418+0000] {subprocess.py:106} INFO - /opt/spark/sbin/spark-daemon.sh: line 151: ps: command not found
[2025-04-03T02:55:53.421+0000] {subprocess.py:106} INFO - failed to launch: nice -n 0 bash /opt/spark/bin/spark-submit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server --master local[*] --hiveconf hive.metastore.warehouse.dir=/opt/spark/spark-warehouse --hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/opt/spark/metastore_db;create=true
[2025-04-03T02:55:53.432+0000] {subprocess.py:106} INFO -   /opt/spark/bin/load-spark-env.sh: line 68: ps: command not found
[2025-04-03T02:55:53.435+0000] {subprocess.py:106} INFO -   Spark Command: /opt/jdk-21/bin/java -cp /opt/spark/conf/:/opt/spark/jars/* -Xmx1g -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false org.apache.spark.deploy.SparkSubmit --master local[*] --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --name Thrift JDBC/ODBC Server spark-internal --hiveconf hive.metastore.warehouse.dir=/opt/spark/spark-warehouse --hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/opt/spark/metastore_db;create=true
[2025-04-03T02:55:53.438+0000] {subprocess.py:106} INFO -   ========================================
[2025-04-03T02:55:53.440+0000] {subprocess.py:106} INFO -   25/04/03 02:55:53 INFO HiveThriftServer2: Started daemon with process name: 89@a72b9165479b
[2025-04-03T02:55:53.442+0000] {subprocess.py:106} INFO -   25/04/03 02:55:53 INFO SignalUtils: Registering signal handler for TERM
[2025-04-03T02:55:53.444+0000] {subprocess.py:106} INFO -   25/04/03 02:55:53 INFO SignalUtils: Registering signal handler for HUP
[2025-04-03T02:55:53.447+0000] {subprocess.py:106} INFO -   25/04/03 02:55:53 INFO SignalUtils: Registering signal handler for INT
[2025-04-03T02:55:53.449+0000] {subprocess.py:106} INFO -   25/04/03 02:55:53 INFO HiveThriftServer2: Starting SparkContext
[2025-04-03T02:55:53.451+0000] {subprocess.py:106} INFO - full log in /opt/spark/logs/spark--org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-a72b9165479b.out
[2025-04-03T02:56:13.434+0000] {subprocess.py:106} INFO - [0m02:56:13  Running with dbt=1.9.3
[2025-04-03T02:56:13.547+0000] {subprocess.py:106} INFO - WARNING:thrift.transport.sslcompat:using legacy validation callback
[2025-04-03T02:56:14.454+0000] {subprocess.py:106} INFO - [0m02:56:14  Registered adapter: spark=1.9.2
[2025-04-03T02:56:14.999+0000] {subprocess.py:106} INFO - [0m02:56:14  [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
[2025-04-03T02:56:15.000+0000] {subprocess.py:106} INFO - There are 1 unused configuration paths:
[2025-04-03T02:56:15.001+0000] {subprocess.py:106} INFO - - models.dbt_spark_project.example
[2025-04-03T02:56:15.299+0000] {subprocess.py:106} INFO - [0m02:56:15  Found 6 models, 1 source, 473 macros
[2025-04-03T02:56:15.305+0000] {subprocess.py:106} INFO - [0m02:56:15
[2025-04-03T02:56:15.306+0000] {subprocess.py:106} INFO - [0m02:56:15  Concurrency: 1 threads (target='dev')
[2025-04-03T02:56:15.308+0000] {subprocess.py:106} INFO - [0m02:56:15
[2025-04-03T02:56:27.646+0000] {subprocess.py:106} INFO - [0m02:56:27  1 of 6 START sql incremental model default.test2 ............................... [RUN]
[2025-04-03T02:56:34.205+0000] {subprocess.py:106} INFO - [0m02:56:34  1 of 6 OK created sql incremental model default.test2 .......................... [[32mOK[0m in 6.55s]
[2025-04-03T02:56:34.212+0000] {subprocess.py:106} INFO - [0m02:56:34  2 of 6 START sql table model default.transformed_data .......................... [RUN]
[2025-04-03T02:56:37.063+0000] {subprocess.py:106} INFO - [0m02:56:37  2 of 6 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 2.84s]
[2025-04-03T02:56:37.070+0000] {subprocess.py:106} INFO - [0m02:56:37  3 of 6 START sql table model default.transformed_data2 ......................... [RUN]
[2025-04-03T02:56:41.117+0000] {subprocess.py:106} INFO - [0m02:56:41  3 of 6 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 4.04s]
[2025-04-03T02:56:41.125+0000] {subprocess.py:106} INFO - [0m02:56:41  4 of 6 START sql table model default.transformed_data3 ......................... [RUN]
[2025-04-03T02:56:43.416+0000] {subprocess.py:106} INFO - [0m02:56:43  4 of 6 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 2.28s]
[2025-04-03T02:56:43.423+0000] {subprocess.py:106} INFO - [0m02:56:43  5 of 6 START sql table model default.transformed_data4 ......................... [RUN]
[2025-04-03T02:56:45.368+0000] {subprocess.py:106} INFO - [0m02:56:45  5 of 6 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 1.93s]
[2025-04-03T02:56:45.374+0000] {subprocess.py:106} INFO - [0m02:56:45  6 of 6 START sql table model default.transformed_data5 ......................... [RUN]
[2025-04-03T02:56:46.785+0000] {subprocess.py:106} INFO - [0m02:56:46  6 of 6 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 1.41s]
[2025-04-03T02:56:46.946+0000] {subprocess.py:106} INFO - [0m02:56:46
[2025-04-03T02:56:46.948+0000] {subprocess.py:106} INFO - [0m02:56:46  Finished running 1 incremental model, 5 table models in 0 hours 0 minutes and 31.64 seconds (31.64s).
[2025-04-03T02:56:47.106+0000] {subprocess.py:106} INFO - [0m02:56:47
[2025-04-03T02:56:47.109+0000] {subprocess.py:106} INFO - [0m02:56:47  [32mCompleted successfully[0m
[2025-04-03T02:56:47.111+0000] {subprocess.py:106} INFO - [0m02:56:47
[2025-04-03T02:56:47.115+0000] {subprocess.py:106} INFO - [0m02:56:47  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[2025-04-03T02:56:49.574+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-04-03T02:56:49.658+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-04-03T02:56:49.660+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=etl_dbt_spark_oracle, task_id=dbt_run, run_id=scheduled__2025-04-02T00:00:00+00:00, execution_date=20250402T000000, start_date=20250403T025545, end_date=20250403T025649
[2025-04-03T02:56:49.748+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2025-04-03T02:56:49.750+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2025-04-03T02:56:49.753+0000] {logging_mixin.py:190} INFO - Dag name:etl_dbt_spark_oracle queued_at:2025-04-03 02:55:34.781359+00:00
[2025-04-03T02:56:49.754+0000] {logging_mixin.py:190} INFO - Task hostname:a72b9165479b operator:BashOperator
[2025-04-03T02:56:49.794+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-03T02:56:49.890+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-03T02:56:49.898+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
