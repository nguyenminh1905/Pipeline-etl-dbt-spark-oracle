[0m10:42:58.340147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8c2adb6600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8c2c6baa80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8c2b8b5c70>]}


============================== 10:42:58.344382 | 36e671e7-2158-4d71-bce3-019bb4885797 ==============================
[0m10:42:58.344382 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:42:58.345009 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:42:58.366042 [info ] [MainThread]: dbt version: 1.9.3
[0m10:42:58.366582 [info ] [MainThread]: python version: 3.12.3
[0m10:42:58.367067 [info ] [MainThread]: python path: /home/minh/Desktop/dbt-spark/spark_env/bin/python3
[0m10:42:58.367575 [info ] [MainThread]: os info: Linux-6.11.0-19-generic-x86_64-with-glibc2.39
[0m10:42:58.438428 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:42:58.439121 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:42:58.439599 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:42:58.472036 [info ] [MainThread]: Using profiles dir at /home/minh/.dbt
[0m10:42:58.472615 [info ] [MainThread]: Using profiles.yml file at /home/minh/.dbt/profiles.yml
[0m10:42:58.473100 [info ] [MainThread]: Using dbt_project.yml file at /home/minh/Desktop/dbt-spark/dbt_spark_project/dbt_project.yml
[0m10:42:58.587535 [info ] [MainThread]: Configuration:
[0m10:42:58.588152 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m10:42:58.588733 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:42:58.589273 [info ] [MainThread]: Required dependencies:
[0m10:42:58.589753 [debug] [MainThread]: Executing "git --help"
[0m10:42:58.592038 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:42:58.592488 [debug] [MainThread]: STDERR: "b''"
[0m10:42:58.592943 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:42:58.593437 [info ] [MainThread]: Connection test skipped since no profile was found
[0m10:42:58.593953 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:42:58.594502 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "dbt_spark_project", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m10:42:58.596705 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.357362, "process_in_blocks": "47016", "process_kernel_time": 0.158036, "process_mem_max_rss": "95180", "process_out_blocks": "16", "process_user_time": 1.52703}
[0m10:42:58.597399 [debug] [MainThread]: Command `dbt debug` failed at 10:42:58.597274 after 0.36 seconds
[0m10:42:58.597969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8c2dfb3e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8c2a46e0f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8c2ad43fb0>]}
[0m10:42:58.598909 [debug] [MainThread]: Flushing usage events
[0m10:43:00.066175 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:43:44.866510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70be5f8e9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70be5f4b44a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70be6124a6c0>]}


============================== 10:43:44.869716 | 78cc5a18-276f-4c9e-8f26-a49f0650858b ==============================
[0m10:43:44.869716 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:43:44.870442 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:43:44.878380 [info ] [MainThread]: dbt version: 1.9.3
[0m10:43:44.878943 [info ] [MainThread]: python version: 3.12.3
[0m10:43:44.879463 [info ] [MainThread]: python path: /home/minh/Desktop/dbt-spark/spark_env/bin/python3
[0m10:43:44.879991 [info ] [MainThread]: os info: Linux-6.11.0-19-generic-x86_64-with-glibc2.39
[0m10:43:44.958879 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:43:44.959378 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:43:44.959768 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:43:44.992115 [info ] [MainThread]: Using profiles dir at /home/minh/.dbt
[0m10:43:44.992716 [info ] [MainThread]: Using profiles.yml file at /home/minh/.dbt/profiles.yml
[0m10:43:44.993237 [info ] [MainThread]: Using dbt_project.yml file at /home/minh/Desktop/dbt-spark/dbt_spark_project/dbt_project.yml
[0m10:43:45.102290 [info ] [MainThread]: Configuration:
[0m10:43:45.102849 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m10:43:45.103436 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:43:45.103876 [info ] [MainThread]: Required dependencies:
[0m10:43:45.104365 [debug] [MainThread]: Executing "git --help"
[0m10:43:45.106417 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:43:45.106905 [debug] [MainThread]: STDERR: "b''"
[0m10:43:45.107334 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:43:45.107862 [info ] [MainThread]: Connection test skipped since no profile was found
[0m10:43:45.108383 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:43:45.108883 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Credentials in profile "dbt_spark_project", target "dev" invalid: Runtime Error
    thrift connection method requires additional dependencies. 
    Install the additional required dependencies with `pip install dbt-spark[PyHive]`


[0m10:43:45.110007 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.33830342, "process_in_blocks": "0", "process_kernel_time": 0.112747, "process_mem_max_rss": "94948", "process_out_blocks": "16", "process_user_time": 1.461724}
[0m10:43:45.110609 [debug] [MainThread]: Command `dbt debug` failed at 10:43:45.110486 after 0.34 seconds
[0m10:43:45.111144 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70be5fa349b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70be5efa1d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70be5ee565d0>]}
[0m10:43:45.111623 [debug] [MainThread]: Flushing usage events
[0m10:43:51.372161 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:45:19.601215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a9f2ad78f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a9f291eb800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a9f290ec140>]}


============================== 10:45:19.604503 | ff7e153d-2679-48e6-92d9-75d9ce1646cf ==============================
[0m10:45:19.604503 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:45:19.605226 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:45:19.613415 [info ] [MainThread]: dbt version: 1.9.3
[0m10:45:19.613949 [info ] [MainThread]: python version: 3.12.3
[0m10:45:19.614384 [info ] [MainThread]: python path: /home/minh/Desktop/dbt-spark/spark_env/bin/python3
[0m10:45:19.614861 [info ] [MainThread]: os info: Linux-6.11.0-19-generic-x86_64-with-glibc2.39
[0m10:45:19.716940 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:45:19.717504 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:45:19.717982 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:45:19.751562 [info ] [MainThread]: Using profiles dir at /home/minh/.dbt
[0m10:45:19.752176 [info ] [MainThread]: Using profiles.yml file at /home/minh/.dbt/profiles.yml
[0m10:45:19.752743 [info ] [MainThread]: Using dbt_project.yml file at /home/minh/Desktop/dbt-spark/dbt_spark_project/dbt_project.yml
[0m10:45:19.753251 [info ] [MainThread]: adapter type: spark
[0m10:45:19.753774 [info ] [MainThread]: adapter version: 1.9.2
[0m10:45:19.865597 [info ] [MainThread]: Configuration:
[0m10:45:19.866230 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:45:19.866745 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:45:19.867250 [info ] [MainThread]: Required dependencies:
[0m10:45:19.867751 [debug] [MainThread]: Executing "git --help"
[0m10:45:19.870121 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:45:19.870824 [debug] [MainThread]: STDERR: "b''"
[0m10:45:19.871316 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:45:19.871801 [info ] [MainThread]: Connection:
[0m10:45:19.872456 [info ] [MainThread]:   host: localhost
[0m10:45:19.873233 [info ] [MainThread]:   port: 10000
[0m10:45:19.873878 [info ] [MainThread]:   cluster: None
[0m10:45:19.874428 [info ] [MainThread]:   endpoint: None
[0m10:45:19.874979 [info ] [MainThread]:   schema: default
[0m10:45:19.875512 [info ] [MainThread]:   organization: 0
[0m10:45:19.876362 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:45:19.987662 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m10:45:19.988159 [debug] [MainThread]: Using spark connection "debug"
[0m10:45:19.988547 [debug] [MainThread]: On debug: select 1 as id
[0m10:45:19.988965 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:45:20.928668 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m10:45:20.929367 [debug] [MainThread]: SQL status: OK in 0.940 seconds
[0m10:45:20.930725 [debug] [MainThread]: On debug: Close
[0m10:45:20.938983 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m10:45:20.939599 [info ] [MainThread]: [32mAll checks passed![0m
[0m10:45:20.940669 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 1.4370447, "process_in_blocks": "648", "process_kernel_time": 0.19518, "process_mem_max_rss": "99484", "process_out_blocks": "16", "process_user_time": 1.707831}
[0m10:45:20.941239 [debug] [MainThread]: Command `dbt debug` succeeded at 10:45:20.941123 after 1.44 seconds
[0m10:45:20.941685 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:45:20.942152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a9f295d2c00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a9f28e485f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a9f28df0ce0>]}
[0m10:45:20.942654 [debug] [MainThread]: Flushing usage events
[0m10:45:22.298936 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:36:25.918610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724d697b9b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724d69671880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724d69048fb0>]}


============================== 13:36:25.925001 | df5072d8-5be4-4bc3-8164-6813f468fead ==============================
[0m13:36:25.925001 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:36:25.926288 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:36:25.942117 [info ] [MainThread]: dbt version: 1.9.3
[0m13:36:25.943488 [info ] [MainThread]: python version: 3.12.3
[0m13:36:25.944679 [info ] [MainThread]: python path: /home/minh/Desktop/dbt-spark/spark_env/bin/python3
[0m13:36:25.945860 [info ] [MainThread]: os info: Linux-6.11.0-19-generic-x86_64-with-glibc2.39
[0m13:36:26.084947 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:36:26.086023 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:36:26.087049 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:36:26.165154 [info ] [MainThread]: Using profiles dir at /home/minh/.dbt
[0m13:36:26.166124 [info ] [MainThread]: Using profiles.yml file at /home/minh/.dbt/profiles.yml
[0m13:36:26.166997 [info ] [MainThread]: Using dbt_project.yml file at /home/minh/Desktop/dbt-spark/dbt_spark_project/dbt_project.yml
[0m13:36:26.167851 [info ] [MainThread]: adapter type: spark
[0m13:36:26.168677 [info ] [MainThread]: adapter version: 1.9.2
[0m13:36:26.367237 [info ] [MainThread]: Configuration:
[0m13:36:26.368275 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m13:36:26.369464 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m13:36:26.370274 [info ] [MainThread]: Required dependencies:
[0m13:36:26.371087 [debug] [MainThread]: Executing "git --help"
[0m13:36:26.375255 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m13:36:26.376850 [debug] [MainThread]: STDERR: "b''"
[0m13:36:26.378107 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m13:36:26.379437 [info ] [MainThread]: Connection:
[0m13:36:26.380873 [info ] [MainThread]:   host: localhost
[0m13:36:26.382199 [info ] [MainThread]:   port: 10000
[0m13:36:26.383539 [info ] [MainThread]:   cluster: None
[0m13:36:26.384939 [info ] [MainThread]:   endpoint: None
[0m13:36:26.386318 [info ] [MainThread]:   schema: default
[0m13:36:26.387587 [info ] [MainThread]:   organization: 0
[0m13:36:26.389259 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:36:26.569821 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m13:36:26.570717 [debug] [MainThread]: Using spark connection "debug"
[0m13:36:26.571459 [debug] [MainThread]: On debug: select 1 as id
[0m13:36:26.572167 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:36:26.717229 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m13:36:26.718319 [debug] [MainThread]: SQL status: OK in 0.146 seconds
[0m13:36:26.720858 [debug] [MainThread]: On debug: Close
[0m13:36:26.734576 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m13:36:26.735679 [info ] [MainThread]: [32mAll checks passed![0m
[0m13:36:26.737662 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.96859163, "process_in_blocks": "600", "process_kernel_time": 0.30444, "process_mem_max_rss": "98948", "process_out_blocks": "24", "process_user_time": 2.937601}
[0m13:36:26.738964 [debug] [MainThread]: Command `dbt debug` succeeded at 13:36:26.738623 after 0.97 seconds
[0m13:36:26.740093 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m13:36:26.740998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724d69126c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724d6926cb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x724d68b62ea0>]}
[0m13:36:26.742031 [debug] [MainThread]: Flushing usage events
[0m13:36:28.219841 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:37:23.518261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb092aa20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb060f5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb092a840>]}


============================== 13:37:23.523888 | 03d781d5-d140-4f41-8168-2fb64d13f3f6 ==============================
[0m13:37:23.523888 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:37:23.525438 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:37:23.650541 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:37:23.651509 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:37:23.652372 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:37:23.920566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb20766f0>]}
[0m13:37:24.039173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb06d1df0>]}
[0m13:37:24.040358 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:37:24.234450 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:37:24.235736 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m13:37:24.236623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb0700bf0>]}
[0m13:37:26.219385 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m13:37:26.226422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb1071370>]}
[0m13:37:26.405094 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:37:26.408913 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:37:26.432715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccafa92b70>]}
[0m13:37:26.433739 [info ] [MainThread]: Found 1 model, 473 macros
[0m13:37:26.434568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccaeb822d0>]}
[0m13:37:26.437726 [info ] [MainThread]: 
[0m13:37:26.438580 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:37:26.439378 [info ] [MainThread]: 
[0m13:37:26.440774 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:37:26.442750 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:37:26.469081 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:37:26.470178 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:37:26.471021 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:37:26.639467 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:37:26.640373 [debug] [ThreadPool]: SQL status: OK in 0.169 seconds
[0m13:37:26.645419 [debug] [ThreadPool]: On list_schemas: Close
[0m13:37:26.655093 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m13:37:26.664507 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:37:26.665349 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m13:37:26.666098 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m13:37:26.666842 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:37:26.865167 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:37:26.866137 [debug] [ThreadPool]: SQL status: OK in 0.199 seconds
[0m13:37:26.872276 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m13:37:26.873302 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:37:26.874158 [debug] [ThreadPool]: On list_None_default: Close
[0m13:37:26.883633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccaeb75c70>]}
[0m13:37:26.885126 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:37:26.886058 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:37:26.891234 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m13:37:26.892483 [info ] [Thread-1 (]: 1 of 1 START sql table model default.transformed_data .......................... [RUN]
[0m13:37:26.893823 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m13:37:26.894841 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m13:37:26.909843 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m13:37:26.911289 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m13:37:26.974231 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m13:37:26.975305 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m13:37:26.976315 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:37:27.074227 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:37:27.075268 [debug] [Thread-1 (]: SQL status: OK in 0.099 seconds
[0m13:37:27.157320 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m13:37:27.158898 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:37:27.159812 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m13:37:27.160654 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m13:37:27.955170 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:37:27.956229 [debug] [Thread-1 (]: SQL status: OK in 0.795 seconds
[0m13:37:27.991537 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m13:37:27.992495 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:37:27.993336 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m13:37:28.005778 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '03d781d5-d140-4f41-8168-2fb64d13f3f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb109cb30>]}
[0m13:37:28.007632 [info ] [Thread-1 (]: 1 of 1 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.11s]
[0m13:37:28.008987 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m13:37:28.011071 [debug] [MainThread]: On master: ROLLBACK
[0m13:37:28.011907 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:37:28.071487 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:37:28.072373 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:37:28.073132 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:37:28.073867 [debug] [MainThread]: On master: ROLLBACK
[0m13:37:28.074581 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:37:28.075311 [debug] [MainThread]: On master: Close
[0m13:37:28.083545 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:37:28.084787 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data' was properly closed.
[0m13:37:28.086026 [info ] [MainThread]: 
[0m13:37:28.087310 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 1.65 seconds (1.65s).
[0m13:37:28.089388 [debug] [MainThread]: Command end result
[0m13:37:28.135425 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:37:28.139393 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:37:28.151008 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m13:37:28.152127 [info ] [MainThread]: 
[0m13:37:28.153392 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:37:28.154286 [info ] [MainThread]: 
[0m13:37:28.155319 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m13:37:28.157104 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.791122, "process_in_blocks": "2696", "process_kernel_time": 0.368858, "process_mem_max_rss": "107896", "process_out_blocks": "2832", "process_user_time": 5.530873}
[0m13:37:28.158304 [debug] [MainThread]: Command `dbt run` succeeded at 13:37:28.158014 after 4.79 seconds
[0m13:37:28.159549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb109da60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccb109cb30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ccaeaffd40>]}
[0m13:37:28.160852 [debug] [MainThread]: Flushing usage events
[0m13:37:29.860437 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:12:51.267668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78d3196ea8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78d319340590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78d319dfec90>]}


============================== 14:12:51.277410 | b121a428-3ea6-40b0-8209-15c8de4ad49f ==============================
[0m14:12:51.277410 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:12:51.279423 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:12:51.310494 [info ] [MainThread]: dbt version: 1.9.3
[0m14:12:51.312133 [info ] [MainThread]: python version: 3.12.3
[0m14:12:51.313666 [info ] [MainThread]: python path: /home/minh/Desktop/dbt-spark/spark_env/bin/python3
[0m14:12:51.315278 [info ] [MainThread]: os info: Linux-6.11.0-19-generic-x86_64-with-glibc2.39
[0m14:12:51.526784 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:12:51.527816 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:12:51.528690 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:12:51.662514 [info ] [MainThread]: Using profiles dir at /home/minh/.dbt
[0m14:12:51.664438 [info ] [MainThread]: Using profiles.yml file at /home/minh/.dbt/profiles.yml
[0m14:12:51.666220 [info ] [MainThread]: Using dbt_project.yml file at /home/minh/Desktop/dbt-spark/dbt_spark_project/dbt_project.yml
[0m14:12:51.668048 [info ] [MainThread]: adapter type: spark
[0m14:12:51.669906 [info ] [MainThread]: adapter version: 1.9.2
[0m14:12:51.942767 [info ] [MainThread]: Configuration:
[0m14:12:51.944186 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:12:51.945501 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:12:51.946783 [info ] [MainThread]: Required dependencies:
[0m14:12:51.948125 [debug] [MainThread]: Executing "git --help"
[0m14:12:51.953344 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:12:51.955019 [debug] [MainThread]: STDERR: "b''"
[0m14:12:51.956454 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:12:51.957899 [info ] [MainThread]: Connection:
[0m14:12:51.959399 [info ] [MainThread]:   host: localhost
[0m14:12:51.960793 [info ] [MainThread]:   port: 10000
[0m14:12:51.962181 [info ] [MainThread]:   cluster: None
[0m14:12:51.963632 [info ] [MainThread]:   endpoint: None
[0m14:12:51.965099 [info ] [MainThread]:   schema: default
[0m14:12:51.966491 [info ] [MainThread]:   organization: 0
[0m14:12:51.968222 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:12:52.234661 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m14:12:52.236285 [debug] [MainThread]: Using spark connection "debug"
[0m14:12:52.237720 [debug] [MainThread]: On debug: select 1 as id
[0m14:12:52.239006 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:12:52.241065 [error] [MainThread]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:12:52.243037 [debug] [MainThread]: Spark adapter: Error while running:
select 1 as id
[0m14:12:52.244487 [debug] [MainThread]: Spark adapter: Database Error
  failed to connect
[0m14:12:52.246104 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m14:12:52.247551 [info ] [MainThread]: [31m1 check failed:[0m
[0m14:12:52.249067 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    failed to connect

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m14:12:52.252750 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 1.2458049, "process_in_blocks": "51512", "process_kernel_time": 0.518209, "process_mem_max_rss": "98716", "process_out_blocks": "32", "process_user_time": 4.962124}
[0m14:12:52.254559 [debug] [MainThread]: Command `dbt debug` failed at 14:12:52.254166 after 1.25 seconds
[0m14:12:52.256001 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:12:52.257438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78d319180a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78d3189b65d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78d319096390>]}
[0m14:12:52.259038 [debug] [MainThread]: Flushing usage events
[0m14:12:53.908796 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:16:05.442639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c2464862240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c246453c9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c2464862930>]}


============================== 14:16:05.449481 | faa6898f-2ee1-4931-8d0f-86da0080c74a ==============================
[0m14:16:05.449481 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:16:05.450684 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m14:16:05.476670 [info ] [MainThread]: dbt version: 1.9.3
[0m14:16:05.477983 [info ] [MainThread]: python version: 3.12.3
[0m14:16:05.479238 [info ] [MainThread]: python path: /home/minh/Desktop/dbt-spark/spark_env/bin/python3
[0m14:16:05.480421 [info ] [MainThread]: os info: Linux-6.11.0-19-generic-x86_64-with-glibc2.39
[0m14:16:05.637878 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:16:05.638809 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:16:05.639605 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:16:05.704497 [info ] [MainThread]: Using profiles dir at /home/minh/.dbt
[0m14:16:05.705438 [info ] [MainThread]: Using profiles.yml file at /home/minh/.dbt/profiles.yml
[0m14:16:05.706215 [info ] [MainThread]: Using dbt_project.yml file at /home/minh/Desktop/dbt-spark/dbt_spark_project/dbt_project.yml
[0m14:16:05.707008 [info ] [MainThread]: adapter type: spark
[0m14:16:05.707842 [info ] [MainThread]: adapter version: 1.9.2
[0m14:16:05.907674 [info ] [MainThread]: Configuration:
[0m14:16:05.908588 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m14:16:05.909606 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m14:16:05.910486 [info ] [MainThread]: Required dependencies:
[0m14:16:05.911334 [debug] [MainThread]: Executing "git --help"
[0m14:16:05.917644 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m14:16:05.918616 [debug] [MainThread]: STDERR: "b''"
[0m14:16:05.919342 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m14:16:05.920241 [info ] [MainThread]: Connection:
[0m14:16:05.921174 [info ] [MainThread]:   host: localhost
[0m14:16:05.922033 [info ] [MainThread]:   port: 10000
[0m14:16:05.922870 [info ] [MainThread]:   cluster: None
[0m14:16:05.923739 [info ] [MainThread]:   endpoint: None
[0m14:16:05.924632 [info ] [MainThread]:   schema: default
[0m14:16:05.925603 [info ] [MainThread]:   organization: 0
[0m14:16:05.927061 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:16:06.091636 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m14:16:06.092568 [debug] [MainThread]: Using spark connection "debug"
[0m14:16:06.093348 [debug] [MainThread]: On debug: select 1 as id
[0m14:16:06.094118 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:16:09.806029 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m14:16:09.807651 [debug] [MainThread]: SQL status: OK in 3.713 seconds
[0m14:16:09.810193 [debug] [MainThread]: On debug: Close
[0m14:16:09.861675 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m14:16:09.862632 [info ] [MainThread]: [32mAll checks passed![0m
[0m14:16:09.864856 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 4.5773425, "process_in_blocks": "44232", "process_kernel_time": 0.391501, "process_mem_max_rss": "98808", "process_out_blocks": "32", "process_user_time": 3.080818}
[0m14:16:09.865874 [debug] [MainThread]: Command `dbt debug` succeeded at 14:16:09.865671 after 4.58 seconds
[0m14:16:09.866680 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m14:16:09.867507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c2464057e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c2463d99dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c2463e46de0>]}
[0m14:16:09.868403 [debug] [MainThread]: Flushing usage events
[0m14:16:11.743814 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:22:43.020192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b5eea5d7080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b5eea292ab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b5eebfcb4a0>]}


============================== 10:22:43.024122 | a2cbd4e0-a6e1-45b9-b619-d792699eaea7 ==============================
[0m10:22:43.024122 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:22:43.025182 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:22:43.037918 [info ] [MainThread]: dbt version: 1.9.3
[0m10:22:43.038996 [info ] [MainThread]: python version: 3.12.3
[0m10:22:43.040011 [info ] [MainThread]: python path: /home/minh/Desktop/dbt-spark/spark_env/bin/python3
[0m10:22:43.040989 [info ] [MainThread]: os info: Linux-6.11.0-19-generic-x86_64-with-glibc2.39
[0m10:22:43.148671 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:22:43.149337 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:22:43.149930 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:22:43.191814 [info ] [MainThread]: Using profiles dir at /home/minh/.dbt
[0m10:22:43.192447 [info ] [MainThread]: Using profiles.yml file at /home/minh/.dbt/profiles.yml
[0m10:22:43.192945 [info ] [MainThread]: Using dbt_project.yml file at /home/minh/Desktop/dbt-spark/dbt_spark_project/dbt_project.yml
[0m10:22:43.193425 [info ] [MainThread]: adapter type: spark
[0m10:22:43.193895 [info ] [MainThread]: adapter version: 1.9.2
[0m10:22:43.347735 [info ] [MainThread]: Configuration:
[0m10:22:43.348450 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m10:22:43.349141 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:22:43.349664 [info ] [MainThread]: Required dependencies:
[0m10:22:43.350231 [debug] [MainThread]: Executing "git --help"
[0m10:22:43.352497 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:22:43.353098 [debug] [MainThread]: STDERR: "b''"
[0m10:22:43.353642 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:22:43.354318 [info ] [MainThread]: Connection:
[0m10:22:43.355130 [info ] [MainThread]:   host: localhost
[0m10:22:43.355766 [info ] [MainThread]:   port: 10000
[0m10:22:43.356362 [info ] [MainThread]:   cluster: None
[0m10:22:43.357189 [info ] [MainThread]:   endpoint: None
[0m10:22:43.357754 [info ] [MainThread]:   schema: default
[0m10:22:43.358270 [info ] [MainThread]:   organization: 0
[0m10:22:43.358958 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:22:43.481911 [debug] [MainThread]: Acquiring new spark connection 'debug'
[0m10:22:43.482506 [debug] [MainThread]: Using spark connection "debug"
[0m10:22:43.482989 [debug] [MainThread]: On debug: select 1 as id
[0m10:22:43.483437 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:22:43.608185 [debug] [MainThread]: Spark adapter: Poll status: 2, query complete
[0m10:22:43.609264 [debug] [MainThread]: SQL status: OK in 0.126 seconds
[0m10:22:43.611842 [debug] [MainThread]: On debug: Close
[0m10:22:43.625431 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m10:22:43.626351 [info ] [MainThread]: [32mAll checks passed![0m
[0m10:22:43.628673 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.7052372, "process_in_blocks": "19032", "process_kernel_time": 0.233042, "process_mem_max_rss": "98544", "process_out_blocks": "16", "process_user_time": 2.056371}
[0m10:22:43.631080 [debug] [MainThread]: Command `dbt debug` succeeded at 10:22:43.630843 after 0.71 seconds
[0m10:22:43.631760 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m10:22:43.634799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b5eea35b530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b5eea965cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b5ee9fedfd0>]}
[0m10:22:43.635997 [debug] [MainThread]: Flushing usage events
[0m10:22:45.443978 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:06:44.899589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7925822bc4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7925822bcd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7925822bd9d0>]}


============================== 11:06:44.904858 | ce9a61a0-d6c0-4f65-81dc-0e6e8e60a4e3 ==============================
[0m11:06:44.904858 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:06:44.905816 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:06:45.037221 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:06:45.037820 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:06:45.038332 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:06:45.186124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ce9a61a0-d6c0-4f65-81dc-0e6e8e60a4e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x792581732a50>]}
[0m11:06:45.247515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ce9a61a0-d6c0-4f65-81dc-0e6e8e60a4e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x792581901280>]}
[0m11:06:45.248292 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:06:45.365009 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:06:45.438465 [error] [MainThread]: Encountered an error:
Parsing Error
  The schema file at models/schema.yml is invalid because a list element for 'models' does not have a name attribute.
[0m11:06:45.440145 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.64995086, "process_in_blocks": "79056", "process_kernel_time": 0.2553, "process_mem_max_rss": "101728", "process_out_blocks": "8", "process_user_time": 1.772023}
[0m11:06:45.440882 [debug] [MainThread]: Command `dbt run` failed at 11:06:45.440727 after 0.65 seconds
[0m11:06:45.441442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x792581238170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7925812382f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79258121b2f0>]}
[0m11:06:45.441988 [debug] [MainThread]: Flushing usage events
[0m11:06:46.865303 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:15:13.729024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791701cce750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79170153ae70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791701ccfa70>]}


============================== 11:15:13.732618 | d632114c-8cfb-4d75-b802-605d43bb6f31 ==============================
[0m11:15:13.732618 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:15:13.733639 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:15:13.831246 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:15:13.832065 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:15:13.832917 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:15:14.029904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd632114c-8cfb-4d75-b802-605d43bb6f31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791700ac54f0>]}
[0m11:15:14.104542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd632114c-8cfb-4d75-b802-605d43bb6f31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791700bcaff0>]}
[0m11:15:14.105534 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:15:14.225320 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:15:14.303255 [error] [MainThread]: Encountered an error:
Parsing Error
  The schema file at models/schema.yml is invalid because a list element for 'models' does not have a name attribute.
[0m11:15:14.304721 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.70036495, "process_in_blocks": "0", "process_kernel_time": 0.198571, "process_mem_max_rss": "102060", "process_out_blocks": "8", "process_user_time": 1.867966}
[0m11:15:14.305257 [debug] [MainThread]: Command `dbt run` failed at 11:15:14.305110 after 0.70 seconds
[0m11:15:14.306213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x791701788fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79170052c4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79170052c0e0>]}
[0m11:15:14.307036 [debug] [MainThread]: Flushing usage events
[0m11:15:16.617816 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:17:00.886265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c6577b1af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c6577d5010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c65761f0b0>]}


============================== 11:17:00.890498 | 0941fc3b-7ac0-49c7-99e0-762f6095ef9c ==============================
[0m11:17:00.890498 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:17:00.891663 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:17:00.991910 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:17:00.992551 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:17:00.993265 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:17:01.138493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0941fc3b-7ac0-49c7-99e0-762f6095ef9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c658d32810>]}
[0m11:17:01.203363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0941fc3b-7ac0-49c7-99e0-762f6095ef9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c656ee1dc0>]}
[0m11:17:01.204249 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:17:01.321643 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:17:01.443222 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 4 files added, 1 files changed.
[0m11:17:01.443934 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/transformed_data3.sql
[0m11:17:01.444456 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/transformed_data4.sql
[0m11:17:01.444937 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/transformed_data5.sql
[0m11:17:01.445488 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/transformed_data2.sql
[0m11:17:01.446079 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/schema.yml
[0m11:17:01.780080 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:17:01.787114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0941fc3b-7ac0-49c7-99e0-762f6095ef9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c656383da0>]}
[0m11:17:01.879520 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:17:01.882258 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:17:01.926115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0941fc3b-7ac0-49c7-99e0-762f6095ef9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c656448290>]}
[0m11:17:01.927763 [info ] [MainThread]: Found 5 models, 473 macros
[0m11:17:01.929139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0941fc3b-7ac0-49c7-99e0-762f6095ef9c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c65697a060>]}
[0m11:17:01.933226 [info ] [MainThread]: 
[0m11:17:01.934310 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:17:01.935567 [info ] [MainThread]: 
[0m11:17:01.937381 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:17:01.952437 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:17:01.974490 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:17:01.975651 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:17:01.976339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:17:01.977567 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m11:17:01.978888 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:17:01.979979 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m11:17:01.980903 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m11:17:01.981478 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m11:17:01.982536 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:17:01.983100 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m11:17:01.983950 [info ] [MainThread]: 
[0m11:17:01.985028 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.05 seconds (0.05s).
[0m11:17:01.986357 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m11:17:01.988100 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.21712, "process_in_blocks": "2752", "process_kernel_time": 0.216181, "process_mem_max_rss": "107568", "process_out_blocks": "1920", "process_user_time": 2.516472}
[0m11:17:01.988777 [debug] [MainThread]: Command `dbt run` failed at 11:17:01.988633 after 1.22 seconds
[0m11:17:01.989297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c65aad3e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c6571aecf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c656371af0>]}
[0m11:17:01.989924 [debug] [MainThread]: Flushing usage events
[0m11:17:03.793027 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:17:36.586600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79d120140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79dc77b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79db2fad0>]}


============================== 11:17:36.590767 | 523d53a8-5d44-4276-bcd0-cfe4185d80f5 ==============================
[0m11:17:36.590767 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:17:36.592166 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:17:36.714725 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:17:36.719809 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:17:36.720897 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:17:36.925807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79cc2c2c0>]}
[0m11:17:36.997525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79ce11400>]}
[0m11:17:36.998346 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:17:37.112199 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:17:37.205656 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:17:37.206251 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:17:37.214207 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:17:37.250575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79d01fc20>]}
[0m11:17:37.323360 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:17:37.326133 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:17:37.340922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79c8052e0>]}
[0m11:17:37.342004 [info ] [MainThread]: Found 5 models, 473 macros
[0m11:17:37.342794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79ca1edb0>]}
[0m11:17:37.345068 [info ] [MainThread]: 
[0m11:17:37.346027 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:17:37.346810 [info ] [MainThread]: 
[0m11:17:37.348055 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:17:37.359438 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:17:37.388929 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:17:37.389960 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:17:37.391012 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:17:40.300397 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:17:40.301102 [debug] [ThreadPool]: SQL status: OK in 2.910 seconds
[0m11:17:40.374814 [debug] [ThreadPool]: On list_schemas: Close
[0m11:17:40.404946 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:17:40.411756 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:40.412338 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:17:40.412902 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:17:40.413405 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:17:40.991603 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:17:40.992809 [debug] [ThreadPool]: SQL status: OK in 0.579 seconds
[0m11:17:41.003474 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:17:41.004178 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:17:41.004848 [debug] [ThreadPool]: On list_None_default: Close
[0m11:17:41.019712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79c8d3f80>]}
[0m11:17:41.020566 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:41.021177 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:17:41.026636 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m11:17:41.027578 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m11:17:41.028491 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m11:17:41.029176 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m11:17:41.039522 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m11:17:41.040961 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m11:17:41.090128 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m11:17:41.091054 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m11:17:41.091905 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:17:41.801207 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:41.801988 [debug] [Thread-1 (]: SQL status: OK in 0.710 seconds
[0m11:17:41.852558 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m11:17:41.853874 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:41.854662 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m11:17:41.855385 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m11:17:44.519712 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:44.520495 [debug] [Thread-1 (]: SQL status: OK in 2.664 seconds
[0m11:17:44.546917 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m11:17:44.547648 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:17:44.548286 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m11:17:44.559750 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79ec718e0>]}
[0m11:17:44.560703 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.53s]
[0m11:17:44.561699 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m11:17:44.562364 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m11:17:44.563214 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m11:17:44.564156 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m11:17:44.564785 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m11:17:44.568834 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m11:17:44.570122 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m11:17:44.578177 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m11:17:44.578932 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m11:17:44.579654 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:17:44.699458 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:44.700321 [debug] [Thread-1 (]: SQL status: OK in 0.121 seconds
[0m11:17:44.704701 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m11:17:44.705663 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:44.706359 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m11:17:44.707009 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m11:17:45.215120 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:45.215838 [debug] [Thread-1 (]: SQL status: OK in 0.508 seconds
[0m11:17:45.218535 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m11:17:45.219038 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:17:45.219630 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m11:17:45.231489 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79c432ea0>]}
[0m11:17:45.233219 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.67s]
[0m11:17:45.235136 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m11:17:45.236306 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m11:17:45.237642 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m11:17:45.239417 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m11:17:45.241017 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m11:17:45.250258 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m11:17:45.251611 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m11:17:45.262654 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m11:17:45.263827 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m11:17:45.264860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:17:45.372330 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:45.373092 [debug] [Thread-1 (]: SQL status: OK in 0.108 seconds
[0m11:17:45.377097 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m11:17:45.378059 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:45.378731 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m11:17:45.379255 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m11:17:45.899511 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:45.900231 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m11:17:45.902704 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m11:17:45.903241 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:17:45.903775 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m11:17:45.910933 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79a280a70>]}
[0m11:17:45.911941 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.67s]
[0m11:17:45.913014 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m11:17:45.913679 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m11:17:45.914427 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m11:17:45.915296 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m11:17:45.915915 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m11:17:45.919378 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m11:17:45.920325 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m11:17:45.926452 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m11:17:45.927316 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m11:17:45.928378 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:17:46.041706 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:46.042546 [debug] [Thread-1 (]: SQL status: OK in 0.114 seconds
[0m11:17:46.047163 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m11:17:46.048123 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:46.048772 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m11:17:46.049387 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintuple_value
FROM raw_data
  
[0m11:17:46.573603 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:46.574460 [debug] [Thread-1 (]: SQL status: OK in 0.524 seconds
[0m11:17:46.577691 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m11:17:46.578452 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:17:46.579099 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m11:17:46.588559 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79a282630>]}
[0m11:17:46.590202 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.67s]
[0m11:17:46.591678 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m11:17:46.592789 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m11:17:46.593957 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m11:17:46.594881 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m11:17:46.595670 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m11:17:46.599903 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m11:17:46.601106 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m11:17:46.609040 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m11:17:46.610278 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m11:17:46.611605 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:17:46.754594 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:46.755306 [debug] [Thread-1 (]: SQL status: OK in 0.144 seconds
[0m11:17:46.759789 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m11:17:46.760802 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:46.761505 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m11:17:46.762154 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextuple_value
FROM raw_data
  
[0m11:17:47.357566 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:17:47.358752 [debug] [Thread-1 (]: SQL status: OK in 0.596 seconds
[0m11:17:47.363751 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m11:17:47.365001 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:17:47.366238 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m11:17:47.376010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '523d53a8-5d44-4276-bcd0-cfe4185d80f5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79a2bc380>]}
[0m11:17:47.377683 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.78s]
[0m11:17:47.379239 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m11:17:47.381537 [debug] [MainThread]: On master: ROLLBACK
[0m11:17:47.382232 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:17:47.462232 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:17:47.463311 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:17:47.464228 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:17:47.465149 [debug] [MainThread]: On master: ROLLBACK
[0m11:17:47.466257 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:17:47.467339 [debug] [MainThread]: On master: Close
[0m11:17:47.477806 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:17:47.478829 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m11:17:47.479825 [info ] [MainThread]: 
[0m11:17:47.480731 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 10.13 seconds (10.13s).
[0m11:17:47.483581 [debug] [MainThread]: Command end result
[0m11:17:47.543582 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:17:47.547083 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:17:47.560791 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:17:47.561818 [info ] [MainThread]: 
[0m11:17:47.562940 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:17:47.564086 [info ] [MainThread]: 
[0m11:17:47.565195 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m11:17:47.570273 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.109943, "process_in_blocks": "208", "process_kernel_time": 0.27406, "process_mem_max_rss": "104740", "process_out_blocks": "2048", "process_user_time": 3.247861}
[0m11:17:47.571499 [debug] [MainThread]: Command `dbt run` succeeded at 11:17:47.571231 after 11.11 seconds
[0m11:17:47.572481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79d0dd010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79f163ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77e79d2483e0>]}
[0m11:17:47.573589 [debug] [MainThread]: Flushing usage events
[0m11:17:49.217789 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:22:28.753986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d78443170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d77fa4530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d78eeaab0>]}


============================== 11:22:28.757409 | 67eda0ad-5da2-4f61-bdd8-8acfe7b442c9 ==============================
[0m11:22:28.757409 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:22:28.758676 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:22:28.896080 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:22:28.896841 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:22:28.897346 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:22:29.042228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d77a4f0e0>]}
[0m11:22:29.101839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d7b1709b0>]}
[0m11:22:29.102645 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:22:29.219006 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:22:29.306849 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:22:29.307640 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/transformed_data4.sql
[0m11:22:29.308187 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/transformed_data5.sql
[0m11:22:29.625138 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:22:29.631429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d7770ab10>]}
[0m11:22:29.721290 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:22:29.723862 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:22:29.737504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d7748aff0>]}
[0m11:22:29.738379 [info ] [MainThread]: Found 5 models, 473 macros
[0m11:22:29.739252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d771300e0>]}
[0m11:22:29.742080 [info ] [MainThread]: 
[0m11:22:29.742830 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:22:29.743452 [info ] [MainThread]: 
[0m11:22:29.744300 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:22:29.755925 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:22:29.778315 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:22:29.778950 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:22:29.779472 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:22:29.884658 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:22:29.885783 [debug] [ThreadPool]: SQL status: OK in 0.106 seconds
[0m11:22:29.892135 [debug] [ThreadPool]: On list_schemas: Close
[0m11:22:29.904661 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:22:29.916297 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:29.917415 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:22:29.918397 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:22:29.919313 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:22:30.139998 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:22:30.140784 [debug] [ThreadPool]: SQL status: OK in 0.221 seconds
[0m11:22:30.145492 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:22:30.146111 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:22:30.146729 [debug] [ThreadPool]: On list_None_default: Close
[0m11:22:30.156481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d77a95f10>]}
[0m11:22:30.157213 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:30.157791 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:22:30.161054 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m11:22:30.161941 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m11:22:30.162789 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m11:22:30.163358 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m11:22:30.172611 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m11:22:30.173736 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m11:22:30.215934 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m11:22:30.216642 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m11:22:30.217279 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:22:30.428455 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:30.429265 [debug] [Thread-1 (]: SQL status: OK in 0.212 seconds
[0m11:22:30.480192 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m11:22:30.481280 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:30.481888 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m11:22:30.482428 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m11:22:30.990532 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:30.991272 [debug] [Thread-1 (]: SQL status: OK in 0.508 seconds
[0m11:22:31.018140 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m11:22:31.018754 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:22:31.019208 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m11:22:31.027538 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d75ec3ad0>]}
[0m11:22:31.028527 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 0.86s]
[0m11:22:31.029588 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m11:22:31.030096 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m11:22:31.030807 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m11:22:31.031514 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m11:22:31.032046 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m11:22:31.035052 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m11:22:31.036164 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m11:22:31.042055 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m11:22:31.043058 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m11:22:31.044023 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:22:31.250045 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:31.251269 [debug] [Thread-1 (]: SQL status: OK in 0.207 seconds
[0m11:22:31.258545 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m11:22:31.260013 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:31.260998 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m11:22:31.262003 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m11:22:31.790717 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:31.791597 [debug] [Thread-1 (]: SQL status: OK in 0.529 seconds
[0m11:22:31.796087 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m11:22:31.796946 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:22:31.797758 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m11:22:31.806937 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d75e6d220>]}
[0m11:22:31.808682 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.78s]
[0m11:22:31.811128 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m11:22:31.812420 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m11:22:31.813881 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m11:22:31.815462 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m11:22:31.816711 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m11:22:31.826311 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m11:22:31.827892 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m11:22:31.844795 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m11:22:31.846000 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m11:22:31.847121 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:22:32.084043 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:32.085270 [debug] [Thread-1 (]: SQL status: OK in 0.238 seconds
[0m11:22:32.092057 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m11:22:32.093559 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:32.094490 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m11:22:32.095488 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m11:22:32.548035 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:32.548828 [debug] [Thread-1 (]: SQL status: OK in 0.452 seconds
[0m11:22:32.551897 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m11:22:32.552560 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:22:32.553056 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m11:22:32.559248 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d75e6d250>]}
[0m11:22:32.560407 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.74s]
[0m11:22:32.561360 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m11:22:32.562035 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m11:22:32.562843 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m11:22:32.563623 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m11:22:32.564179 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m11:22:32.567718 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m11:22:32.568635 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m11:22:32.579688 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m11:22:32.580793 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m11:22:32.581772 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:22:32.760551 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:32.761390 [debug] [Thread-1 (]: SQL status: OK in 0.180 seconds
[0m11:22:32.765826 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m11:22:32.767130 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:32.767985 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m11:22:32.768797 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m11:22:33.165611 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:33.166364 [debug] [Thread-1 (]: SQL status: OK in 0.397 seconds
[0m11:22:33.169704 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m11:22:33.170343 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:22:33.170957 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m11:22:33.180585 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d75e6d1c0>]}
[0m11:22:33.181787 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.62s]
[0m11:22:33.183059 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m11:22:33.183772 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m11:22:33.184563 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m11:22:33.185398 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m11:22:33.185988 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m11:22:33.191195 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m11:22:33.192255 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m11:22:33.197317 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m11:22:33.198425 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m11:22:33.199355 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:22:33.376815 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:33.377998 [debug] [Thread-1 (]: SQL status: OK in 0.179 seconds
[0m11:22:33.385040 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m11:22:33.386623 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:33.387649 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m11:22:33.388587 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m11:22:33.856993 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:22:33.858186 [debug] [Thread-1 (]: SQL status: OK in 0.469 seconds
[0m11:22:33.861315 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m11:22:33.861976 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:22:33.862571 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m11:22:33.869293 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67eda0ad-5da2-4f61-bdd8-8acfe7b442c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d744fb9b0>]}
[0m11:22:33.870304 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.68s]
[0m11:22:33.871343 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m11:22:33.872970 [debug] [MainThread]: On master: ROLLBACK
[0m11:22:33.873738 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:22:33.932072 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:22:33.933163 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:33.934124 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:22:33.935019 [debug] [MainThread]: On master: ROLLBACK
[0m11:22:33.935944 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:22:33.936786 [debug] [MainThread]: On master: Close
[0m11:22:33.943580 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:22:33.944419 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m11:22:33.945576 [info ] [MainThread]: 
[0m11:22:33.946544 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 4.20 seconds (4.20s).
[0m11:22:33.949177 [debug] [MainThread]: Command end result
[0m11:22:34.000063 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:22:34.004161 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:22:34.016958 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:22:34.017796 [info ] [MainThread]: 
[0m11:22:34.018780 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:22:34.019672 [info ] [MainThread]: 
[0m11:22:34.020711 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m11:22:34.022320 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.371819, "process_in_blocks": "5512", "process_kernel_time": 0.246449, "process_mem_max_rss": "109456", "process_out_blocks": "2984", "process_user_time": 2.717932}
[0m11:22:34.023452 [debug] [MainThread]: Command `dbt run` succeeded at 11:22:34.023202 after 5.37 seconds
[0m11:22:34.024288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d7851da00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d7704afc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x760d77048d70>]}
[0m11:22:34.025592 [debug] [MainThread]: Flushing usage events
[0m11:22:36.657935 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:51:16.441846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b9fb9fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b9e3acf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89bbabf170>]}


============================== 13:51:16.447586 | 7f98c4ea-c3f7-4377-b583-9396185fb143 ==============================
[0m13:51:16.447586 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:51:16.448391 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:51:16.539877 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:51:16.540476 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:51:16.540965 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:51:16.731994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b9f87fb0>]}
[0m13:51:16.799960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89bdf8ba70>]}
[0m13:51:16.800690 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:51:16.884697 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:51:16.999548 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:51:17.000006 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:51:17.007368 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m13:51:17.030810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b9be5160>]}
[0m13:51:17.094206 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:51:17.100087 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:51:17.115611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b8e0d280>]}
[0m13:51:17.116215 [info ] [MainThread]: Found 5 models, 473 macros
[0m13:51:17.116628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b90baf60>]}
[0m13:51:17.119212 [info ] [MainThread]: 
[0m13:51:17.119811 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:51:17.120210 [info ] [MainThread]: 
[0m13:51:17.120803 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:51:17.128019 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:51:17.141868 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:51:17.142471 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:51:17.142960 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:51:17.572661 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:51:17.573547 [debug] [ThreadPool]: SQL status: OK in 0.431 seconds
[0m13:51:17.582208 [debug] [ThreadPool]: On list_schemas: Close
[0m13:51:17.597882 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m13:51:17.605469 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:17.606095 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m13:51:17.606669 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m13:51:17.607212 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:51:17.950442 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:51:17.951126 [debug] [ThreadPool]: SQL status: OK in 0.344 seconds
[0m13:51:17.956663 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m13:51:17.957266 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:51:17.958000 [debug] [ThreadPool]: On list_None_default: Close
[0m13:51:17.970171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b9b13800>]}
[0m13:51:17.971256 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:17.971749 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:51:17.974331 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m13:51:17.974906 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m13:51:17.975391 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m13:51:17.975817 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m13:51:17.982739 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m13:51:17.984302 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m13:51:18.004184 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m13:51:18.004667 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m13:51:18.005055 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:18.317819 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:18.318496 [debug] [Thread-1 (]: SQL status: OK in 0.313 seconds
[0m13:51:18.369730 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m13:51:18.371248 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:18.371807 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m13:51:18.372288 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m13:51:19.384803 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:19.385749 [debug] [Thread-1 (]: SQL status: OK in 1.013 seconds
[0m13:51:19.430517 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m13:51:19.431381 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:51:19.432151 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m13:51:19.444232 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89bb612d80>]}
[0m13:51:19.445530 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.47s]
[0m13:51:19.446848 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m13:51:19.447881 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m13:51:19.449143 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m13:51:19.450342 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m13:51:19.451273 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m13:51:19.456133 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m13:51:19.457579 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m13:51:19.466249 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m13:51:19.467233 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m13:51:19.468145 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:19.807596 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:19.808608 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m13:51:19.814473 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m13:51:19.815776 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:19.816665 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m13:51:19.817545 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m13:51:20.624902 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:20.625715 [debug] [Thread-1 (]: SQL status: OK in 0.807 seconds
[0m13:51:20.629514 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m13:51:20.630141 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:51:20.630676 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m13:51:20.643269 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b8eced20>]}
[0m13:51:20.644262 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 1.19s]
[0m13:51:20.645198 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m13:51:20.645975 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m13:51:20.647217 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m13:51:20.648039 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m13:51:20.648670 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m13:51:20.651784 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m13:51:20.652596 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m13:51:20.657612 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m13:51:20.658202 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m13:51:20.658702 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:20.986805 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:20.987754 [debug] [Thread-1 (]: SQL status: OK in 0.329 seconds
[0m13:51:20.993445 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m13:51:20.994715 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:20.995602 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m13:51:20.996490 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m13:51:21.433610 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:21.434261 [debug] [Thread-1 (]: SQL status: OK in 0.437 seconds
[0m13:51:21.438949 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m13:51:21.439484 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:51:21.439926 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m13:51:21.445935 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b8587530>]}
[0m13:51:21.447043 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.80s]
[0m13:51:21.447957 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m13:51:21.448575 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m13:51:21.449480 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m13:51:21.450543 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m13:51:21.451281 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m13:51:21.455356 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m13:51:21.456517 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m13:51:21.463523 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m13:51:21.464432 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m13:51:21.465207 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:21.619293 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:21.619952 [debug] [Thread-1 (]: SQL status: OK in 0.155 seconds
[0m13:51:21.623516 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m13:51:21.624607 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:21.625227 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m13:51:21.625823 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m13:51:22.027535 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:22.028537 [debug] [Thread-1 (]: SQL status: OK in 0.402 seconds
[0m13:51:22.032958 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m13:51:22.033827 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:51:22.034633 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m13:51:22.042489 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b8587230>]}
[0m13:51:22.043961 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.59s]
[0m13:51:22.045362 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m13:51:22.046444 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m13:51:22.048006 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m13:51:22.049253 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m13:51:22.050244 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m13:51:22.055354 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m13:51:22.056652 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m13:51:22.064700 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m13:51:22.065691 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m13:51:22.066614 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:51:22.242192 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:22.243269 [debug] [Thread-1 (]: SQL status: OK in 0.177 seconds
[0m13:51:22.249291 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m13:51:22.250599 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:22.251343 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m13:51:22.252156 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m13:51:22.687519 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:51:22.688564 [debug] [Thread-1 (]: SQL status: OK in 0.435 seconds
[0m13:51:22.693519 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m13:51:22.694381 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:51:22.695201 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m13:51:22.705394 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f98c4ea-c3f7-4377-b583-9396185fb143', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b8ece510>]}
[0m13:51:22.706787 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.66s]
[0m13:51:22.708149 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m13:51:22.710138 [debug] [MainThread]: On master: ROLLBACK
[0m13:51:22.711012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:51:22.766021 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:51:22.766636 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:51:22.767141 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:51:22.767758 [debug] [MainThread]: On master: ROLLBACK
[0m13:51:22.768363 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:51:22.768875 [debug] [MainThread]: On master: Close
[0m13:51:22.776501 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:51:22.777029 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m13:51:22.777692 [info ] [MainThread]: 
[0m13:51:22.778291 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 5.66 seconds (5.66s).
[0m13:51:22.780792 [debug] [MainThread]: Command end result
[0m13:51:22.803723 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:51:22.805171 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:51:22.810584 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m13:51:22.810976 [info ] [MainThread]: 
[0m13:51:22.811411 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:51:22.811788 [info ] [MainThread]: 
[0m13:51:22.812200 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m13:51:22.813524 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.454411, "process_in_blocks": "49784", "process_kernel_time": 0.234679, "process_mem_max_rss": "108544", "process_out_blocks": "2048", "process_user_time": 2.517286}
[0m13:51:22.814151 [debug] [MainThread]: Command `dbt run` succeeded at 13:51:22.813998 after 6.46 seconds
[0m13:51:22.814706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b9e96780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b994f950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e89b9af6c30>]}
[0m13:51:22.815331 [debug] [MainThread]: Flushing usage events
[0m13:51:24.373178 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:30.417420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748eda858fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748edab14320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748eda0fc6b0>]}


============================== 14:25:30.421623 | a226b9aa-8c18-4096-8b64-83a3a7458ccf ==============================
[0m14:25:30.421623 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:25:30.422511 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:25:30.510034 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:25:30.510562 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:25:30.510950 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:25:30.695323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a226b9aa-8c18-4096-8b64-83a3a7458ccf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748eda9999d0>]}
[0m14:25:30.760157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a226b9aa-8c18-4096-8b64-83a3a7458ccf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748ed987bf50>]}
[0m14:25:30.760879 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:25:30.838521 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:25:30.955619 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:30.956084 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:30.962935 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:25:30.985297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a226b9aa-8c18-4096-8b64-83a3a7458ccf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748ed981dca0>]}
[0m14:25:31.044089 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:25:31.047210 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:25:31.068759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a226b9aa-8c18-4096-8b64-83a3a7458ccf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748ed93a1760>]}
[0m14:25:31.069233 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:25:31.069606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a226b9aa-8c18-4096-8b64-83a3a7458ccf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748ed95d28a0>]}
[0m14:25:31.071383 [info ] [MainThread]: 
[0m14:25:31.071782 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:31.072122 [info ] [MainThread]: 
[0m14:25:31.072612 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:25:31.078954 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:25:31.091435 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:25:31.091872 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:31.092206 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:31.092827 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:25:31.093339 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:31.093722 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:25:31.094159 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:25:31.094503 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:25:31.095285 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:31.095679 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:25:31.096078 [info ] [MainThread]: 
[0m14:25:31.096553 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:25:31.097117 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:25:31.098184 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.740217, "process_in_blocks": "53680", "process_kernel_time": 0.221161, "process_mem_max_rss": "106688", "process_out_blocks": "984", "process_user_time": 2.132633}
[0m14:25:31.099087 [debug] [MainThread]: Command `dbt run` failed at 14:25:31.098923 after 0.74 seconds
[0m14:25:31.099624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748ed9eceea0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748edd72c110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x748ed95acb00>]}
[0m14:25:31.100158 [debug] [MainThread]: Flushing usage events
[0m14:25:32.920211 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:37.725661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b87b4f4d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b87b678350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b879c30f80>]}


============================== 14:25:37.730543 | 8a4e90bd-1a19-48e5-a93f-f2bb0bbcbdcb ==============================
[0m14:25:37.730543 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:25:37.731394 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:25:37.800021 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:25:37.800664 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:25:37.801181 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:25:37.991032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a4e90bd-1a19-48e5-a93f-f2bb0bbcbdcb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b87a198200>]}
[0m14:25:38.073553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a4e90bd-1a19-48e5-a93f-f2bb0bbcbdcb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b87972db20>]}
[0m14:25:38.074343 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:25:38.220981 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:25:38.374810 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:38.375566 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:38.385774 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:25:38.416718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a4e90bd-1a19-48e5-a93f-f2bb0bbcbdcb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b879646690>]}
[0m14:25:38.491319 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:25:38.494506 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:25:38.508271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a4e90bd-1a19-48e5-a93f-f2bb0bbcbdcb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b878aa8c20>]}
[0m14:25:38.508992 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:25:38.509572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a4e90bd-1a19-48e5-a93f-f2bb0bbcbdcb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b878dd1970>]}
[0m14:25:38.512219 [info ] [MainThread]: 
[0m14:25:38.512909 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:38.513434 [info ] [MainThread]: 
[0m14:25:38.514235 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:25:38.522522 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:25:38.537741 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:25:38.538315 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:38.538772 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:38.539581 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:25:38.540243 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:38.540705 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:25:38.541279 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:25:38.541830 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:25:38.542896 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:38.543483 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:25:38.544191 [info ] [MainThread]: 
[0m14:25:38.544757 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:25:38.545670 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:25:38.546979 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8852294, "process_in_blocks": "0", "process_kernel_time": 0.152734, "process_mem_max_rss": "106972", "process_out_blocks": "976", "process_user_time": 2.168228}
[0m14:25:38.547714 [debug] [MainThread]: Command `dbt run` failed at 14:25:38.547554 after 0.89 seconds
[0m14:25:38.548266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b87d9b3b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b878dd1af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78b878bd3fb0>]}
[0m14:25:38.548826 [debug] [MainThread]: Flushing usage events
[0m14:25:39.846978 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:44.648823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a757a453d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a757a45c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a757a45370>]}


============================== 14:25:44.651727 | 1ee6359f-c95b-480f-a30c-6d0fef3de157 ==============================
[0m14:25:44.651727 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:25:44.652319 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:25:44.706070 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:25:44.706551 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:25:44.706953 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:25:44.852823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ee6359f-c95b-480f-a30c-6d0fef3de157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a758018830>]}
[0m14:25:44.915471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ee6359f-c95b-480f-a30c-6d0fef3de157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a756d64230>]}
[0m14:25:44.916285 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:25:44.989348 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:25:45.071832 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:45.072251 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:45.079381 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:25:45.099821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ee6359f-c95b-480f-a30c-6d0fef3de157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a756be7e60>]}
[0m14:25:45.155871 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:25:45.158970 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:25:45.170435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ee6359f-c95b-480f-a30c-6d0fef3de157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a756baee10>]}
[0m14:25:45.170940 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:25:45.171320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ee6359f-c95b-480f-a30c-6d0fef3de157', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a756d66cc0>]}
[0m14:25:45.173272 [info ] [MainThread]: 
[0m14:25:45.173729 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:45.174243 [info ] [MainThread]: 
[0m14:25:45.174835 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:25:45.181860 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:25:45.192657 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:25:45.193080 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:45.193407 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:45.194084 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:25:45.194639 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:45.195050 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:25:45.195564 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:25:45.195985 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:25:45.197267 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:45.197741 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:25:45.198182 [info ] [MainThread]: 
[0m14:25:45.198666 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:25:45.199345 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:25:45.200346 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6069805, "process_in_blocks": "0", "process_kernel_time": 0.162822, "process_mem_max_rss": "107136", "process_out_blocks": "984", "process_user_time": 1.794004}
[0m14:25:45.200923 [debug] [MainThread]: Command `dbt run` failed at 14:25:45.200797 after 0.61 seconds
[0m14:25:45.201394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a757f4d970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a7569220c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72a756e4aa20>]}
[0m14:25:45.201895 [debug] [MainThread]: Flushing usage events
[0m14:25:46.401647 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:51.686630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744c6b7d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744ca63b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744bea2360>]}


============================== 14:25:51.689851 | ef26fef7-3f1b-4afe-bd63-2256c051c1c3 ==============================
[0m14:25:51.689851 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:25:51.690558 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m14:25:51.751195 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:25:51.751708 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:25:51.752118 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:25:51.919249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ef26fef7-3f1b-4afe-bd63-2256c051c1c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744d8d8140>]}
[0m14:25:51.993550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ef26fef7-3f1b-4afe-bd63-2256c051c1c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744b8f96d0>]}
[0m14:25:51.994531 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:25:52.082199 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:25:52.177207 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:52.177745 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:52.184767 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:25:52.204755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef26fef7-3f1b-4afe-bd63-2256c051c1c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744b4fede0>]}
[0m14:25:52.261942 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:25:52.264650 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:25:52.275019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef26fef7-3f1b-4afe-bd63-2256c051c1c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744b2e81d0>]}
[0m14:25:52.275526 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:25:52.275949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef26fef7-3f1b-4afe-bd63-2256c051c1c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744b37bc50>]}
[0m14:25:52.278431 [info ] [MainThread]: 
[0m14:25:52.278936 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:52.279280 [info ] [MainThread]: 
[0m14:25:52.279811 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:25:52.286117 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:25:52.299084 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:25:52.299604 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:52.300059 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:52.300841 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:25:52.301633 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:52.302128 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:25:52.302646 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:25:52.303103 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:25:52.304017 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:52.304398 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:25:52.304785 [info ] [MainThread]: 
[0m14:25:52.305174 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:25:52.305708 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:25:52.306636 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6845268, "process_in_blocks": "0", "process_kernel_time": 0.181209, "process_mem_max_rss": "106852", "process_out_blocks": "992", "process_user_time": 2.298966}
[0m14:25:52.307247 [debug] [MainThread]: Command `dbt run` failed at 14:25:52.307119 after 0.69 seconds
[0m14:25:52.307722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744c9146b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744b555c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d744b1be270>]}
[0m14:25:52.308132 [debug] [MainThread]: Flushing usage events
[0m14:25:53.671676 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:58.780337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711f67a390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711f20d2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7121070a10>]}


============================== 14:25:58.783214 | afab71ac-cab4-44d2-b46c-db5f96974c5f ==============================
[0m14:25:58.783214 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:25:58.783770 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:25:58.835593 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:25:58.836144 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:25:58.836542 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:25:58.977768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'afab71ac-cab4-44d2-b46c-db5f96974c5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711fbe0080>]}
[0m14:25:59.038002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'afab71ac-cab4-44d2-b46c-db5f96974c5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711f175a60>]}
[0m14:25:59.038734 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:25:59.109029 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:25:59.191312 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:59.191789 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:59.200151 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:25:59.220752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'afab71ac-cab4-44d2-b46c-db5f96974c5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711e95f740>]}
[0m14:25:59.276496 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:25:59.279352 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:25:59.289987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'afab71ac-cab4-44d2-b46c-db5f96974c5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711e4f0d10>]}
[0m14:25:59.290557 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:25:59.290979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'afab71ac-cab4-44d2-b46c-db5f96974c5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711fd33dd0>]}
[0m14:25:59.293046 [info ] [MainThread]: 
[0m14:25:59.293562 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:59.293956 [info ] [MainThread]: 
[0m14:25:59.294566 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:25:59.300970 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:25:59.312771 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:25:59.313180 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:59.313559 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:59.314164 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:25:59.314678 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:25:59.315019 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:25:59.315483 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:25:59.315840 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:25:59.316651 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:59.316967 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:25:59.317281 [info ] [MainThread]: 
[0m14:25:59.317657 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:25:59.318170 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:25:59.319007 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5946448, "process_in_blocks": "0", "process_kernel_time": 0.154669, "process_mem_max_rss": "107100", "process_out_blocks": "976", "process_user_time": 1.863013}
[0m14:25:59.319509 [debug] [MainThread]: Command `dbt run` failed at 14:25:59.319382 after 0.60 seconds
[0m14:25:59.319987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711f8e50a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711e732ab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f711e5626c0>]}
[0m14:25:59.320447 [debug] [MainThread]: Flushing usage events
[0m14:26:00.736887 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:06.166046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54d02d1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54ced6420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54cc5f320>]}


============================== 14:26:06.168920 | 1757d714-a645-4797-8943-65771dcf677c ==============================
[0m14:26:06.168920 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:06.169522 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:26:06.222820 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:06.223371 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:06.223817 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:06.382474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1757d714-a645-4797-8943-65771dcf677c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54c312fc0>]}
[0m14:26:06.470271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1757d714-a645-4797-8943-65771dcf677c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54c6f67e0>]}
[0m14:26:06.471058 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:06.552667 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:06.640823 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:06.641394 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:06.649180 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:06.670206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1757d714-a645-4797-8943-65771dcf677c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54c1d1490>]}
[0m14:26:06.730476 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:06.732961 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:06.743616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1757d714-a645-4797-8943-65771dcf677c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54bea4cb0>]}
[0m14:26:06.744285 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:06.744802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1757d714-a645-4797-8943-65771dcf677c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54be98350>]}
[0m14:26:06.746828 [info ] [MainThread]: 
[0m14:26:06.747265 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:06.747702 [info ] [MainThread]: 
[0m14:26:06.748315 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:06.754403 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:06.765917 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:06.766333 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:06.766711 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:06.767379 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:06.767923 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:06.768281 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:06.768807 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:06.769132 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:06.769952 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:06.770278 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:06.770678 [info ] [MainThread]: 
[0m14:26:06.771070 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:26:06.771713 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:06.772587 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6614087, "process_in_blocks": "0", "process_kernel_time": 0.172612, "process_mem_max_rss": "106928", "process_out_blocks": "984", "process_user_time": 1.929664}
[0m14:26:06.773059 [debug] [MainThread]: Command `dbt run` failed at 14:26:06.772958 after 0.66 seconds
[0m14:26:06.773445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54ce07bc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54bea4b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73d54cb29c70>]}
[0m14:26:06.773910 [debug] [MainThread]: Flushing usage events
[0m14:26:08.007663 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:13.198019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da106f39df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da107150740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da10705b980>]}


============================== 14:26:13.201178 | c73618fe-d698-4fa5-ac8e-8390d262186e ==============================
[0m14:26:13.201178 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:13.201898 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:26:13.276609 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:13.277561 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:13.278364 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:13.471472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c73618fe-d698-4fa5-ac8e-8390d262186e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da108570140>]}
[0m14:26:13.565398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c73618fe-d698-4fa5-ac8e-8390d262186e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da1063acf50>]}
[0m14:26:13.566257 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:13.664633 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:13.777555 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:13.778121 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:13.786626 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:13.814459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c73618fe-d698-4fa5-ac8e-8390d262186e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da10606d490>]}
[0m14:26:13.890431 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:13.894259 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:13.908038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c73618fe-d698-4fa5-ac8e-8390d262186e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da105f80410>]}
[0m14:26:13.908879 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:13.909521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c73618fe-d698-4fa5-ac8e-8390d262186e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da105e68fb0>]}
[0m14:26:13.912168 [info ] [MainThread]: 
[0m14:26:13.912770 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:13.913256 [info ] [MainThread]: 
[0m14:26:13.913979 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:13.922226 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:13.937424 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:13.938038 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:13.938520 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:13.939354 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:13.940093 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:13.940840 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:13.941969 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:13.942847 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:13.944100 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:13.944929 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:13.945677 [info ] [MainThread]: 
[0m14:26:13.946478 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:26:13.947575 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:13.949287 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8131472, "process_in_blocks": "0", "process_kernel_time": 0.169783, "process_mem_max_rss": "107144", "process_out_blocks": "984", "process_user_time": 2.22217}
[0m14:26:13.950324 [debug] [MainThread]: Command `dbt run` failed at 14:26:13.950111 after 0.81 seconds
[0m14:26:13.951131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da106c5d790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da105f82ff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7da106013c80>]}
[0m14:26:13.951879 [debug] [MainThread]: Flushing usage events
[0m14:26:15.174195 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:20.181624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b03544e30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b02f1d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b03151c40>]}


============================== 14:26:20.184509 | 61beb64c-7fe4-4d96-98ba-885f236c16e9 ==============================
[0m14:26:20.184509 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:20.185314 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:26:20.239880 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:20.240566 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:20.241294 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:20.388632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '61beb64c-7fe4-4d96-98ba-885f236c16e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b0453c080>]}
[0m14:26:20.453295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '61beb64c-7fe4-4d96-98ba-885f236c16e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b02847f20>]}
[0m14:26:20.453992 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:20.531482 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:20.615692 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:20.616086 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:20.623143 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:20.645295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '61beb64c-7fe4-4d96-98ba-885f236c16e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b01f99b80>]}
[0m14:26:20.702340 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:20.705269 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:20.717243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '61beb64c-7fe4-4d96-98ba-885f236c16e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b01e88d10>]}
[0m14:26:20.717789 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:20.718230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '61beb64c-7fe4-4d96-98ba-885f236c16e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b0212e660>]}
[0m14:26:20.720303 [info ] [MainThread]: 
[0m14:26:20.720849 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:20.721340 [info ] [MainThread]: 
[0m14:26:20.722120 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:20.729115 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:20.740542 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:20.741267 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:20.741705 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:20.742403 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:20.742943 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:20.743320 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:20.743803 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:20.744126 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:20.745009 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:20.745328 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:20.745697 [info ] [MainThread]: 
[0m14:26:20.746172 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:26:20.746756 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:20.747815 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.62446046, "process_in_blocks": "0", "process_kernel_time": 0.148651, "process_mem_max_rss": "107188", "process_out_blocks": "976", "process_user_time": 1.902533}
[0m14:26:20.748352 [debug] [MainThread]: Command `dbt run` failed at 14:26:20.748220 after 0.63 seconds
[0m14:26:20.748803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b04967620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b028f5c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x723b01e88f50>]}
[0m14:26:20.749201 [debug] [MainThread]: Flushing usage events
[0m14:26:22.036602 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:28.018030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5bfba810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5b90c680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5bfba750>]}


============================== 14:26:28.021189 | c856af7f-0bfe-44e3-8878-e653fe41a0ab ==============================
[0m14:26:28.021189 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:28.021934 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:26:28.076943 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:28.077444 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:28.077881 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:28.229970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c856af7f-0bfe-44e3-8878-e653fe41a0ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5bde19d0>]}
[0m14:26:28.295323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c856af7f-0bfe-44e3-8878-e653fe41a0ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5af12b40>]}
[0m14:26:28.295994 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:28.367496 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:28.454259 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:28.454741 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:28.463747 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:28.486303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c856af7f-0bfe-44e3-8878-e653fe41a0ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5ab9e330>]}
[0m14:26:28.551949 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:28.554503 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:28.567165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c856af7f-0bfe-44e3-8878-e653fe41a0ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5ab9c590>]}
[0m14:26:28.567709 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:28.568105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c856af7f-0bfe-44e3-8878-e653fe41a0ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5a9e5fd0>]}
[0m14:26:28.570002 [info ] [MainThread]: 
[0m14:26:28.570447 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:28.570836 [info ] [MainThread]: 
[0m14:26:28.571440 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:28.579478 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:28.591439 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:28.592105 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:28.592666 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:28.593697 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:28.594456 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:28.594934 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:28.595433 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:28.595827 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:28.596621 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:28.596970 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:28.597330 [info ] [MainThread]: 
[0m14:26:28.597729 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:26:28.598243 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:28.599148 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6366924, "process_in_blocks": "0", "process_kernel_time": 0.18978, "process_mem_max_rss": "106888", "process_out_blocks": "1000", "process_user_time": 2.367257}
[0m14:26:28.599626 [debug] [MainThread]: Command `dbt run` failed at 14:26:28.599517 after 0.64 seconds
[0m14:26:28.600015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5bb9a840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5aa21070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x755d5aa23980>]}
[0m14:26:28.600437 [debug] [MainThread]: Flushing usage events
[0m14:26:29.817359 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:34.873799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adc5a7ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adc69f710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adc161d00>]}


============================== 14:26:34.876730 | c74db17e-9f7e-48eb-9f69-ee06f664ad8a ==============================
[0m14:26:34.876730 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:34.877404 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:26:34.931546 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:34.932077 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:34.932466 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:35.084706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c74db17e-9f7e-48eb-9f69-ee06f664ad8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adc01fdd0>]}
[0m14:26:35.150876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c74db17e-9f7e-48eb-9f69-ee06f664ad8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adde78e00>]}
[0m14:26:35.151580 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:35.225079 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:35.308628 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:35.309071 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:35.315810 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:35.336443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c74db17e-9f7e-48eb-9f69-ee06f664ad8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adb799880>]}
[0m14:26:35.393480 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:35.395954 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:35.406918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c74db17e-9f7e-48eb-9f69-ee06f664ad8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adb6e9c40>]}
[0m14:26:35.407472 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:35.407940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c74db17e-9f7e-48eb-9f69-ee06f664ad8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adb57b7a0>]}
[0m14:26:35.410027 [info ] [MainThread]: 
[0m14:26:35.410507 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:35.410985 [info ] [MainThread]: 
[0m14:26:35.411640 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:35.417728 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:35.430046 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:35.430458 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:35.430823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:35.431480 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:35.432011 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:35.432378 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:35.432880 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:35.433325 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:35.434137 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:35.434463 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:35.434790 [info ] [MainThread]: 
[0m14:26:35.435212 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:26:35.435803 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:35.436779 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.62150365, "process_in_blocks": "0", "process_kernel_time": 0.155695, "process_mem_max_rss": "107076", "process_out_blocks": "968", "process_user_time": 1.911266}
[0m14:26:35.437307 [debug] [MainThread]: Command `dbt run` failed at 14:26:35.437187 after 0.62 seconds
[0m14:26:35.437769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adc5db3b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adb798050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x799adb58fd70>]}
[0m14:26:35.438233 [debug] [MainThread]: Flushing usage events
[0m14:26:36.984885 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:42.287008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da6be41730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da6a1ef9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da6a1eca10>]}


============================== 14:26:42.290309 | af62d3a4-48b6-4837-9fae-0e797f0206fb ==============================
[0m14:26:42.290309 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:42.290992 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:26:42.345908 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:42.346365 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:42.346737 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:42.492980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af62d3a4-48b6-4837-9fae-0e797f0206fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da6a598650>]}
[0m14:26:42.555569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af62d3a4-48b6-4837-9fae-0e797f0206fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da6a576690>]}
[0m14:26:42.556284 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:42.626850 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:42.714471 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:42.715165 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:42.728795 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:42.758069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af62d3a4-48b6-4837-9fae-0e797f0206fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da690e3ef0>]}
[0m14:26:42.830469 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:42.833590 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:42.846030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af62d3a4-48b6-4837-9fae-0e797f0206fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da69088d70>]}
[0m14:26:42.846581 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:42.847024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af62d3a4-48b6-4837-9fae-0e797f0206fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da6919a8a0>]}
[0m14:26:42.849399 [info ] [MainThread]: 
[0m14:26:42.850000 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:42.850434 [info ] [MainThread]: 
[0m14:26:42.851096 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:42.859158 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:42.874084 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:42.874670 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:42.875111 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:42.875845 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:42.876623 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:42.877195 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:42.877793 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:42.878237 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:42.879145 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:42.879494 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:42.879866 [info ] [MainThread]: 
[0m14:26:42.880269 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:26:42.881019 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:42.882358 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.65946215, "process_in_blocks": "0", "process_kernel_time": 0.160375, "process_mem_max_rss": "106468", "process_out_blocks": "976", "process_user_time": 1.984277}
[0m14:26:42.883190 [debug] [MainThread]: Command `dbt run` failed at 14:26:42.883041 after 0.66 seconds
[0m14:26:42.883737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da6be41730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da69bc7a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72da69187fb0>]}
[0m14:26:42.884253 [debug] [MainThread]: Flushing usage events
[0m14:26:44.155511 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:49.559370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adbda68d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adc509fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adc4b0ec0>]}


============================== 14:26:49.565518 | 58c573e9-1bf2-453d-9abc-0c22f8af75f1 ==============================
[0m14:26:49.565518 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:49.566913 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:26:49.682199 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:49.683152 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:49.684013 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:49.927645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '58c573e9-1bf2-453d-9abc-0c22f8af75f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adc3884a0>]}
[0m14:26:50.017697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '58c573e9-1bf2-453d-9abc-0c22f8af75f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adb804c50>]}
[0m14:26:50.018483 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:50.113982 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:50.225128 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:50.225725 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:50.234433 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:50.263894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '58c573e9-1bf2-453d-9abc-0c22f8af75f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adb16af60>]}
[0m14:26:50.344227 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:50.347802 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:50.363192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '58c573e9-1bf2-453d-9abc-0c22f8af75f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adae64b60>]}
[0m14:26:50.363844 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:50.364444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '58c573e9-1bf2-453d-9abc-0c22f8af75f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adaf95040>]}
[0m14:26:50.366992 [info ] [MainThread]: 
[0m14:26:50.367783 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:50.368464 [info ] [MainThread]: 
[0m14:26:50.369465 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:50.379195 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:50.396833 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:50.397430 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:50.397949 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:50.398807 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:50.399512 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:50.399999 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:50.400784 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:50.401421 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:50.402589 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:50.403115 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:50.403636 [info ] [MainThread]: 
[0m14:26:50.404300 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:26:50.405218 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:50.406547 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9319177, "process_in_blocks": "0", "process_kernel_time": 0.172846, "process_mem_max_rss": "106900", "process_out_blocks": "984", "process_user_time": 2.446833}
[0m14:26:50.407166 [debug] [MainThread]: Command `dbt run` failed at 14:26:50.407027 after 0.93 seconds
[0m14:26:50.407676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adbe0a690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adaf95c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x726adae64e30>]}
[0m14:26:50.408190 [debug] [MainThread]: Flushing usage events
[0m14:26:51.628167 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:26:57.113675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7959297c2b70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x795929e5b8f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7959297c1100>]}


============================== 14:26:57.116601 | cc5d0786-d64e-46cb-9087-55a9dc432179 ==============================
[0m14:26:57.116601 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:26:57.117201 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:26:57.183129 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:26:57.183651 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:26:57.184055 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:26:57.422789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cc5d0786-d64e-46cb-9087-55a9dc432179', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x795929d8dbb0>]}
[0m14:26:57.512149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cc5d0786-d64e-46cb-9087-55a9dc432179', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x795928ef27b0>]}
[0m14:26:57.513107 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:26:57.636602 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:26:57.797965 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:57.798458 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:57.809362 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:26:57.836170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cc5d0786-d64e-46cb-9087-55a9dc432179', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7959292c49b0>]}
[0m14:26:57.909093 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:26:57.911956 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:26:57.925449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cc5d0786-d64e-46cb-9087-55a9dc432179', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7959288e3ec0>]}
[0m14:26:57.926031 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:26:57.926466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cc5d0786-d64e-46cb-9087-55a9dc432179', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7959287b5fd0>]}
[0m14:26:57.928776 [info ] [MainThread]: 
[0m14:26:57.929337 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:26:57.929765 [info ] [MainThread]: 
[0m14:26:57.930508 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:26:57.939085 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:26:57.953793 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:26:57.954569 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:57.955279 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:26:57.956414 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:26:57.957409 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:26:57.957985 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:26:57.958582 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:26:57.959029 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:26:57.959907 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:26:57.960329 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:26:57.960789 [info ] [MainThread]: 
[0m14:26:57.961256 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:26:57.961874 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:26:57.962888 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.91441625, "process_in_blocks": "0", "process_kernel_time": 0.20348, "process_mem_max_rss": "106876", "process_out_blocks": "968", "process_user_time": 2.271203}
[0m14:26:57.963612 [debug] [MainThread]: Command `dbt run` failed at 14:26:57.963425 after 0.92 seconds
[0m14:26:57.964157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x795929361a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x795928b4dc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x795928b4dcd0>]}
[0m14:26:57.964665 [debug] [MainThread]: Flushing usage events
[0m14:26:59.206658 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:05.346955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07ef64860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07ed58410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07f57c380>]}


============================== 14:27:05.352466 | 186ccac1-9155-4f24-9e33-12f148fe974b ==============================
[0m14:27:05.352466 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:27:05.353576 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:27:05.433136 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:27:05.434420 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:27:05.435717 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:27:05.640653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '186ccac1-9155-4f24-9e33-12f148fe974b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07ea9e960>]}
[0m14:27:05.720163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '186ccac1-9155-4f24-9e33-12f148fe974b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07eb85a90>]}
[0m14:27:05.721022 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:27:05.809489 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:27:05.913696 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:27:05.914267 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:27:05.923599 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:27:05.945435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '186ccac1-9155-4f24-9e33-12f148fe974b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07e13b9e0>]}
[0m14:27:06.005515 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:27:06.008142 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:27:06.019202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '186ccac1-9155-4f24-9e33-12f148fe974b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07e168620>]}
[0m14:27:06.019764 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:27:06.020200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '186ccac1-9155-4f24-9e33-12f148fe974b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07e21d040>]}
[0m14:27:06.022586 [info ] [MainThread]: 
[0m14:27:06.023147 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:27:06.023563 [info ] [MainThread]: 
[0m14:27:06.024137 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:27:06.030439 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:27:06.041634 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:27:06.042053 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:06.042390 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:06.043018 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:27:06.043636 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:06.044112 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:27:06.044724 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:27:06.045158 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:27:06.046396 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:06.046789 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:27:06.047142 [info ] [MainThread]: 
[0m14:27:06.047568 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:27:06.048163 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:27:06.049196 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.77744377, "process_in_blocks": "0", "process_kernel_time": 0.20561, "process_mem_max_rss": "107140", "process_out_blocks": "976", "process_user_time": 2.434383}
[0m14:27:06.049692 [debug] [MainThread]: Command `dbt run` failed at 14:27:06.049580 after 0.78 seconds
[0m14:27:06.050108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07eff3020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07dff2de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70b07e723cb0>]}
[0m14:27:06.050522 [debug] [MainThread]: Flushing usage events
[0m14:27:07.295773 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:13.144800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea534ebe30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea539e5640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea535b8aa0>]}


============================== 14:27:13.148144 | 07ab333d-83df-4d3e-94a0-4293320f22db ==============================
[0m14:27:13.148144 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:27:13.149324 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:27:13.219113 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:27:13.219774 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:27:13.220389 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:27:13.434095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07ab333d-83df-4d3e-94a0-4293320f22db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea54bd0350>]}
[0m14:27:13.512101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07ab333d-83df-4d3e-94a0-4293320f22db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea52edf590>]}
[0m14:27:13.512864 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:27:13.610568 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:27:13.720024 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:27:13.720478 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:27:13.728938 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:27:13.754338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07ab333d-83df-4d3e-94a0-4293320f22db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea526f07a0>]}
[0m14:27:13.831134 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:27:13.835337 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:27:13.851961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07ab333d-83df-4d3e-94a0-4293320f22db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea523c47a0>]}
[0m14:27:13.852721 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:27:13.853529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07ab333d-83df-4d3e-94a0-4293320f22db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea530a3860>]}
[0m14:27:13.857640 [info ] [MainThread]: 
[0m14:27:13.858515 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:27:13.859250 [info ] [MainThread]: 
[0m14:27:13.860074 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:27:13.869013 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:27:13.888495 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:27:13.889396 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:13.890824 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:13.892052 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:27:13.892865 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:13.893408 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:27:13.894103 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:27:13.894727 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:27:13.896574 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:13.897211 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:27:13.897945 [info ] [MainThread]: 
[0m14:27:13.898664 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.04 seconds (0.04s).
[0m14:27:13.899999 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:27:13.901817 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8263073, "process_in_blocks": "0", "process_kernel_time": 0.206406, "process_mem_max_rss": "107000", "process_out_blocks": "976", "process_user_time": 2.400093}
[0m14:27:13.902572 [debug] [MainThread]: Command `dbt run` failed at 14:27:13.902377 after 0.83 seconds
[0m14:27:13.903295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea539e5640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea523c4a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x72ea53085a30>]}
[0m14:27:13.904046 [debug] [MainThread]: Flushing usage events
[0m14:27:15.180187 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:27:20.679206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cdb55f8f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cdc622630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cdb3395b0>]}


============================== 14:27:20.682828 | e098026c-12ac-4afc-b005-dbcc114d016e ==============================
[0m14:27:20.682828 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:27:20.683581 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:27:20.756047 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:27:20.757068 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:27:20.757712 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:27:20.938825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e098026c-12ac-4afc-b005-dbcc114d016e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cdb8a0380>]}
[0m14:27:21.006110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e098026c-12ac-4afc-b005-dbcc114d016e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cdaa020f0>]}
[0m14:27:21.007239 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:27:21.085479 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:27:21.177422 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:27:21.177882 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:27:21.184408 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:27:21.206049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e098026c-12ac-4afc-b005-dbcc114d016e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cda3ede20>]}
[0m14:27:21.264843 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:27:21.267688 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:27:21.278856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e098026c-12ac-4afc-b005-dbcc114d016e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cda390bc0>]}
[0m14:27:21.279352 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:27:21.279840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e098026c-12ac-4afc-b005-dbcc114d016e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cda4ce090>]}
[0m14:27:21.282040 [info ] [MainThread]: 
[0m14:27:21.282542 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:27:21.283010 [info ] [MainThread]: 
[0m14:27:21.283657 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:27:21.290621 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:27:21.302341 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:27:21.302790 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:21.303118 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:27:21.303779 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:27:21.304356 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:27:21.304765 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:27:21.305330 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:27:21.305804 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:27:21.306844 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:27:21.307302 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:27:21.307685 [info ] [MainThread]: 
[0m14:27:21.308148 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:27:21.308808 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:27:21.309936 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6952533, "process_in_blocks": "0", "process_kernel_time": 0.174592, "process_mem_max_rss": "106864", "process_out_blocks": "968", "process_user_time": 2.283672}
[0m14:27:21.310492 [debug] [MainThread]: Command `dbt run` failed at 14:27:21.310385 after 0.70 seconds
[0m14:27:21.310928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cdb8fc8c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cdef30140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b2cda6bff50>]}
[0m14:27:21.311337 [debug] [MainThread]: Flushing usage events
[0m14:27:22.553490 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:08.318664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa86d67b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa870b2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa8c7ecc0>]}


============================== 14:33:08.323117 | 2396b466-4665-4c65-a10f-d6f51ded056e ==============================
[0m14:33:08.323117 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:08.323733 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:33:08.378951 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:08.379471 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:08.379898 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:08.528644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2396b466-4665-4c65-a10f-d6f51ded056e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa8ca0770>]}
[0m14:33:08.591746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2396b466-4665-4c65-a10f-d6f51ded056e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa79f4740>]}
[0m14:33:08.592373 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:08.665948 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:08.751356 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:08.751797 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:08.758333 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:08.779719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2396b466-4665-4c65-a10f-d6f51ded056e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa79d8230>]}
[0m14:33:08.840154 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:08.842844 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:08.854776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2396b466-4665-4c65-a10f-d6f51ded056e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa77907a0>]}
[0m14:33:08.855261 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:08.855685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2396b466-4665-4c65-a10f-d6f51ded056e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa85a60f0>]}
[0m14:33:08.857757 [info ] [MainThread]: 
[0m14:33:08.858197 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:08.858620 [info ] [MainThread]: 
[0m14:33:08.859203 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:08.865777 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:08.877135 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:08.877711 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:08.878094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:08.878814 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:08.879349 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:08.879743 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:08.880200 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:08.880645 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:08.881527 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:08.881908 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:08.882289 [info ] [MainThread]: 
[0m14:33:08.882756 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:33:08.883399 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:08.884772 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.622459, "process_in_blocks": "16", "process_kernel_time": 0.165849, "process_mem_max_rss": "106904", "process_out_blocks": "984", "process_user_time": 2.07412}
[0m14:33:08.885658 [debug] [MainThread]: Command `dbt run` failed at 14:33:08.885453 after 0.62 seconds
[0m14:33:08.886380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa8df3d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa78c50d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x717fa7dffdd0>]}
[0m14:33:08.886851 [debug] [MainThread]: Flushing usage events
[0m14:33:10.522386 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:15.292042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6b207c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6b3c9c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6b3c9460>]}


============================== 14:33:15.296195 | 8afa0670-66b0-44f5-b4da-e0b4c042f1ca ==============================
[0m14:33:15.296195 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:15.297145 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:33:15.372690 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:15.373589 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:15.374259 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:15.605879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8afa0670-66b0-44f5-b4da-e0b4c042f1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6b9b4800>]}
[0m14:33:15.715947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8afa0670-66b0-44f5-b4da-e0b4c042f1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6a703aa0>]}
[0m14:33:15.718417 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:15.836973 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:15.957184 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:15.957682 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:15.964907 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:15.989393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8afa0670-66b0-44f5-b4da-e0b4c042f1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6a519a00>]}
[0m14:33:16.054588 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:16.057482 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:16.070211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8afa0670-66b0-44f5-b4da-e0b4c042f1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6a515fd0>]}
[0m14:33:16.071101 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:16.071873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8afa0670-66b0-44f5-b4da-e0b4c042f1ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6a5bbf20>]}
[0m14:33:16.075259 [info ] [MainThread]: 
[0m14:33:16.076110 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:16.076815 [info ] [MainThread]: 
[0m14:33:16.077791 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:16.088024 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:16.106182 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:16.107018 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:16.107597 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:16.108550 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:16.109324 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:16.109918 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:16.110676 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:16.111248 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:16.112392 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:16.112934 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:16.113454 [info ] [MainThread]: 
[0m14:33:16.114002 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.04 seconds (0.04s).
[0m14:33:16.115054 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:16.116854 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.89283, "process_in_blocks": "0", "process_kernel_time": 0.194683, "process_mem_max_rss": "107104", "process_out_blocks": "968", "process_user_time": 2.2124}
[0m14:33:16.117841 [debug] [MainThread]: Command `dbt run` failed at 14:33:16.117657 after 0.89 seconds
[0m14:33:16.118437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6ce88740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6a867bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x732a6a46b7a0>]}
[0m14:33:16.119060 [debug] [MainThread]: Flushing usage events
[0m14:33:17.485574 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:22.362066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24ddef740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24da9e030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24de5eea0>]}


============================== 14:33:22.365271 | fe78a408-cf7d-4b97-9b50-8e0026ca24d5 ==============================
[0m14:33:22.365271 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:22.366298 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:33:22.421059 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:22.421542 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:22.421919 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:22.569337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe78a408-cf7d-4b97-9b50-8e0026ca24d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24e198860>]}
[0m14:33:22.632155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fe78a408-cf7d-4b97-9b50-8e0026ca24d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24e199ac0>]}
[0m14:33:22.632861 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:22.703411 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:22.785348 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:22.785778 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:22.792266 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:22.812633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fe78a408-cf7d-4b97-9b50-8e0026ca24d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24cdd9010>]}
[0m14:33:22.873426 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:22.876467 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:22.889353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fe78a408-cf7d-4b97-9b50-8e0026ca24d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24ccefb90>]}
[0m14:33:22.889935 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:22.890371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe78a408-cf7d-4b97-9b50-8e0026ca24d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24ea0df40>]}
[0m14:33:22.892338 [info ] [MainThread]: 
[0m14:33:22.892868 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:22.893290 [info ] [MainThread]: 
[0m14:33:22.893931 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:22.902491 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:22.915632 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:22.916132 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:22.916515 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:22.917198 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:22.917707 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:22.918100 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:22.918537 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:22.918904 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:22.919631 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:22.919982 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:22.920318 [info ] [MainThread]: 
[0m14:33:22.920708 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:33:22.921207 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:22.922007 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.63045204, "process_in_blocks": "0", "process_kernel_time": 0.131383, "process_mem_max_rss": "107124", "process_out_blocks": "976", "process_user_time": 1.893694}
[0m14:33:22.922459 [debug] [MainThread]: Command `dbt run` failed at 14:33:22.922359 after 0.63 seconds
[0m14:33:22.922939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24d6b1220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24ca8d8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e24d118d70>]}
[0m14:33:22.923364 [debug] [MainThread]: Flushing usage events
[0m14:33:24.141756 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:28.896437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55eb4f62a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e825fe60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e825d160>]}


============================== 14:33:28.900324 | 59796748-39b0-4395-8614-19dbfec382a9 ==============================
[0m14:33:28.900324 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:28.901102 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:33:28.961378 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:28.961967 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:28.962464 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:29.129460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '59796748-39b0-4395-8614-19dbfec382a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e8999a90>]}
[0m14:33:29.202295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '59796748-39b0-4395-8614-19dbfec382a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e7812ba0>]}
[0m14:33:29.203142 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:29.286838 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:29.378974 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:29.379526 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:29.387569 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:29.410094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59796748-39b0-4395-8614-19dbfec382a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e7598ad0>]}
[0m14:33:29.476443 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:29.479564 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:29.491842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59796748-39b0-4395-8614-19dbfec382a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e74d03b0>]}
[0m14:33:29.492384 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:29.492818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59796748-39b0-4395-8614-19dbfec382a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e921e000>]}
[0m14:33:29.495011 [info ] [MainThread]: 
[0m14:33:29.495456 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:29.495837 [info ] [MainThread]: 
[0m14:33:29.496478 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:29.504398 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:29.517698 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:29.518204 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:29.518611 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:29.519327 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:29.519923 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:29.520349 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:29.520851 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:29.521258 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:29.522210 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:29.522777 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:29.523397 [info ] [MainThread]: 
[0m14:33:29.524053 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:33:29.524585 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:29.525653 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.69114727, "process_in_blocks": "0", "process_kernel_time": 0.157171, "process_mem_max_rss": "107132", "process_out_blocks": "984", "process_user_time": 2.092969}
[0m14:33:29.526326 [debug] [MainThread]: Command `dbt run` failed at 14:33:29.526180 after 0.69 seconds
[0m14:33:29.526897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55eb4f62a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55eb72c140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d55e74cec90>]}
[0m14:33:29.527394 [debug] [MainThread]: Flushing usage events
[0m14:33:30.796698 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:35.224847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa584bef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa5df7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa5df79b0>]}


============================== 14:33:35.227788 | 8b2cc183-a8ea-4e69-8b5a-50777e206a06 ==============================
[0m14:33:35.227788 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:35.228650 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:33:35.284256 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:35.284758 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:35.285149 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:35.430588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b2cc183-a8ea-4e69-8b5a-50777e206a06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa6f3c380>]}
[0m14:33:35.493753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b2cc183-a8ea-4e69-8b5a-50777e206a06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa4d98e00>]}
[0m14:33:35.494467 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:35.566799 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:35.646193 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:35.646707 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:35.653456 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:35.673659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8b2cc183-a8ea-4e69-8b5a-50777e206a06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa4a5f740>]}
[0m14:33:35.729191 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:35.732106 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:35.743075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8b2cc183-a8ea-4e69-8b5a-50777e206a06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa472d1f0>]}
[0m14:33:35.743620 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:35.743993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b2cc183-a8ea-4e69-8b5a-50777e206a06', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa4a5e0f0>]}
[0m14:33:35.745994 [info ] [MainThread]: 
[0m14:33:35.746566 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:35.747013 [info ] [MainThread]: 
[0m14:33:35.747733 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:35.754876 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:35.766487 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:35.767004 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:35.767401 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:35.768081 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:35.768715 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:35.769168 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:35.769747 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:35.770180 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:35.771108 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:35.771558 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:35.772007 [info ] [MainThread]: 
[0m14:33:35.772470 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:33:35.773104 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:35.774089 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.60592693, "process_in_blocks": "0", "process_kernel_time": 0.165333, "process_mem_max_rss": "106980", "process_out_blocks": "968", "process_user_time": 1.834604}
[0m14:33:35.774685 [debug] [MainThread]: Command `dbt run` failed at 14:33:35.774532 after 0.61 seconds
[0m14:33:35.775177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa5332ab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa485d850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71eaa479ea80>]}
[0m14:33:35.775690 [debug] [MainThread]: Flushing usage events
[0m14:33:37.044565 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:41.915286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9508c6300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d950658d40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d950933260>]}


============================== 14:33:41.918840 | 9052e964-eeef-44eb-af4b-e6c66b642fba ==============================
[0m14:33:41.918840 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:41.919516 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:33:41.979810 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:41.980561 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:41.981244 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:42.141861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9052e964-eeef-44eb-af4b-e6c66b642fba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d950e2da00>]}
[0m14:33:42.218757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9052e964-eeef-44eb-af4b-e6c66b642fba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d94ff62c00>]}
[0m14:33:42.219844 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:42.337022 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:42.423294 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:42.423759 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:42.431522 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:42.453707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9052e964-eeef-44eb-af4b-e6c66b642fba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d9502dbfe0>]}
[0m14:33:42.523910 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:42.527261 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:42.539929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9052e964-eeef-44eb-af4b-e6c66b642fba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d94f9bd4c0>]}
[0m14:33:42.540500 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:42.540952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9052e964-eeef-44eb-af4b-e6c66b642fba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d94fa43cb0>]}
[0m14:33:42.543202 [info ] [MainThread]: 
[0m14:33:42.543715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:42.544225 [info ] [MainThread]: 
[0m14:33:42.544970 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:42.553715 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:42.566732 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:42.567166 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:42.567508 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:42.568160 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:42.568664 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:42.569048 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:42.569504 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:42.569870 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:42.570649 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:42.571014 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:42.571358 [info ] [MainThread]: 
[0m14:33:42.571730 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:33:42.572211 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:42.573005 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.71950656, "process_in_blocks": "0", "process_kernel_time": 0.168698, "process_mem_max_rss": "107128", "process_out_blocks": "968", "process_user_time": 2.130193}
[0m14:33:42.573447 [debug] [MainThread]: Command `dbt run` failed at 14:33:42.573349 after 0.72 seconds
[0m14:33:42.573848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d950a80800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d94f97dd60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75d94fa1bb60>]}
[0m14:33:42.574224 [debug] [MainThread]: Flushing usage events
[0m14:33:43.801964 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:48.484413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec555682f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec564a1a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec55568da0>]}


============================== 14:33:48.487377 | d0b3f376-82cf-4ac8-82e8-ea84b6a3ec81 ==============================
[0m14:33:48.487377 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:48.488056 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:33:48.543194 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:48.543843 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:48.544563 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:48.722857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0b3f376-82cf-4ac8-82e8-ea84b6a3ec81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec557e1a30>]}
[0m14:33:48.785570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0b3f376-82cf-4ac8-82e8-ea84b6a3ec81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec5520ed20>]}
[0m14:33:48.786190 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:48.862212 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:48.949378 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:48.949802 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:48.956358 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:48.976743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0b3f376-82cf-4ac8-82e8-ea84b6a3ec81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec543f9490>]}
[0m14:33:49.037505 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:49.040006 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:49.050674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0b3f376-82cf-4ac8-82e8-ea84b6a3ec81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec543741d0>]}
[0m14:33:49.051163 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:49.051542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0b3f376-82cf-4ac8-82e8-ea84b6a3ec81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec54223f20>]}
[0m14:33:49.053367 [info ] [MainThread]: 
[0m14:33:49.053859 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:49.054228 [info ] [MainThread]: 
[0m14:33:49.054785 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:49.061198 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:49.072206 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:49.072645 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:49.073000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:49.073703 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:49.074336 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:49.074770 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:49.075256 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:49.075591 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:49.076417 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:49.076779 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:49.077187 [info ] [MainThread]: 
[0m14:33:49.077595 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:33:49.078220 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:49.079485 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.65088415, "process_in_blocks": "0", "process_kernel_time": 0.144572, "process_mem_max_rss": "107068", "process_out_blocks": "976", "process_user_time": 1.873459}
[0m14:33:49.080069 [debug] [MainThread]: Command `dbt run` failed at 14:33:49.079940 after 0.65 seconds
[0m14:33:49.080506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec56c50830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec5437a8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75ec558d7e30>]}
[0m14:33:49.080949 [debug] [MainThread]: Flushing usage events
[0m14:33:50.355665 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:33:55.475495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2597a5cef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2595baac30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2595baabd0>]}


============================== 14:33:55.479334 | d572ef35-8818-49c8-bb13-30151ab04a07 ==============================
[0m14:33:55.479334 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:33:55.480222 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:33:55.535791 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:33:55.536258 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:33:55.536636 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:33:55.693584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd572ef35-8818-49c8-bb13-30151ab04a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d25966bd7f0>]}
[0m14:33:55.761338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd572ef35-8818-49c8-bb13-30151ab04a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d25960d38c0>]}
[0m14:33:55.762163 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:33:55.840842 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:33:55.953962 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:33:55.954530 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:33:55.963240 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:33:55.992936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd572ef35-8818-49c8-bb13-30151ab04a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d25957ed820>]}
[0m14:33:56.058441 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:33:56.062632 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:33:56.074628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd572ef35-8818-49c8-bb13-30151ab04a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d259521ddf0>]}
[0m14:33:56.075151 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:33:56.075623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd572ef35-8818-49c8-bb13-30151ab04a07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2595f35430>]}
[0m14:33:56.078095 [info ] [MainThread]: 
[0m14:33:56.078772 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:33:56.079314 [info ] [MainThread]: 
[0m14:33:56.080076 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:33:56.086680 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:33:56.099708 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:33:56.100321 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:56.100913 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:33:56.101645 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:33:56.102176 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:33:56.102562 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:33:56.103028 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:33:56.103464 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:33:56.104296 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:33:56.104664 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:33:56.105011 [info ] [MainThread]: 
[0m14:33:56.105488 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:33:56.106061 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:33:56.107148 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.69246125, "process_in_blocks": "0", "process_kernel_time": 0.164651, "process_mem_max_rss": "107128", "process_out_blocks": "976", "process_user_time": 1.925925}
[0m14:33:56.107690 [debug] [MainThread]: Command `dbt run` failed at 14:33:56.107586 after 0.69 seconds
[0m14:33:56.108073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d2597a5cef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d259516db50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d259516f680>]}
[0m14:33:56.108477 [debug] [MainThread]: Flushing usage events
[0m14:33:57.319825 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:02.034428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef0750ef30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef05996780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef07688530>]}


============================== 14:34:02.038148 | 736794ce-418e-40f1-8766-4962449fa1a0 ==============================
[0m14:34:02.038148 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:02.038873 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:34:02.124484 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:02.125192 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:02.125790 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:02.340863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '736794ce-418e-40f1-8766-4962449fa1a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef056468a0>]}
[0m14:34:02.434841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '736794ce-418e-40f1-8766-4962449fa1a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef05644c80>]}
[0m14:34:02.435715 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:02.539120 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:02.644282 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:02.645758 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:02.655083 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:02.683865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '736794ce-418e-40f1-8766-4962449fa1a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef0502b440>]}
[0m14:34:02.772740 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:02.775539 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:02.787438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '736794ce-418e-40f1-8766-4962449fa1a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef04c88c80>]}
[0m14:34:02.788013 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:02.788447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '736794ce-418e-40f1-8766-4962449fa1a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef04dbd040>]}
[0m14:34:02.790477 [info ] [MainThread]: 
[0m14:34:02.790973 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:02.791413 [info ] [MainThread]: 
[0m14:34:02.792137 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:02.799205 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:02.812198 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:02.812714 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:02.813103 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:02.813807 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:02.814442 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:02.814912 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:02.815452 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:02.815888 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:02.816774 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:02.817158 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:02.817528 [info ] [MainThread]: 
[0m14:34:02.817957 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:34:02.818499 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:02.819563 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.8631068, "process_in_blocks": "0", "process_kernel_time": 0.171642, "process_mem_max_rss": "107064", "process_out_blocks": "968", "process_user_time": 2.180459}
[0m14:34:02.820129 [debug] [MainThread]: Command `dbt run` failed at 14:34:02.820012 after 0.86 seconds
[0m14:34:02.820585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef062bcdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef04dbd0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x73ef04dbd010>]}
[0m14:34:02.821047 [debug] [MainThread]: Flushing usage events
[0m14:34:04.181289 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:08.758965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbf4dfc80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbeca4620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbf4df140>]}


============================== 14:34:08.762107 | a0ffa541-9217-4ae2-9898-3d54affce99a ==============================
[0m14:34:08.762107 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:08.762671 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m14:34:08.822327 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:08.822927 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:08.823436 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:08.980997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a0ffa541-9217-4ae2-9898-3d54affce99a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbf38c3b0>]}
[0m14:34:09.044675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a0ffa541-9217-4ae2-9898-3d54affce99a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddc01ff620>]}
[0m14:34:09.045430 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:09.118990 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:09.201254 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:09.201717 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:09.208443 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:09.228688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a0ffa541-9217-4ae2-9898-3d54affce99a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbdf279b0>]}
[0m14:34:09.282586 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:09.285523 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:09.296203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a0ffa541-9217-4ae2-9898-3d54affce99a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbdee0530>]}
[0m14:34:09.296709 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:09.297059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a0ffa541-9217-4ae2-9898-3d54affce99a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbdc7dd30>]}
[0m14:34:09.299044 [info ] [MainThread]: 
[0m14:34:09.299516 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:09.299884 [info ] [MainThread]: 
[0m14:34:09.300441 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:09.306589 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:09.317918 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:09.318456 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:09.318850 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:09.319559 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:09.320110 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:09.320520 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:09.321046 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:09.321461 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:09.322319 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:09.322704 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:09.323079 [info ] [MainThread]: 
[0m14:34:09.323562 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:34:09.324263 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:09.325552 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.62477005, "process_in_blocks": "0", "process_kernel_time": 0.142828, "process_mem_max_rss": "107124", "process_out_blocks": "976", "process_user_time": 1.840784}
[0m14:34:09.326153 [debug] [MainThread]: Command `dbt run` failed at 14:34:09.326020 after 0.63 seconds
[0m14:34:09.326662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbeca4620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddc212c140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ddbdfa5c10>]}
[0m14:34:09.327173 [debug] [MainThread]: Flushing usage events
[0m14:34:14.422039 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:19.768097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ed500830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46eccd98e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ed5fbce0>]}


============================== 14:34:19.771176 | f363dd24-1f69-42fc-9220-66b24ef17402 ==============================
[0m14:34:19.771176 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:19.771840 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:34:19.827361 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:19.827896 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:19.828287 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:19.989560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f363dd24-1f69-42fc-9220-66b24ef17402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ed4b08f0>]}
[0m14:34:20.057243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f363dd24-1f69-42fc-9220-66b24ef17402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ec219160>]}
[0m14:34:20.057951 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:20.148073 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:20.249246 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:20.249723 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:20.256931 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:20.279492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f363dd24-1f69-42fc-9220-66b24ef17402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ec273440>]}
[0m14:34:20.346775 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:20.349605 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:20.361154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f363dd24-1f69-42fc-9220-66b24ef17402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46edd1e150>]}
[0m14:34:20.361764 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:20.362218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f363dd24-1f69-42fc-9220-66b24ef17402', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ebd9faa0>]}
[0m14:34:20.364496 [info ] [MainThread]: 
[0m14:34:20.365038 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:20.365453 [info ] [MainThread]: 
[0m14:34:20.366077 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:20.373364 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:20.386194 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:20.386690 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:20.387071 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:20.387858 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:20.388484 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:20.388894 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:20.389378 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:20.389829 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:20.390808 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:20.391224 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:20.391617 [info ] [MainThread]: 
[0m14:34:20.392005 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:34:20.393039 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:20.394652 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.68719405, "process_in_blocks": "0", "process_kernel_time": 0.172764, "process_mem_max_rss": "107120", "process_out_blocks": "976", "process_user_time": 2.079166}
[0m14:34:20.395318 [debug] [MainThread]: Command `dbt run` failed at 14:34:20.395167 after 0.69 seconds
[0m14:34:20.395920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ed62c6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ec0f5d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46ec9eba70>]}
[0m14:34:20.396440 [debug] [MainThread]: Flushing usage events
[0m14:34:21.379479 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:26.223290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a17c8aa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a1edf590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a19338c0>]}


============================== 14:34:26.227100 | 645aec8a-c03d-472a-b0a3-0fe139a1e89b ==============================
[0m14:34:26.227100 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:26.228087 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m14:34:26.311895 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:26.312499 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:26.313022 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:26.537235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '645aec8a-c03d-472a-b0a3-0fe139a1e89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a11dfda0>]}
[0m14:34:26.620640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '645aec8a-c03d-472a-b0a3-0fe139a1e89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a248d6a0>]}
[0m14:34:26.621481 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:26.723266 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:26.852859 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:26.853442 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:26.863684 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:26.895183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '645aec8a-c03d-472a-b0a3-0fe139a1e89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a087ff80>]}
[0m14:34:26.980358 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:26.984127 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:27.001042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '645aec8a-c03d-472a-b0a3-0fe139a1e89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a0883a40>]}
[0m14:34:27.001952 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:27.002567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '645aec8a-c03d-472a-b0a3-0fe139a1e89b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a076aed0>]}
[0m14:34:27.005999 [info ] [MainThread]: 
[0m14:34:27.006845 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:27.007379 [info ] [MainThread]: 
[0m14:34:27.008223 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:27.021158 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:27.049113 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:27.050161 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:27.051023 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:27.052355 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:27.053582 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:27.054585 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:27.056070 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:27.057111 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:27.058845 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:27.059461 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:27.060084 [info ] [MainThread]: 
[0m14:34:27.060966 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.05 seconds (0.05s).
[0m14:34:27.062233 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:27.064300 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.913784, "process_in_blocks": "0", "process_kernel_time": 0.173647, "process_mem_max_rss": "107180", "process_out_blocks": "976", "process_user_time": 2.380171}
[0m14:34:27.065634 [debug] [MainThread]: Command `dbt run` failed at 14:34:27.065271 after 0.92 seconds
[0m14:34:27.066763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a1d80860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a08194f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70f2a06b7260>]}
[0m14:34:27.067912 [debug] [MainThread]: Flushing usage events
[0m14:34:28.065506 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:32.976588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bdb143b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bee287a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bd169cd0>]}


============================== 14:34:32.979829 | 9eb5ca82-8800-4165-afd5-9518e5a1b074 ==============================
[0m14:34:32.979829 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:32.980595 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:34:33.040236 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:33.040776 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:33.041385 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:33.225908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9eb5ca82-8800-4165-afd5-9518e5a1b074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bc6d8530>]}
[0m14:34:33.295905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9eb5ca82-8800-4165-afd5-9518e5a1b074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bcfc4f20>]}
[0m14:34:33.296691 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:33.376772 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:33.469222 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:33.469695 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:33.477025 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:33.499085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9eb5ca82-8800-4165-afd5-9518e5a1b074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bc756120>]}
[0m14:34:33.559441 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:33.562313 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:33.574531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9eb5ca82-8800-4165-afd5-9518e5a1b074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bc2a89b0>]}
[0m14:34:33.575111 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:33.575584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9eb5ca82-8800-4165-afd5-9518e5a1b074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bc756240>]}
[0m14:34:33.577796 [info ] [MainThread]: 
[0m14:34:33.578302 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:33.578729 [info ] [MainThread]: 
[0m14:34:33.579469 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:33.587883 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:33.600176 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:33.600652 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:33.601032 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:33.601734 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:33.602329 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:33.602755 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:33.603245 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:33.603643 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:33.604588 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:33.605028 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:33.605445 [info ] [MainThread]: 
[0m14:34:33.605930 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:34:33.606508 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:33.607551 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.69210184, "process_in_blocks": "0", "process_kernel_time": 0.138677, "process_mem_max_rss": "107044", "process_out_blocks": "968", "process_user_time": 2.042245}
[0m14:34:33.608164 [debug] [MainThread]: Command `dbt run` failed at 14:34:33.608028 after 0.69 seconds
[0m14:34:33.608659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bcecdc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bc595cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71b8bceed0a0>]}
[0m14:34:33.609143 [debug] [MainThread]: Flushing usage events
[0m14:34:34.591792 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:39.891697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74408afd4890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74408c408ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74408b749b80>]}


============================== 14:34:39.895690 | b2c00c1a-77cf-493a-b1f3-4cc47984d3b1 ==============================
[0m14:34:39.895690 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:39.896849 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:34:39.970167 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:39.970800 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:39.971471 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:40.201639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2c00c1a-77cf-493a-b1f3-4cc47984d3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74408a08a6c0>]}
[0m14:34:40.322891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2c00c1a-77cf-493a-b1f3-4cc47984d3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74408bdbe420>]}
[0m14:34:40.323900 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:40.446461 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:40.552594 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:40.553136 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:40.561607 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:40.587458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b2c00c1a-77cf-493a-b1f3-4cc47984d3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744089b85f10>]}
[0m14:34:40.662457 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:40.665773 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:40.679674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b2c00c1a-77cf-493a-b1f3-4cc47984d3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7440899a8590>]}
[0m14:34:40.680335 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:40.680898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2c00c1a-77cf-493a-b1f3-4cc47984d3b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744089a699d0>]}
[0m14:34:40.683376 [info ] [MainThread]: 
[0m14:34:40.683972 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:40.684501 [info ] [MainThread]: 
[0m14:34:40.685306 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:40.693647 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:40.708359 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:40.708894 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:40.709349 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:40.710192 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:40.710995 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:40.711572 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:40.712244 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:40.712728 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:40.713719 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:40.714248 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:40.714771 [info ] [MainThread]: 
[0m14:34:40.715470 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:34:40.716378 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:40.717745 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.9282892, "process_in_blocks": "0", "process_kernel_time": 0.195684, "process_mem_max_rss": "107120", "process_out_blocks": "976", "process_user_time": 2.666702}
[0m14:34:40.718481 [debug] [MainThread]: Command `dbt run` failed at 14:34:40.718340 after 0.93 seconds
[0m14:34:40.719004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74408e7b3b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744089badc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x744089dfce30>]}
[0m14:34:40.719519 [debug] [MainThread]: Flushing usage events
[0m14:34:41.709706 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:46.493696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a928cb17f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a928cb16a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a928cb20c0>]}


============================== 14:34:46.497738 | 600ee218-e652-4229-b2ed-48c57c7d7339 ==============================
[0m14:34:46.497738 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:46.499349 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:34:46.557966 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:46.558531 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:46.558976 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:46.716716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '600ee218-e652-4229-b2ed-48c57c7d7339', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a9292180b0>]}
[0m14:34:46.785850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '600ee218-e652-4229-b2ed-48c57c7d7339', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a927f9b980>]}
[0m14:34:46.786571 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:46.864939 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:46.955568 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:46.956022 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:46.963204 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:46.987331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '600ee218-e652-4229-b2ed-48c57c7d7339', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a927e59490>]}
[0m14:34:47.048318 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:47.051159 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:47.062668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '600ee218-e652-4229-b2ed-48c57c7d7339', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a927daed80>]}
[0m14:34:47.063200 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:47.063642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '600ee218-e652-4229-b2ed-48c57c7d7339', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a927f83830>]}
[0m14:34:47.065808 [info ] [MainThread]: 
[0m14:34:47.066266 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:47.066660 [info ] [MainThread]: 
[0m14:34:47.067334 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:47.074475 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:47.087011 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:47.087503 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:47.087881 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:47.088597 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:47.089211 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:47.089657 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:47.090173 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:47.090578 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:47.091428 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:47.091838 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:47.092243 [info ] [MainThread]: 
[0m14:34:47.092759 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:34:47.093440 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:47.094677 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.66280943, "process_in_blocks": "0", "process_kernel_time": 0.148778, "process_mem_max_rss": "107116", "process_out_blocks": "976", "process_user_time": 2.002018}
[0m14:34:47.095287 [debug] [MainThread]: Command `dbt run` failed at 14:34:47.095150 after 0.66 seconds
[0m14:34:47.095805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a928a16b70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a927b21ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77a927c403b0>]}
[0m14:34:47.096322 [debug] [MainThread]: Flushing usage events
[0m14:34:48.084910 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:34:53.803728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243e0a20c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243decbbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243e0a0e30>]}


============================== 14:34:53.807288 | f322fe14-eeac-4117-80d9-ffbea6cdc689 ==============================
[0m14:34:53.807288 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:34:53.807937 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:34:53.874350 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:34:53.874866 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:34:53.875266 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:34:54.067031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f322fe14-eeac-4117-80d9-ffbea6cdc689', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243d767ce0>]}
[0m14:34:54.140591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f322fe14-eeac-4117-80d9-ffbea6cdc689', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243d377890>]}
[0m14:34:54.141256 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:34:54.223359 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:34:54.298668 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:34:54.299106 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:34:54.305511 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:34:54.324486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f322fe14-eeac-4117-80d9-ffbea6cdc689', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243d209d60>]}
[0m14:34:54.382289 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:34:54.384874 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:34:54.395302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f322fe14-eeac-4117-80d9-ffbea6cdc689', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243d141850>]}
[0m14:34:54.395851 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:34:54.396224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f322fe14-eeac-4117-80d9-ffbea6cdc689', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243d005850>]}
[0m14:34:54.398088 [info ] [MainThread]: 
[0m14:34:54.398517 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:34:54.398941 [info ] [MainThread]: 
[0m14:34:54.399532 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:34:54.405736 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:34:54.416357 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:34:54.416811 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:54.417147 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:34:54.417818 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:34:54.418446 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:34:54.418904 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:34:54.419386 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:34:54.419763 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:34:54.420595 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:34:54.420923 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:34:54.421268 [info ] [MainThread]: 
[0m14:34:54.421680 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:34:54.422242 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:34:54.423125 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6826449, "process_in_blocks": "0", "process_kernel_time": 0.219318, "process_mem_max_rss": "107144", "process_out_blocks": "968", "process_user_time": 2.492258}
[0m14:34:54.423622 [debug] [MainThread]: Command `dbt run` failed at 14:34:54.423476 after 0.68 seconds
[0m14:34:54.424010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243db3a810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243d236030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75243d0df7a0>]}
[0m14:34:54.424419 [debug] [MainThread]: Flushing usage events
[0m14:34:55.570344 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:01.887273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596db7456d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d8cd10a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d8cd27e0>]}


============================== 14:35:01.890069 | aca91316-fe4e-4d00-b1e3-81a74ea97114 ==============================
[0m14:35:01.890069 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:35:01.890638 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:35:01.945079 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:01.945530 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:01.945928 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:02.095528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aca91316-fe4e-4d00-b1e3-81a74ea97114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d800aff0>]}
[0m14:35:02.157812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aca91316-fe4e-4d00-b1e3-81a74ea97114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d8cd3c20>]}
[0m14:35:02.158591 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:35:02.234784 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:35:02.341939 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:02.342473 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:02.350424 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:35:02.375697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aca91316-fe4e-4d00-b1e3-81a74ea97114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d7e98d70>]}
[0m14:35:02.441026 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:35:02.443789 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:35:02.456142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aca91316-fe4e-4d00-b1e3-81a74ea97114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d7d89130>]}
[0m14:35:02.456740 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:35:02.457173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aca91316-fe4e-4d00-b1e3-81a74ea97114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d87f7cb0>]}
[0m14:35:02.459200 [info ] [MainThread]: 
[0m14:35:02.459670 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:02.460055 [info ] [MainThread]: 
[0m14:35:02.460671 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:35:02.468766 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:35:02.481808 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:35:02.482388 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:02.482906 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:02.483962 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:35:02.484846 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:02.485436 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:35:02.486135 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:35:02.486674 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:35:02.487916 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:02.488375 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:35:02.488920 [info ] [MainThread]: 
[0m14:35:02.489545 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:35:02.490303 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:35:02.491583 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.66162133, "process_in_blocks": "0", "process_kernel_time": 0.181894, "process_mem_max_rss": "107120", "process_out_blocks": "976", "process_user_time": 1.997837}
[0m14:35:02.492176 [debug] [MainThread]: Command `dbt run` failed at 14:35:02.492061 after 0.66 seconds
[0m14:35:02.492627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d915d880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d87b71a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7596d7bfee70>]}
[0m14:35:02.493060 [debug] [MainThread]: Flushing usage events
[0m14:35:03.780555 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:08.374703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bdddf500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20be08d9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bdd7f5f0>]}


============================== 14:35:08.377654 | e69d606c-08c5-477b-83b1-12f59a8068ff ==============================
[0m14:35:08.377654 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:35:08.378207 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:35:08.431251 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:08.431787 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:08.432213 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:08.581221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e69d606c-08c5-477b-83b1-12f59a8068ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bedd4260>]}
[0m14:35:08.638672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e69d606c-08c5-477b-83b1-12f59a8068ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bcd67a40>]}
[0m14:35:08.639354 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:35:08.708516 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:35:08.785115 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:08.785519 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:08.791900 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:35:08.812299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e69d606c-08c5-477b-83b1-12f59a8068ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bc859ca0>]}
[0m14:35:08.867249 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:35:08.870123 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:35:08.880216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e69d606c-08c5-477b-83b1-12f59a8068ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bc77ccb0>]}
[0m14:35:08.880873 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:35:08.881435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e69d606c-08c5-477b-83b1-12f59a8068ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bd16f500>]}
[0m14:35:08.883734 [info ] [MainThread]: 
[0m14:35:08.884381 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:08.885101 [info ] [MainThread]: 
[0m14:35:08.886135 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:35:08.895484 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:35:08.906948 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:35:08.907335 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:08.907720 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:08.908339 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:35:08.908905 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:08.909253 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:35:08.909769 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:35:08.910077 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:35:08.910885 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:08.911198 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:35:08.911528 [info ] [MainThread]: 
[0m14:35:08.911960 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:35:08.912437 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:35:08.913304 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5932213, "process_in_blocks": "0", "process_kernel_time": 0.140697, "process_mem_max_rss": "107248", "process_out_blocks": "976", "process_user_time": 1.73227}
[0m14:35:08.913925 [debug] [MainThread]: Command `dbt run` failed at 14:35:08.913794 after 0.59 seconds
[0m14:35:08.914342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20c1707aa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bc8580e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d20bcb06ed0>]}
[0m14:35:08.914800 [debug] [MainThread]: Flushing usage events
[0m14:35:10.131768 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:21.800078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c3dfcf20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c42a5dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c42a40b0>]}


============================== 14:35:21.803095 | a943b521-6a82-4797-a576-bb4cf665b0ef ==============================
[0m14:35:21.803095 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:35:21.803678 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:35:21.856637 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:21.857105 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:21.857473 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:22.003367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a943b521-6a82-4797-a576-bb4cf665b0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c480c410>]}
[0m14:35:22.061037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a943b521-6a82-4797-a576-bb4cf665b0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c41eaa20>]}
[0m14:35:22.061774 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:35:22.130035 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:35:22.206309 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:22.206834 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:22.213062 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:35:22.231599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a943b521-6a82-4797-a576-bb4cf665b0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c35b9e20>]}
[0m14:35:22.283470 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:35:22.286079 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:35:22.295949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a943b521-6a82-4797-a576-bb4cf665b0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c3347d10>]}
[0m14:35:22.296460 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:35:22.296866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a943b521-6a82-4797-a576-bb4cf665b0ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c3685040>]}
[0m14:35:22.298707 [info ] [MainThread]: 
[0m14:35:22.299105 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:22.299495 [info ] [MainThread]: 
[0m14:35:22.300093 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:35:22.306127 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:35:22.317696 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:35:22.318126 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:22.318503 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:22.319158 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:35:22.319758 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:22.320136 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:35:22.320599 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:35:22.320938 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:35:22.321652 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:22.321999 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:35:22.322339 [info ] [MainThread]: 
[0m14:35:22.322745 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:35:22.323208 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:35:22.324037 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.57576686, "process_in_blocks": "0", "process_kernel_time": 0.120645, "process_mem_max_rss": "107008", "process_out_blocks": "976", "process_user_time": 1.725929}
[0m14:35:22.324489 [debug] [MainThread]: Command `dbt run` failed at 14:35:22.324389 after 0.58 seconds
[0m14:35:22.324895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c49ba720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c32f2300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ca6c35b9c40>]}
[0m14:35:22.325298 [debug] [MainThread]: Flushing usage events
[0m14:35:23.546501 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:28.815394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd3b90f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd41129c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd46f2540>]}


============================== 14:35:28.818384 | c2942efd-72eb-41c0-9b99-633efe10f9c7 ==============================
[0m14:35:28.818384 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:35:28.819038 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m14:35:28.880909 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:28.881433 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:28.881896 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:29.023174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c2942efd-72eb-41c0-9b99-633efe10f9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd358ac60>]}
[0m14:35:29.083174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c2942efd-72eb-41c0-9b99-633efe10f9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd4428050>]}
[0m14:35:29.083869 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:35:29.151023 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:35:29.230182 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:29.230624 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:29.237470 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:35:29.256336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c2942efd-72eb-41c0-9b99-633efe10f9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd3065490>]}
[0m14:35:29.308323 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:35:29.310840 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:35:29.321228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c2942efd-72eb-41c0-9b99-633efe10f9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd2fb9a30>]}
[0m14:35:29.321796 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:35:29.322184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c2942efd-72eb-41c0-9b99-633efe10f9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd2e4f590>]}
[0m14:35:29.324065 [info ] [MainThread]: 
[0m14:35:29.324474 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:29.324856 [info ] [MainThread]: 
[0m14:35:29.325417 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:35:29.332163 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:35:29.342807 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:35:29.343227 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:29.343625 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:29.344296 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:35:29.344881 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:29.345285 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:35:29.345816 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:35:29.346157 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:35:29.346888 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:29.347251 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:35:29.347608 [info ] [MainThread]: 
[0m14:35:29.347991 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:35:29.348522 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:35:29.349438 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.5902019, "process_in_blocks": "0", "process_kernel_time": 0.1841, "process_mem_max_rss": "107244", "process_out_blocks": "968", "process_user_time": 2.111682}
[0m14:35:29.349903 [debug] [MainThread]: Command `dbt run` failed at 14:35:29.349803 after 0.59 seconds
[0m14:35:29.350289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd3a54560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd32c6ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x785bd32c5c10>]}
[0m14:35:29.350693 [debug] [MainThread]: Flushing usage events
[0m14:35:30.511962 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:36.957882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c40038e7b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4003e92870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4003e928a0>]}


============================== 14:35:36.961009 | 93584827-4552-4e9f-806a-a422f00604d3 ==============================
[0m14:35:36.961009 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:35:36.961686 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:35:37.024020 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:37.024486 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:37.024874 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:37.193804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '93584827-4552-4e9f-806a-a422f00604d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4003eb02c0>]}
[0m14:35:37.255532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '93584827-4552-4e9f-806a-a422f00604d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4002cec170>]}
[0m14:35:37.256196 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:35:37.327805 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:35:37.432896 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:37.433384 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:37.445391 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:35:37.469927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93584827-4552-4e9f-806a-a422f00604d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4002af1ac0>]}
[0m14:35:37.541948 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:35:37.545641 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:35:37.556225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93584827-4552-4e9f-806a-a422f00604d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4002a07c20>]}
[0m14:35:37.556731 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:35:37.557115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '93584827-4552-4e9f-806a-a422f00604d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4004726060>]}
[0m14:35:37.558941 [info ] [MainThread]: 
[0m14:35:37.559356 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:37.559724 [info ] [MainThread]: 
[0m14:35:37.560272 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:35:37.566679 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:35:37.577318 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:35:37.577835 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:37.578191 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:37.578867 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:35:37.579364 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:37.579718 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:35:37.580139 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:35:37.580498 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:35:37.581382 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:37.581866 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:35:37.582331 [info ] [MainThread]: 
[0m14:35:37.582842 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:35:37.583477 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:35:37.584453 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.696946, "process_in_blocks": "0", "process_kernel_time": 0.17175, "process_mem_max_rss": "107064", "process_out_blocks": "976", "process_user_time": 2.075988}
[0m14:35:37.585146 [debug] [MainThread]: Command `dbt run` failed at 14:35:37.585001 after 0.70 seconds
[0m14:35:37.585656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4003e92870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4007510140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c4002a07ef0>]}
[0m14:35:37.586142 [debug] [MainThread]: Flushing usage events
[0m14:35:38.907991 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:44.837992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e01077b1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e0107c45010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e01077b3e90>]}


============================== 14:35:44.841280 | 0467f24a-f360-4e60-8815-fbacb6833c1f ==============================
[0m14:35:44.841280 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:35:44.841899 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:35:44.899406 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:44.900141 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:44.900751 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:45.054753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0467f24a-f360-4e60-8815-fbacb6833c1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e0107d85a90>]}
[0m14:35:45.120399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0467f24a-f360-4e60-8815-fbacb6833c1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e0107de0dd0>]}
[0m14:35:45.121340 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:35:45.206597 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:35:45.291686 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:45.292093 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:45.298698 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:35:45.319932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0467f24a-f360-4e60-8815-fbacb6833c1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e0107d84a70>]}
[0m14:35:45.378444 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:35:45.381018 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:35:45.391943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0467f24a-f360-4e60-8815-fbacb6833c1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e0106874950>]}
[0m14:35:45.392426 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:35:45.392845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0467f24a-f360-4e60-8815-fbacb6833c1f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e010686a1e0>]}
[0m14:35:45.394776 [info ] [MainThread]: 
[0m14:35:45.395200 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:45.395632 [info ] [MainThread]: 
[0m14:35:45.396235 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:35:45.403118 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:35:45.414394 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:35:45.415164 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:45.415979 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:45.417484 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:35:45.418910 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:45.419751 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:35:45.420755 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:35:45.421482 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:35:45.422726 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:45.423184 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:35:45.423654 [info ] [MainThread]: 
[0m14:35:45.424133 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:35:45.424805 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:35:45.425898 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.64534444, "process_in_blocks": "0", "process_kernel_time": 0.171452, "process_mem_max_rss": "106844", "process_out_blocks": "968", "process_user_time": 1.931827}
[0m14:35:45.426500 [debug] [MainThread]: Command `dbt run` failed at 14:35:45.426366 after 0.65 seconds
[0m14:35:45.427045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e0107ed7020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e01069adc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e01068bd430>]}
[0m14:35:45.427621 [debug] [MainThread]: Flushing usage events
[0m14:35:46.689682 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:35:54.452305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b533f680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b4d96420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b61ede80>]}


============================== 14:35:54.455275 | 05a2452e-0497-42f2-92bd-c5aac7fce577 ==============================
[0m14:35:54.455275 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:35:54.455995 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:35:54.511116 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:35:54.511641 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:35:54.512004 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:35:54.653351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '05a2452e-0497-42f2-92bd-c5aac7fce577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b52b17f0>]}
[0m14:35:54.715999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '05a2452e-0497-42f2-92bd-c5aac7fce577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b415dc70>]}
[0m14:35:54.716761 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:35:54.786454 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:35:54.870377 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:54.870886 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:54.877193 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:35:54.896289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05a2452e-0497-42f2-92bd-c5aac7fce577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b3e4bfe0>]}
[0m14:35:54.947618 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:35:54.950071 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:35:54.960500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05a2452e-0497-42f2-92bd-c5aac7fce577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b3e455b0>]}
[0m14:35:54.961167 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:35:54.961636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05a2452e-0497-42f2-92bd-c5aac7fce577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b4d17dd0>]}
[0m14:35:54.964079 [info ] [MainThread]: 
[0m14:35:54.964518 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:35:54.964871 [info ] [MainThread]: 
[0m14:35:54.965366 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:35:54.971277 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:35:54.981926 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:35:54.982347 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:54.982748 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:54.983526 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:35:54.984185 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:35:54.984598 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:35:54.985110 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:35:54.985529 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:35:54.986479 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:35:54.986872 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:35:54.987193 [info ] [MainThread]: 
[0m14:35:54.987548 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:35:54.988019 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:35:54.988835 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.6092403, "process_in_blocks": "0", "process_kernel_time": 0.220215, "process_mem_max_rss": "106832", "process_out_blocks": "976", "process_user_time": 2.48415}
[0m14:35:54.989272 [debug] [MainThread]: Command `dbt run` failed at 14:35:54.989175 after 0.61 seconds
[0m14:35:54.989650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b5a98470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b3f9a9c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7613b3babbf0>]}
[0m14:35:54.990033 [debug] [MainThread]: Flushing usage events
[0m14:35:56.213218 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:01.095676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c31d1ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c2c16120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c2a0b680>]}


============================== 14:36:01.098510 | a19fc0e6-0fb3-4186-8718-5f59205d198d ==============================
[0m14:36:01.098510 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:36:01.099086 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:36:01.148990 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:36:01.149520 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:36:01.150001 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:36:01.294138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a19fc0e6-0fb3-4186-8718-5f59205d198d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c34187d0>]}
[0m14:36:01.358311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a19fc0e6-0fb3-4186-8718-5f59205d198d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c257e7b0>]}
[0m14:36:01.359090 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:36:01.427418 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:36:01.505032 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:36:01.505464 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:36:01.511974 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:36:01.530679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a19fc0e6-0fb3-4186-8718-5f59205d198d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c22ff500>]}
[0m14:36:01.584586 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:36:01.587102 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:36:01.597299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a19fc0e6-0fb3-4186-8718-5f59205d198d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c1d2cda0>]}
[0m14:36:01.597866 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:36:01.598239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a19fc0e6-0fb3-4186-8718-5f59205d198d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c1d23da0>]}
[0m14:36:01.600093 [info ] [MainThread]: 
[0m14:36:01.600505 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:36:01.600890 [info ] [MainThread]: 
[0m14:36:01.601426 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:36:01.608277 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:36:01.618943 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:36:01.619384 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:36:01.619745 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:36:01.620362 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:36:01.621048 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:36:01.621468 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:36:01.621982 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:36:01.622311 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:36:01.623198 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:36:01.623512 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:36:01.623933 [info ] [MainThread]: 
[0m14:36:01.624313 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:36:01.624874 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:36:01.625741 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.58637077, "process_in_blocks": "0", "process_kernel_time": 0.123754, "process_mem_max_rss": "106972", "process_out_blocks": "976", "process_user_time": 1.777478}
[0m14:36:01.626231 [debug] [MainThread]: Command `dbt run` failed at 14:36:01.626102 after 0.59 seconds
[0m14:36:01.626658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c30aff20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c1d2cd70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7a65c1e21820>]}
[0m14:36:01.627084 [debug] [MainThread]: Flushing usage events
[0m14:36:02.868161 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:36:09.074296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40f153050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40e96e420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40f4f44a0>]}


============================== 14:36:09.077480 | 60da6fc0-9192-4c40-95c0-9c34970fa51a ==============================
[0m14:36:09.077480 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:36:09.078089 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:36:09.151881 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:36:09.152742 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:36:09.153474 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:36:09.337265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '60da6fc0-9192-4c40-95c0-9c34970fa51a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40df04b00>]}
[0m14:36:09.401156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '60da6fc0-9192-4c40-95c0-9c34970fa51a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40f4bb530>]}
[0m14:36:09.401899 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:36:09.493994 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:36:09.595457 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:36:09.595872 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:36:09.605438 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:36:09.628800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '60da6fc0-9192-4c40-95c0-9c34970fa51a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40ddde4b0>]}
[0m14:36:09.694579 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:36:09.697326 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:36:09.710259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '60da6fc0-9192-4c40-95c0-9c34970fa51a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40db20c20>]}
[0m14:36:09.710750 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:36:09.711114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60da6fc0-9192-4c40-95c0-9c34970fa51a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40dd87cb0>]}
[0m14:36:09.712895 [info ] [MainThread]: 
[0m14:36:09.713278 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:36:09.713635 [info ] [MainThread]: 
[0m14:36:09.714220 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:36:09.722667 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:36:09.736079 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:36:09.736652 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:36:09.737140 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:36:09.737958 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:36:09.738594 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:36:09.739019 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:36:09.739549 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:36:09.740063 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:36:09.741022 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:36:09.741457 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:36:09.741867 [info ] [MainThread]: 
[0m14:36:09.742293 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:36:09.742904 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:36:09.744007 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7457036, "process_in_blocks": "0", "process_kernel_time": 0.160661, "process_mem_max_rss": "107124", "process_out_blocks": "968", "process_user_time": 2.033713}
[0m14:36:09.744704 [debug] [MainThread]: Command `dbt run` failed at 14:36:09.744555 after 0.75 seconds
[0m14:36:09.745229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40faccbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40dc31dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bd40dc17fb0>]}
[0m14:36:09.745758 [debug] [MainThread]: Flushing usage events
[0m14:36:11.164666 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:37:55.948478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ecdeca930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ecdac0e30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ecdecbbf0>]}


============================== 14:37:55.954063 | 8b613617-da03-434f-8cab-0e1999890ec4 ==============================
[0m14:37:55.954063 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:37:55.955256 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:37:56.068845 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:37:56.069882 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:37:56.070913 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:37:56.420555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b613617-da03-434f-8cab-0e1999890ec4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ece494590>]}
[0m14:37:56.562610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b613617-da03-434f-8cab-0e1999890ec4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ecd1ce720>]}
[0m14:37:56.563880 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:37:56.742325 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:37:56.907851 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:37:56.908322 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:37:56.915796 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:37:56.940978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8b613617-da03-434f-8cab-0e1999890ec4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ecd241e20>]}
[0m14:37:57.041554 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:37:57.045509 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:37:57.059782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8b613617-da03-434f-8cab-0e1999890ec4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ecd00d880>]}
[0m14:37:57.060458 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:37:57.061043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b613617-da03-434f-8cab-0e1999890ec4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ecd327cb0>]}
[0m14:37:57.064168 [info ] [MainThread]: 
[0m14:37:57.064985 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:37:57.065815 [info ] [MainThread]: 
[0m14:37:57.066894 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:37:57.079396 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:37:57.105973 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:37:57.106919 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:37:57.107797 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:37:57.109195 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:37:57.110482 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:37:57.111551 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:37:57.112870 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:37:57.113981 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:37:57.115700 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:37:57.116255 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:37:57.116778 [info ] [MainThread]: 
[0m14:37:57.117366 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.05 seconds (0.05s).
[0m14:37:57.118569 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:37:57.122145 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.279131, "process_in_blocks": "936", "process_kernel_time": 0.21386, "process_mem_max_rss": "107016", "process_out_blocks": "984", "process_user_time": 2.902103}
[0m14:37:57.123645 [debug] [MainThread]: Command `dbt run` failed at 14:37:57.123299 after 1.28 seconds
[0m14:37:57.125107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8ece202e40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8eccfce9f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8eceb45b20>]}
[0m14:37:57.126793 [debug] [MainThread]: Flushing usage events
[0m14:37:59.098522 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:39:02.903712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc388eab0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc402d670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc3457560>]}


============================== 14:39:02.909687 | a3c353aa-3ffc-4a9e-ae70-1d0639af6727 ==============================
[0m14:39:02.909687 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:39:02.910740 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:39:02.987953 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:39:02.988623 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:39:02.989174 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:39:03.188756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a3c353aa-3ffc-4a9e-ae70-1d0639af6727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc2515f40>]}
[0m14:39:03.249047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a3c353aa-3ffc-4a9e-ae70-1d0639af6727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc2f969c0>]}
[0m14:39:03.249912 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:39:03.330520 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:39:03.427238 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:39:03.427694 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:39:03.434920 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:39:03.457586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a3c353aa-3ffc-4a9e-ae70-1d0639af6727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc22ff740>]}
[0m14:39:03.524432 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:39:03.527813 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:39:03.546850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a3c353aa-3ffc-4a9e-ae70-1d0639af6727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc22e8260>]}
[0m14:39:03.547616 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:39:03.548417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a3c353aa-3ffc-4a9e-ae70-1d0639af6727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc28fa8a0>]}
[0m14:39:03.552006 [info ] [MainThread]: 
[0m14:39:03.552673 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:39:03.553131 [info ] [MainThread]: 
[0m14:39:03.553754 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:39:03.562133 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:39:03.576520 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:39:03.577093 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:39:03.577566 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:39:03.578425 [error] [ThreadPool]: Could not connect to any of [('127.0.0.1', 10000)]
[0m14:39:03.579217 [debug] [ThreadPool]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:39:03.579846 [debug] [ThreadPool]: Spark adapter: Database Error
  failed to connect
[0m14:39:03.580699 [debug] [ThreadPool]: Spark adapter: Error while running:
macro list_schemas
[0m14:39:03.581333 [debug] [ThreadPool]: Spark adapter: Runtime Error
  Database Error
    failed to connect
[0m14:39:03.582982 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:39:03.584027 [debug] [MainThread]: Connection 'list_schemas' was properly closed.
[0m14:39:03.585189 [info ] [MainThread]: 
[0m14:39:03.586174 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m14:39:03.586943 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Database Error
      failed to connect
[0m14:39:03.588226 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.76415396, "process_in_blocks": "1592", "process_kernel_time": 0.18955, "process_mem_max_rss": "107124", "process_out_blocks": "976", "process_user_time": 2.374366}
[0m14:39:03.588941 [debug] [MainThread]: Command `dbt run` failed at 14:39:03.588765 after 0.77 seconds
[0m14:39:03.589568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc2cd3080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc652c080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b0bc22eaf60>]}
[0m14:39:03.590212 [debug] [MainThread]: Flushing usage events
[0m14:39:05.150424 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:39:18.313420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf526889b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf519fe870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf5101e6f0>]}


============================== 14:39:18.316368 | 2bb5b9aa-4c70-4137-95ee-24e70abb176b ==============================
[0m14:39:18.316368 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:39:18.317231 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:39:18.369914 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:39:18.370518 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:39:18.371007 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:39:18.507964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf511b40b0>]}
[0m14:39:18.565277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf50c75790>]}
[0m14:39:18.566015 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:39:18.633458 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:39:18.716608 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:39:18.717037 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:39:18.723257 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:39:18.743177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf50067440>]}
[0m14:39:18.796158 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:39:18.798566 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:39:18.808506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf4fc90680>]}
[0m14:39:18.809102 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:39:18.809512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf4ff8b860>]}
[0m14:39:18.811775 [info ] [MainThread]: 
[0m14:39:18.812358 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:39:18.812859 [info ] [MainThread]: 
[0m14:39:18.813548 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:39:18.821301 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:39:18.832325 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:39:18.832802 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:39:18.833141 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:39:21.707107 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:39:21.707611 [debug] [ThreadPool]: SQL status: OK in 2.874 seconds
[0m14:39:21.767799 [debug] [ThreadPool]: On list_schemas: Close
[0m14:39:21.791768 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:39:21.796776 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:21.797174 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:39:21.797548 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:39:21.797895 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:39:22.419186 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:39:22.419750 [debug] [ThreadPool]: SQL status: OK in 0.622 seconds
[0m14:39:22.427836 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:39:22.428664 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:39:22.429375 [debug] [ThreadPool]: On list_None_default: Close
[0m14:39:22.445298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf4fdb8080>]}
[0m14:39:22.446062 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:22.446577 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:39:22.450880 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:39:22.451718 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:39:22.452418 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:39:22.452985 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:39:22.463160 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:39:22.464119 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:39:22.495663 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:39:22.496391 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:39:22.496900 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:23.065108 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:23.065809 [debug] [Thread-1 (]: SQL status: OK in 0.569 seconds
[0m14:39:23.114581 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:39:23.115365 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:23.115785 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:39:23.116207 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:39:25.181339 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:25.181944 [debug] [Thread-1 (]: SQL status: OK in 2.065 seconds
[0m14:39:25.204979 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:39:25.205552 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:39:25.206009 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:39:25.217216 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf506a3290>]}
[0m14:39:25.217993 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 2.76s]
[0m14:39:25.218698 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:39:25.219310 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:39:25.220102 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:39:25.220756 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:39:25.221353 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:39:25.225112 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:39:25.226187 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:39:25.230877 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:39:25.231411 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:39:25.231867 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:25.411186 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:25.411783 [debug] [Thread-1 (]: SQL status: OK in 0.180 seconds
[0m14:39:25.415236 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:39:25.416059 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:25.416542 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:39:25.417006 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:39:25.894972 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:25.895569 [debug] [Thread-1 (]: SQL status: OK in 0.478 seconds
[0m14:39:25.897941 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:39:25.898384 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:39:25.898767 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:39:25.907051 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf4fb32630>]}
[0m14:39:25.907924 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.69s]
[0m14:39:25.908712 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:39:25.909307 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:39:25.910101 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:39:25.910834 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:39:25.911470 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:39:25.915614 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:39:25.917007 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:39:25.921860 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:39:25.922386 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:39:25.922899 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:26.090890 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:26.091452 [debug] [Thread-1 (]: SQL status: OK in 0.169 seconds
[0m14:39:26.094250 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:39:26.094933 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:26.095367 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:39:26.095815 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:39:26.516413 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:26.516989 [debug] [Thread-1 (]: SQL status: OK in 0.421 seconds
[0m14:39:26.523220 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:39:26.524099 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:39:26.524898 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:39:26.533445 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf4d9e75f0>]}
[0m14:39:26.534866 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.62s]
[0m14:39:26.536159 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:39:26.537123 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:39:26.538323 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:39:26.539872 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:39:26.541334 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:39:26.546070 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:39:26.547283 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:39:26.555973 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:39:26.556911 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:39:26.557816 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:26.725498 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:26.726377 [debug] [Thread-1 (]: SQL status: OK in 0.169 seconds
[0m14:39:26.731347 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:39:26.732435 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:26.733089 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:39:26.733783 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:39:27.257267 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:27.257891 [debug] [Thread-1 (]: SQL status: OK in 0.523 seconds
[0m14:39:27.260544 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:39:27.261094 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:39:27.261621 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:39:27.268919 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf4d9e73e0>]}
[0m14:39:27.269840 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.73s]
[0m14:39:27.270681 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:39:27.271258 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:39:27.272134 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:39:27.272980 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:39:27.273780 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:39:27.277339 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:39:27.278405 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:39:27.283617 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:39:27.284217 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:39:27.284761 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:39:27.458632 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:27.459310 [debug] [Thread-1 (]: SQL status: OK in 0.175 seconds
[0m14:39:27.463043 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:39:27.463991 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:27.464582 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:39:27.465125 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:39:27.906035 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:39:27.907066 [debug] [Thread-1 (]: SQL status: OK in 0.441 seconds
[0m14:39:27.911808 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:39:27.912665 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:39:27.913464 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:39:27.921909 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bb5b9aa-4c70-4137-95ee-24e70abb176b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf4fb32990>]}
[0m14:39:27.923344 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.65s]
[0m14:39:27.924687 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:39:27.926639 [debug] [MainThread]: On master: ROLLBACK
[0m14:39:27.927312 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:39:27.990881 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:39:27.991643 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:39:27.992204 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:39:27.992738 [debug] [MainThread]: On master: ROLLBACK
[0m14:39:27.993275 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:39:27.993825 [debug] [MainThread]: On master: Close
[0m14:39:28.002808 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:39:28.003420 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:39:28.004149 [info ] [MainThread]: 
[0m14:39:28.004877 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 9.19 seconds (9.19s).
[0m14:39:28.007790 [debug] [MainThread]: Command end result
[0m14:39:28.038322 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:39:28.040396 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:39:28.048470 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:39:28.049021 [info ] [MainThread]: 
[0m14:39:28.049746 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:39:28.050597 [info ] [MainThread]: 
[0m14:39:28.051354 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:39:28.052801 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.791072, "process_in_blocks": "768", "process_kernel_time": 0.142146, "process_mem_max_rss": "108892", "process_out_blocks": "2040", "process_user_time": 2.176243}
[0m14:39:28.053628 [debug] [MainThread]: Command `dbt run` succeeded at 14:39:28.053421 after 9.79 seconds
[0m14:39:28.054389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf5101e6f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf51394860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cbf50e1af60>]}
[0m14:39:28.055120 [debug] [MainThread]: Flushing usage events
[0m14:39:29.554114 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:41:45.013607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474d740c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474f5f4c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474e2095b0>]}


============================== 14:41:45.017037 | 6e14800a-856f-463f-a809-65f5d2d0102c ==============================
[0m14:41:45.017037 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:41:45.017992 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:41:45.084151 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:41:45.084682 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:41:45.085093 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:41:45.240943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474e1159d0>]}
[0m14:41:45.305827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474d2755b0>]}
[0m14:41:45.306563 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:41:45.388091 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:41:45.477858 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:41:45.478375 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:41:45.486764 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:41:45.511331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474cd51490>]}
[0m14:41:45.572217 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:41:45.575022 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:41:45.586590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474ce5cb30>]}
[0m14:41:45.587188 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:41:45.587635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474f0c1f70>]}
[0m14:41:45.590200 [info ] [MainThread]: 
[0m14:41:45.591139 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:41:45.591658 [info ] [MainThread]: 
[0m14:41:45.592385 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:41:45.599766 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:41:45.612336 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:41:45.612935 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:41:45.613396 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:41:48.618994 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:41:48.619858 [debug] [ThreadPool]: SQL status: OK in 3.006 seconds
[0m14:41:48.681456 [debug] [ThreadPool]: On list_schemas: Close
[0m14:41:48.709709 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:41:48.716214 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:48.716766 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:41:48.717211 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:41:48.717650 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:41:49.442591 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:41:49.443184 [debug] [ThreadPool]: SQL status: OK in 0.726 seconds
[0m14:41:49.449916 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:41:49.450434 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:41:49.450848 [debug] [ThreadPool]: On list_None_default: Close
[0m14:41:49.463879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474cb4ccb0>]}
[0m14:41:49.464519 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:49.464995 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:41:49.467414 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:41:49.468130 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:41:49.468807 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:41:49.469391 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:41:49.478956 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:41:49.479901 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:41:49.501097 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:41:49.501605 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:41:49.502048 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:50.181983 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:50.182616 [debug] [Thread-1 (]: SQL status: OK in 0.681 seconds
[0m14:41:50.235306 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:41:50.236179 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:50.236663 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:41:50.237176 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:41:52.738867 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:52.739737 [debug] [Thread-1 (]: SQL status: OK in 2.502 seconds
[0m14:41:52.773125 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:41:52.773840 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:41:52.774425 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:41:52.789915 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474cae1e20>]}
[0m14:41:52.790955 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.32s]
[0m14:41:52.791969 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:41:52.792730 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:41:52.794211 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:41:52.795251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:41:52.796105 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:41:52.801741 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:41:52.802991 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:41:52.809004 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:41:52.809708 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:41:52.810294 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:53.045547 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:53.046259 [debug] [Thread-1 (]: SQL status: OK in 0.236 seconds
[0m14:41:53.049999 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:41:53.051152 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:53.051820 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:41:53.052361 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:41:53.561626 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:53.562754 [debug] [Thread-1 (]: SQL status: OK in 0.510 seconds
[0m14:41:53.567652 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:41:53.568481 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:41:53.569271 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:41:53.583303 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474cac2f30>]}
[0m14:41:53.584751 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.79s]
[0m14:41:53.586103 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:41:53.587175 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:41:53.588599 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:41:53.590002 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:41:53.591096 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:41:53.596798 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:41:53.598085 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:41:53.606667 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:41:53.607645 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:41:53.608704 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:53.817012 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:53.817720 [debug] [Thread-1 (]: SQL status: OK in 0.209 seconds
[0m14:41:53.821021 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:41:53.821889 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:53.822389 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:41:53.822899 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:41:54.410424 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:54.411801 [debug] [Thread-1 (]: SQL status: OK in 0.588 seconds
[0m14:41:54.417805 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:41:54.418446 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:41:54.419132 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:41:54.428267 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474a9750d0>]}
[0m14:41:54.430274 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.84s]
[0m14:41:54.431912 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:41:54.433157 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:41:54.434300 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:41:54.435380 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:41:54.436321 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:41:54.440040 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:41:54.440954 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:41:54.449875 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:41:54.450738 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:41:54.451362 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:54.693661 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:54.694692 [debug] [Thread-1 (]: SQL status: OK in 0.243 seconds
[0m14:41:54.699218 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:41:54.700174 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:54.700769 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:41:54.701338 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:41:55.177192 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:55.177973 [debug] [Thread-1 (]: SQL status: OK in 0.476 seconds
[0m14:41:55.180994 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:41:55.181602 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:41:55.182126 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:41:55.189690 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474a974980>]}
[0m14:41:55.190633 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.75s]
[0m14:41:55.191703 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:41:55.192637 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:41:55.193472 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:41:55.194395 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:41:55.195585 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:41:55.199655 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:41:55.200499 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:41:55.205444 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:41:55.206023 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:41:55.206517 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:55.373687 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:55.374343 [debug] [Thread-1 (]: SQL status: OK in 0.168 seconds
[0m14:41:55.377654 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:41:55.378528 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:55.379107 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:41:55.379667 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:41:55.853233 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:41:55.854178 [debug] [Thread-1 (]: SQL status: OK in 0.474 seconds
[0m14:41:55.857383 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:41:55.857946 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:41:55.858435 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:41:55.866309 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6e14800a-856f-463f-a809-65f5d2d0102c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474cac26f0>]}
[0m14:41:55.867701 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.67s]
[0m14:41:55.869078 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:41:55.870944 [debug] [MainThread]: On master: ROLLBACK
[0m14:41:55.871501 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:41:55.930488 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:41:55.931511 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:41:55.932425 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:41:55.933301 [debug] [MainThread]: On master: ROLLBACK
[0m14:41:55.934176 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:41:55.934959 [debug] [MainThread]: On master: Close
[0m14:41:55.941272 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:41:55.942064 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:41:55.942871 [info ] [MainThread]: 
[0m14:41:55.943618 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 10.35 seconds (10.35s).
[0m14:41:55.946337 [debug] [MainThread]: Command end result
[0m14:41:55.978853 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:41:55.980496 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:41:55.986314 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:41:55.986708 [info ] [MainThread]: 
[0m14:41:55.987126 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:41:55.987496 [info ] [MainThread]: 
[0m14:41:55.987901 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:41:55.988777 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.034912, "process_in_blocks": "2952", "process_kernel_time": 0.175922, "process_mem_max_rss": "108540", "process_out_blocks": "2056", "process_user_time": 2.221022}
[0m14:41:55.989274 [debug] [MainThread]: Command `dbt run` succeeded at 14:41:55.989165 after 11.04 seconds
[0m14:41:55.989684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c47519b3a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474e0f0aa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c474e986030>]}
[0m14:41:55.990113 [debug] [MainThread]: Flushing usage events
[0m14:41:57.903507 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:43:17.658419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b0338eade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b033a1e420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b034493e00>]}


============================== 14:43:17.661676 | 99b82ecc-7a20-46ca-abd1-d384fa64abff ==============================
[0m14:43:17.661676 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:43:17.662328 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:43:17.722104 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:43:17.722888 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:43:17.723395 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:43:17.883249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b0343140e0>]}
[0m14:43:17.952545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b033142b70>]}
[0m14:43:17.953376 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:43:18.053959 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:43:18.176218 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:43:18.176873 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:43:18.187837 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:43:18.215193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b032edd820>]}
[0m14:43:18.292366 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:43:18.295267 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:43:18.307447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b032c05640>]}
[0m14:43:18.308123 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:43:18.308676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b034b85fd0>]}
[0m14:43:18.310912 [info ] [MainThread]: 
[0m14:43:18.311488 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:43:18.312032 [info ] [MainThread]: 
[0m14:43:18.312735 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:43:18.319575 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:43:18.332588 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:43:18.333151 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:43:18.333592 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:43:21.068039 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:43:21.068598 [debug] [ThreadPool]: SQL status: OK in 2.735 seconds
[0m14:43:21.130511 [debug] [ThreadPool]: On list_schemas: Close
[0m14:43:21.165693 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:43:21.174673 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:21.175323 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:43:21.175798 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:43:21.176226 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:43:21.813853 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:43:21.814403 [debug] [ThreadPool]: SQL status: OK in 0.638 seconds
[0m14:43:21.821351 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:43:21.821917 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:43:21.822307 [debug] [ThreadPool]: On list_None_default: Close
[0m14:43:21.836965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b0331ce300>]}
[0m14:43:21.837809 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:21.838280 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:43:21.840587 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:43:21.841267 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:43:21.841854 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:43:21.842327 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:43:21.849699 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:43:21.850474 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:43:21.874219 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:43:21.874838 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:43:21.875331 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:22.657866 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:22.658782 [debug] [Thread-1 (]: SQL status: OK in 0.783 seconds
[0m14:43:22.709881 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:43:22.710763 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:22.711254 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:43:22.711731 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:43:25.147771 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:25.148520 [debug] [Thread-1 (]: SQL status: OK in 2.436 seconds
[0m14:43:25.178425 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:43:25.179088 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:43:25.179669 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:43:25.193185 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b032cdeb40>]}
[0m14:43:25.194241 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.35s]
[0m14:43:25.195169 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:43:25.195833 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:43:25.196649 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:43:25.197354 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:43:25.197952 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:43:25.201213 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:43:25.202150 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:43:25.208070 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:43:25.208671 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:43:25.209182 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:25.450876 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:25.451969 [debug] [Thread-1 (]: SQL status: OK in 0.243 seconds
[0m14:43:25.462078 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:43:25.463689 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:25.464778 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:43:25.466134 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:43:26.155992 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:26.156925 [debug] [Thread-1 (]: SQL status: OK in 0.690 seconds
[0m14:43:26.161563 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:43:26.162629 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:43:26.163454 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:43:26.178861 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b032cc2690>]}
[0m14:43:26.179922 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.98s]
[0m14:43:26.181173 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:43:26.182159 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:43:26.183194 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:43:26.183981 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:43:26.184548 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:43:26.187845 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:43:26.188729 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:43:26.192874 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:43:26.193374 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:43:26.193853 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:26.404856 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:26.405635 [debug] [Thread-1 (]: SQL status: OK in 0.212 seconds
[0m14:43:26.408869 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:43:26.409688 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:26.410167 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:43:26.410672 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:43:26.951967 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:26.953549 [debug] [Thread-1 (]: SQL status: OK in 0.542 seconds
[0m14:43:26.965525 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:43:26.966898 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:43:26.968173 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:43:26.983019 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b030b740b0>]}
[0m14:43:26.984974 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.80s]
[0m14:43:26.987069 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:43:26.988471 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:43:26.990399 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:43:26.992248 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:43:26.993704 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:43:27.000338 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:43:27.002086 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:43:27.014869 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:43:27.016217 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:43:27.017473 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:27.300673 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:27.301957 [debug] [Thread-1 (]: SQL status: OK in 0.284 seconds
[0m14:43:27.308874 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:43:27.309858 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:27.310444 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:43:27.311004 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:43:27.797964 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:27.798662 [debug] [Thread-1 (]: SQL status: OK in 0.487 seconds
[0m14:43:27.801731 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:43:27.802286 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:43:27.802808 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:43:27.809704 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b030b77260>]}
[0m14:43:27.810500 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.82s]
[0m14:43:27.811198 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:43:27.811707 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:43:27.812304 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:43:27.812956 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:43:27.813432 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:43:27.816183 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:43:27.816913 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:43:27.821719 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:43:27.822211 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:43:27.822653 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:43:27.976899 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:27.977492 [debug] [Thread-1 (]: SQL status: OK in 0.155 seconds
[0m14:43:27.980488 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:43:27.981193 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:27.981621 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:43:27.982044 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:43:28.402195 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:43:28.403613 [debug] [Thread-1 (]: SQL status: OK in 0.421 seconds
[0m14:43:28.409627 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:43:28.410486 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:43:28.411259 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:43:28.420264 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b82ecc-7a20-46ca-abd1-d384fa64abff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b032cc2cf0>]}
[0m14:43:28.421465 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.61s]
[0m14:43:28.422954 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:43:28.425327 [debug] [MainThread]: On master: ROLLBACK
[0m14:43:28.426034 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:43:28.476020 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:43:28.476661 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:43:28.477186 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:43:28.477719 [debug] [MainThread]: On master: ROLLBACK
[0m14:43:28.478212 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:43:28.478689 [debug] [MainThread]: On master: Close
[0m14:43:28.484782 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:43:28.485520 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:43:28.486519 [info ] [MainThread]: 
[0m14:43:28.487261 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 10.17 seconds (10.17s).
[0m14:43:28.489693 [debug] [MainThread]: Command end result
[0m14:43:28.519223 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:43:28.522894 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:43:28.532469 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:43:28.533004 [info ] [MainThread]: 
[0m14:43:28.533545 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:43:28.534020 [info ] [MainThread]: 
[0m14:43:28.534520 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:43:28.535847 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.939462, "process_in_blocks": "0", "process_kernel_time": 0.174703, "process_mem_max_rss": "108724", "process_out_blocks": "2048", "process_user_time": 2.389942}
[0m14:43:28.536716 [debug] [MainThread]: Command `dbt run` succeeded at 14:43:28.536501 after 10.94 seconds
[0m14:43:28.537361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b0342f0710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b03386d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74b030b4bc50>]}
[0m14:43:28.538101 [debug] [MainThread]: Flushing usage events
[0m14:43:30.374157 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:44:00.707282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e84ec980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e86e35c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e70289b0>]}


============================== 14:44:00.710184 | 18018c77-10af-4b06-8d41-0c43e665c2d3 ==============================
[0m14:44:00.710184 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:44:00.710804 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:44:00.763928 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:44:00.764456 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:44:00.764887 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:44:00.907505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e87644d0>]}
[0m14:44:00.970192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e650b440>]}
[0m14:44:00.970846 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:44:01.051002 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:44:01.148727 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:44:01.149349 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:44:01.159025 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:44:01.181364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e623ddf0>]}
[0m14:44:01.242670 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:44:01.245074 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:44:01.257678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e61783e0>]}
[0m14:44:01.258181 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:44:01.258568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e678a8a0>]}
[0m14:44:01.260368 [info ] [MainThread]: 
[0m14:44:01.260763 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:44:01.261100 [info ] [MainThread]: 
[0m14:44:01.261592 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:44:01.269222 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:44:01.281163 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:44:01.281661 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:44:01.282045 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:05.248282 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:44:05.249552 [debug] [ThreadPool]: SQL status: OK in 3.967 seconds
[0m14:44:05.343105 [debug] [ThreadPool]: On list_schemas: Close
[0m14:44:05.373147 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:44:05.379438 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:05.379950 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:44:05.380383 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:44:05.380820 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:44:06.111962 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:44:06.112709 [debug] [ThreadPool]: SQL status: OK in 0.732 seconds
[0m14:44:06.122548 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:44:06.123261 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:44:06.123958 [debug] [ThreadPool]: On list_None_default: Close
[0m14:44:06.143165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e63e5640>]}
[0m14:44:06.144016 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:06.144651 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:44:06.147344 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:44:06.148287 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:44:06.149239 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:44:06.149888 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:44:06.162373 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:44:06.163375 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:44:06.196259 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:44:06.196903 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:44:06.197425 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:44:07.057088 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:07.058857 [debug] [Thread-1 (]: SQL status: OK in 0.861 seconds
[0m14:44:07.124366 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:44:07.125277 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:07.125873 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:44:07.126435 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:44:10.125347 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:10.126114 [debug] [Thread-1 (]: SQL status: OK in 2.999 seconds
[0m14:44:10.152890 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:44:10.153463 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:44:10.154003 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:44:10.167783 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e623dd30>]}
[0m14:44:10.168639 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 4.02s]
[0m14:44:10.169374 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:44:10.169915 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:44:10.170539 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:44:10.171308 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:44:10.171891 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:44:10.175389 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:44:10.176248 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:44:10.181591 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:44:10.182167 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:44:10.182752 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:44:10.380083 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:10.380778 [debug] [Thread-1 (]: SQL status: OK in 0.198 seconds
[0m14:44:10.384680 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:44:10.385599 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:10.386155 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:44:10.386691 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:44:10.931133 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:10.931967 [debug] [Thread-1 (]: SQL status: OK in 0.545 seconds
[0m14:44:10.935456 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:44:10.936272 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:44:10.936942 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:44:10.952044 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e464d880>]}
[0m14:44:10.953093 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.78s]
[0m14:44:10.954547 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:44:10.955724 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:44:10.958082 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:44:10.960325 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:44:10.961483 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:44:10.967727 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:44:10.968944 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:44:10.980698 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:44:10.981668 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:44:10.982906 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:44:11.266666 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:11.267611 [debug] [Thread-1 (]: SQL status: OK in 0.285 seconds
[0m14:44:11.273018 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:44:11.274697 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:11.276222 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:44:11.277559 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:44:11.890551 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:11.891189 [debug] [Thread-1 (]: SQL status: OK in 0.613 seconds
[0m14:44:11.896486 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:44:11.897100 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:44:11.897791 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:44:11.905847 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e468b020>]}
[0m14:44:11.906878 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.95s]
[0m14:44:11.907948 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:44:11.908558 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:44:11.909349 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:44:11.910132 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:44:11.910683 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:44:11.913557 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:44:11.914401 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:44:11.919083 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:44:11.919631 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:44:11.920085 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:44:12.132552 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:12.133210 [debug] [Thread-1 (]: SQL status: OK in 0.213 seconds
[0m14:44:12.137279 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:44:12.138369 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:12.138982 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:44:12.139555 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:44:12.713979 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:12.715023 [debug] [Thread-1 (]: SQL status: OK in 0.575 seconds
[0m14:44:12.719343 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:44:12.720165 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:44:12.720994 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:44:12.729777 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e4689f10>]}
[0m14:44:12.730773 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.82s]
[0m14:44:12.731913 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:44:12.732799 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:44:12.733588 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:44:12.734242 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:44:12.734894 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:44:12.738243 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:44:12.739174 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:44:12.744043 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:44:12.744686 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:44:12.745209 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:44:12.917323 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:12.917888 [debug] [Thread-1 (]: SQL status: OK in 0.173 seconds
[0m14:44:12.921380 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:44:12.922609 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:12.923410 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:44:12.924105 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:44:13.329117 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:44:13.329759 [debug] [Thread-1 (]: SQL status: OK in 0.405 seconds
[0m14:44:13.332374 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:44:13.332850 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:44:13.333278 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:44:13.340628 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18018c77-10af-4b06-8d41-0c43e665c2d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e5f35ac0>]}
[0m14:44:13.341620 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.61s]
[0m14:44:13.342434 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:44:13.343867 [debug] [MainThread]: On master: ROLLBACK
[0m14:44:13.344387 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:44:13.394080 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:44:13.394670 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:13.395142 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:44:13.395611 [debug] [MainThread]: On master: ROLLBACK
[0m14:44:13.396043 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:44:13.396457 [debug] [MainThread]: On master: Close
[0m14:44:13.402897 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:44:13.403622 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:44:13.404411 [info ] [MainThread]: 
[0m14:44:13.404944 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 12.14 seconds (12.14s).
[0m14:44:13.407633 [debug] [MainThread]: Command end result
[0m14:44:13.434228 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:44:13.435804 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:44:13.445205 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:44:13.445701 [info ] [MainThread]: 
[0m14:44:13.446197 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:44:13.446639 [info ] [MainThread]: 
[0m14:44:13.447062 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:44:13.448132 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.795421, "process_in_blocks": "8", "process_kernel_time": 0.173055, "process_mem_max_rss": "108568", "process_out_blocks": "2056", "process_user_time": 2.38076}
[0m14:44:13.448948 [debug] [MainThread]: Command `dbt run` succeeded at 14:44:13.448801 after 12.80 seconds
[0m14:44:13.449505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e6b40ce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e9038e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7223e7800050>]}
[0m14:44:13.450228 [debug] [MainThread]: Flushing usage events
[0m14:44:15.022721 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:45:30.748587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c9517a7170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94fbb87a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94fedeb40>]}


============================== 14:45:30.751803 | 23821d0e-33c5-4b1e-a85d-c09f6d347d6e ==============================
[0m14:45:30.751803 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:45:30.752350 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:45:30.806840 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:45:30.807324 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:45:30.807721 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:45:30.951910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c9504b19d0>]}
[0m14:45:31.012191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94f5eaa20>]}
[0m14:45:31.012887 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:45:31.081865 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:45:31.165048 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:45:31.165480 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:45:31.171960 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:45:31.193489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94f0cb650>]}
[0m14:45:31.245890 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:45:31.248448 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:45:31.258880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94f0032f0>]}
[0m14:45:31.259430 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:45:31.259949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94f045460>]}
[0m14:45:31.262421 [info ] [MainThread]: 
[0m14:45:31.262944 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:45:31.263320 [info ] [MainThread]: 
[0m14:45:31.263885 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:45:31.271315 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:45:31.284192 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:45:31.284793 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:45:31.285406 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:34.032455 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:45:34.033111 [debug] [ThreadPool]: SQL status: OK in 2.748 seconds
[0m14:45:34.096555 [debug] [ThreadPool]: On list_schemas: Close
[0m14:45:34.121743 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:45:34.127153 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:34.127532 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:45:34.127867 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:45:34.128175 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:45:34.763849 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:45:34.764452 [debug] [ThreadPool]: SQL status: OK in 0.636 seconds
[0m14:45:34.771201 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:45:34.771773 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:45:34.772213 [debug] [ThreadPool]: On list_None_default: Close
[0m14:45:34.785820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94f2306b0>]}
[0m14:45:34.786663 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:34.787265 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:45:34.791080 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:45:34.792067 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:45:34.792968 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:45:34.793799 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:45:34.802897 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:45:34.803637 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:45:34.823968 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:45:34.824442 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:45:34.824904 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:35.494132 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:35.494937 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m14:45:35.550715 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:45:35.551529 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:35.552011 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:45:35.552463 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:45:38.689677 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:38.690822 [debug] [Thread-1 (]: SQL status: OK in 3.138 seconds
[0m14:45:38.730770 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:45:38.731486 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:45:38.732129 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:45:38.745480 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94f5bbaa0>]}
[0m14:45:38.746598 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.95s]
[0m14:45:38.747621 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:45:38.748368 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:45:38.749369 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:45:38.750334 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:45:38.751132 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:45:38.756279 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:45:38.757595 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:45:38.764067 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:45:38.764879 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:45:38.765632 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:38.998781 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:38.999808 [debug] [Thread-1 (]: SQL status: OK in 0.234 seconds
[0m14:45:39.005550 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:45:39.006830 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:39.007713 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:45:39.008682 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:45:39.642287 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:39.643676 [debug] [Thread-1 (]: SQL status: OK in 0.634 seconds
[0m14:45:39.649993 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:45:39.651174 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:45:39.652258 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:45:39.665978 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94ee62180>]}
[0m14:45:39.667849 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.92s]
[0m14:45:39.669747 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:45:39.671133 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:45:39.673043 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:45:39.674624 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:45:39.675862 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:45:39.682592 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:45:39.684149 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:45:39.693745 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:45:39.694671 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:45:39.695492 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:39.880711 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:39.881638 [debug] [Thread-1 (]: SQL status: OK in 0.186 seconds
[0m14:45:39.886618 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:45:39.887774 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:39.888546 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:45:39.889242 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:45:40.330692 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:40.331599 [debug] [Thread-1 (]: SQL status: OK in 0.442 seconds
[0m14:45:40.338010 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:45:40.338787 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:45:40.339461 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:45:40.348204 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94cd17ad0>]}
[0m14:45:40.349625 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.67s]
[0m14:45:40.350938 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:45:40.351865 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:45:40.352978 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:45:40.354037 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:45:40.354924 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:45:40.359289 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:45:40.360354 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:45:40.367197 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:45:40.368000 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:45:40.368670 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:40.543964 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:40.544665 [debug] [Thread-1 (]: SQL status: OK in 0.176 seconds
[0m14:45:40.548294 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:45:40.549219 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:40.549771 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:45:40.550286 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:45:41.015410 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:41.016149 [debug] [Thread-1 (]: SQL status: OK in 0.465 seconds
[0m14:45:41.018852 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:45:41.019437 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:45:41.019968 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:45:41.028799 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94cce4140>]}
[0m14:45:41.029749 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.67s]
[0m14:45:41.030712 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:45:41.031397 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:45:41.032240 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:45:41.033177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:45:41.033992 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:45:41.038892 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:45:41.039875 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:45:41.044638 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:45:41.045256 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:45:41.045798 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:45:41.241431 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:41.242161 [debug] [Thread-1 (]: SQL status: OK in 0.196 seconds
[0m14:45:41.245826 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:45:41.246764 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:41.247322 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:45:41.247924 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:45:41.608253 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:45:41.608904 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m14:45:41.611758 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:45:41.612289 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:45:41.612773 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:45:41.619917 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23821d0e-33c5-4b1e-a85d-c09f6d347d6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94ee626c0>]}
[0m14:45:41.620838 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.59s]
[0m14:45:41.621708 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:45:41.623249 [debug] [MainThread]: On master: ROLLBACK
[0m14:45:41.623895 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:45:41.665867 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:45:41.666418 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:41.666910 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:45:41.667432 [debug] [MainThread]: On master: ROLLBACK
[0m14:45:41.667912 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:45:41.668321 [debug] [MainThread]: On master: Close
[0m14:45:41.674170 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:45:41.674713 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:45:41.675287 [info ] [MainThread]: 
[0m14:45:41.675905 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 10.41 seconds (10.41s).
[0m14:45:41.677847 [debug] [MainThread]: Command end result
[0m14:45:41.702901 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:45:41.704788 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:45:41.712234 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:45:41.712853 [info ] [MainThread]: 
[0m14:45:41.713636 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:45:41.714507 [info ] [MainThread]: 
[0m14:45:41.715195 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:45:41.716753 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.026857, "process_in_blocks": "40", "process_kernel_time": 0.160162, "process_mem_max_rss": "108636", "process_out_blocks": "2048", "process_user_time": 2.267142}
[0m14:45:41.717813 [debug] [MainThread]: Command `dbt run` succeeded at 14:45:41.717596 after 11.03 seconds
[0m14:45:41.718571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94f9cab10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c950f388f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x76c94fa444a0>]}
[0m14:45:41.719336 [debug] [MainThread]: Flushing usage events
[0m14:45:43.294848 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:48:16.228910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183c335c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183bf157f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183c38f560>]}


============================== 14:48:16.231786 | c5cf98c3-0778-42a1-954a-0a5e78002c55 ==============================
[0m14:48:16.231786 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:48:16.232399 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:48:16.286732 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:48:16.287383 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:48:16.287804 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:48:16.457388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183b64ca70>]}
[0m14:48:16.532888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183d111460>]}
[0m14:48:16.533678 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:48:16.619023 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:48:16.701245 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:48:16.701713 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:48:16.709262 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:48:16.729674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183afbdee0>]}
[0m14:48:16.784947 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:48:16.787827 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:48:16.798815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183ac90e60>]}
[0m14:48:16.799283 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:48:16.799705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183af95040>]}
[0m14:48:16.801506 [info ] [MainThread]: 
[0m14:48:16.801983 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:48:16.802411 [info ] [MainThread]: 
[0m14:48:16.803021 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:48:16.810151 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:48:16.821279 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:48:16.821749 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:48:16.822077 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:48:19.832273 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:48:19.833148 [debug] [ThreadPool]: SQL status: OK in 3.011 seconds
[0m14:48:19.890976 [debug] [ThreadPool]: On list_schemas: Close
[0m14:48:19.923482 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:48:19.929310 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:19.929758 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:48:19.930126 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:48:19.930476 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:48:20.632153 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:48:20.632689 [debug] [ThreadPool]: SQL status: OK in 0.702 seconds
[0m14:48:20.639611 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:48:20.640132 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:48:20.640578 [debug] [ThreadPool]: On list_None_default: Close
[0m14:48:20.653598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183d3d1100>]}
[0m14:48:20.654219 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:20.654683 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:48:20.657494 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:48:20.658279 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:48:20.658994 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:48:20.659566 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:48:20.667218 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:48:20.667922 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:48:20.694090 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:48:20.694656 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:48:20.695147 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:48:21.614443 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:21.615234 [debug] [Thread-1 (]: SQL status: OK in 0.920 seconds
[0m14:48:21.700888 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:48:21.702290 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:21.703467 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:48:21.704601 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:48:24.413678 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:24.414794 [debug] [Thread-1 (]: SQL status: OK in 2.709 seconds
[0m14:48:24.479828 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:48:24.481221 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:48:24.482162 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:48:24.516506 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183d3336e0>]}
[0m14:48:24.519822 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.86s]
[0m14:48:24.523055 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:48:24.524398 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:48:24.525815 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:48:24.528072 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:48:24.529379 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:48:24.536094 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:48:24.537580 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:48:24.547480 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:48:24.548397 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:48:24.549210 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:48:24.792068 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:24.793075 [debug] [Thread-1 (]: SQL status: OK in 0.244 seconds
[0m14:48:24.799198 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:48:24.800408 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:24.801279 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:48:24.802287 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:48:25.422856 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:25.423671 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m14:48:25.427255 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:48:25.427966 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:48:25.428565 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:48:25.455167 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183ab32990>]}
[0m14:48:25.457161 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.93s]
[0m14:48:25.458881 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:48:25.460104 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:48:25.461583 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:48:25.462953 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:48:25.464055 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:48:25.469696 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:48:25.471062 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:48:25.477145 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:48:25.477722 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:48:25.478211 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:48:25.732781 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:25.733752 [debug] [Thread-1 (]: SQL status: OK in 0.255 seconds
[0m14:48:25.740960 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:48:25.742378 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:25.743328 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:48:25.744238 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:48:26.357883 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:26.359061 [debug] [Thread-1 (]: SQL status: OK in 0.614 seconds
[0m14:48:26.367971 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:48:26.369167 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:48:26.370255 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:48:26.381378 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d18389e6810>]}
[0m14:48:26.383008 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.92s]
[0m14:48:26.384516 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:48:26.386027 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:48:26.387275 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:48:26.388474 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:48:26.390092 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:48:26.395359 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:48:26.396617 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:48:26.405670 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:48:26.406742 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:48:26.407699 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:48:26.581659 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:26.582272 [debug] [Thread-1 (]: SQL status: OK in 0.175 seconds
[0m14:48:26.585901 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:48:26.586897 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:26.587453 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:48:26.588054 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:48:27.003716 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:27.004369 [debug] [Thread-1 (]: SQL status: OK in 0.416 seconds
[0m14:48:27.007304 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:48:27.007847 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:48:27.008341 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:48:27.015485 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d18389e7b00>]}
[0m14:48:27.016339 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.63s]
[0m14:48:27.017114 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:48:27.017692 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:48:27.018408 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:48:27.019151 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:48:27.019716 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:48:27.022582 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:48:27.023328 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:48:27.027456 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:48:27.027963 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:48:27.028393 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:48:27.212924 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:27.213858 [debug] [Thread-1 (]: SQL status: OK in 0.185 seconds
[0m14:48:27.219127 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:48:27.221033 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:27.221936 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:48:27.222841 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:48:27.805527 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:48:27.806117 [debug] [Thread-1 (]: SQL status: OK in 0.583 seconds
[0m14:48:27.809174 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:48:27.809663 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:48:27.810091 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:48:27.816666 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c5cf98c3-0778-42a1-954a-0a5e78002c55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183ab32810>]}
[0m14:48:27.817638 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.80s]
[0m14:48:27.818997 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:48:27.820564 [debug] [MainThread]: On master: ROLLBACK
[0m14:48:27.821058 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:48:27.860252 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:48:27.860827 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:48:27.861294 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:48:27.861762 [debug] [MainThread]: On master: ROLLBACK
[0m14:48:27.862215 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:48:27.862649 [debug] [MainThread]: On master: Close
[0m14:48:27.868295 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:48:27.868897 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:48:27.869554 [info ] [MainThread]: 
[0m14:48:27.870054 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 11.07 seconds (11.07s).
[0m14:48:27.871593 [debug] [MainThread]: Command end result
[0m14:48:27.894197 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:48:27.895889 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:48:27.902503 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:48:27.903042 [info ] [MainThread]: 
[0m14:48:27.903562 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:48:27.903987 [info ] [MainThread]: 
[0m14:48:27.904435 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:48:27.905506 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.733472, "process_in_blocks": "0", "process_kernel_time": 0.178484, "process_mem_max_rss": "108880", "process_out_blocks": "2056", "process_user_time": 2.414027}
[0m14:48:27.906166 [debug] [MainThread]: Command `dbt run` succeeded at 14:48:27.906015 after 11.73 seconds
[0m14:48:27.906627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183bc35760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183ace7fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d183d684b60>]}
[0m14:48:27.907105 [debug] [MainThread]: Flushing usage events
[0m14:48:29.599744 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:48:56.676666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a177cbf80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a164d1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a15b06300>]}


============================== 14:48:56.679688 | 3c2357bc-6b6c-4710-b5cb-f05528cf6d74 ==============================
[0m14:48:56.679688 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:48:56.680615 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:48:56.739987 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:48:56.740484 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:48:56.740898 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:48:56.909525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a155a6240>]}
[0m14:48:56.974038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a151ce030>]}
[0m14:48:56.974696 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:48:57.056615 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:48:57.156863 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:48:57.157313 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:48:57.164560 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:48:57.188456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a14fe4fb0>]}
[0m14:48:57.250420 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:48:57.253696 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:48:57.265371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a14ba80b0>]}
[0m14:48:57.266118 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:48:57.266780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a14bbacc0>]}
[0m14:48:57.269185 [info ] [MainThread]: 
[0m14:48:57.269717 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:48:57.270153 [info ] [MainThread]: 
[0m14:48:57.270789 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:48:57.278057 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:48:57.292430 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:48:57.292952 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:48:57.293318 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:49:00.721560 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:49:00.722131 [debug] [ThreadPool]: SQL status: OK in 3.429 seconds
[0m14:49:00.785709 [debug] [ThreadPool]: On list_schemas: Close
[0m14:49:00.813128 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:49:00.819065 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:00.819552 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:49:00.819982 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:49:00.820370 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:49:01.762119 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:49:01.762934 [debug] [ThreadPool]: SQL status: OK in 0.943 seconds
[0m14:49:01.772772 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:49:01.773591 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:49:01.774273 [debug] [ThreadPool]: On list_None_default: Close
[0m14:49:01.794467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a14a78800>]}
[0m14:49:01.795189 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:01.795678 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:01.798936 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:49:01.799888 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:49:01.800655 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:49:01.801463 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:49:01.811200 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:49:01.812025 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:49:01.841498 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:49:01.842138 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:49:01.842620 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:02.667587 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:02.668450 [debug] [Thread-1 (]: SQL status: OK in 0.826 seconds
[0m14:49:02.722773 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:49:02.723741 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:02.724296 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:49:02.724846 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:49:05.194646 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:05.195598 [debug] [Thread-1 (]: SQL status: OK in 2.470 seconds
[0m14:49:05.238138 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:49:05.239199 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:05.240174 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:49:05.257934 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a14b53920>]}
[0m14:49:05.259574 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.46s]
[0m14:49:05.261267 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:49:05.262487 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:49:05.263940 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:49:05.264972 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:49:05.265753 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:49:05.269751 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:49:05.271007 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:49:05.277055 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:49:05.277832 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:49:05.278807 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:05.640227 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:05.641229 [debug] [Thread-1 (]: SQL status: OK in 0.362 seconds
[0m14:49:05.648062 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:49:05.649760 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:05.650699 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:49:05.651648 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:49:06.245048 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:06.245725 [debug] [Thread-1 (]: SQL status: OK in 0.593 seconds
[0m14:49:06.249613 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:49:06.250360 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:06.250980 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:49:06.263071 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a12888a70>]}
[0m14:49:06.264193 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 1.00s]
[0m14:49:06.265654 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:49:06.266353 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:49:06.267285 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:49:06.267979 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:49:06.268591 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:49:06.271704 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:49:06.272603 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:49:06.277521 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:49:06.278162 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:49:06.278703 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:06.491233 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:06.491895 [debug] [Thread-1 (]: SQL status: OK in 0.213 seconds
[0m14:49:06.495438 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:49:06.496506 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:06.497146 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:49:06.497959 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:49:07.109901 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:07.110560 [debug] [Thread-1 (]: SQL status: OK in 0.612 seconds
[0m14:49:07.117566 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:49:07.118562 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:07.119238 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:49:07.128011 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a128aff80>]}
[0m14:49:07.129017 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.86s]
[0m14:49:07.130191 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:49:07.130926 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:49:07.131860 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:49:07.132528 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:49:07.133087 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:49:07.136190 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:49:07.137012 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:49:07.142104 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:49:07.142831 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:49:07.143368 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:07.381413 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:07.382602 [debug] [Thread-1 (]: SQL status: OK in 0.239 seconds
[0m14:49:07.390415 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:49:07.392015 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:07.393406 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:49:07.394508 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:49:07.851777 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:07.852373 [debug] [Thread-1 (]: SQL status: OK in 0.457 seconds
[0m14:49:07.855143 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:49:07.855622 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:07.856062 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:49:07.863523 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a128ac7d0>]}
[0m14:49:07.864869 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.73s]
[0m14:49:07.865778 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:49:07.866446 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:49:07.867319 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:49:07.867976 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:49:07.868457 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:49:07.871455 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:49:07.872266 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:49:07.876419 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:49:07.876931 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:49:07.877414 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:08.042003 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:08.042644 [debug] [Thread-1 (]: SQL status: OK in 0.165 seconds
[0m14:49:08.046265 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:49:08.047458 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:08.048012 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:49:08.048509 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:49:08.612635 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:08.613503 [debug] [Thread-1 (]: SQL status: OK in 0.564 seconds
[0m14:49:08.617474 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:49:08.618040 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:08.618499 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:49:08.625283 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c2357bc-6b6c-4710-b5cb-f05528cf6d74', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a149feb40>]}
[0m14:49:08.626697 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.76s]
[0m14:49:08.627852 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:49:08.629256 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:08.629827 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:49:08.670960 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:49:08.671577 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:08.672075 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:08.672546 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:08.673010 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:49:08.673416 [debug] [MainThread]: On master: Close
[0m14:49:08.679260 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:49:08.680049 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:49:08.680919 [info ] [MainThread]: 
[0m14:49:08.681781 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 11.41 seconds (11.41s).
[0m14:49:08.684063 [debug] [MainThread]: Command end result
[0m14:49:08.719266 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:49:08.721488 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:49:08.730562 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:49:08.731376 [info ] [MainThread]: 
[0m14:49:08.732118 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:49:08.732720 [info ] [MainThread]: 
[0m14:49:08.733330 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:49:08.734782 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.1167755, "process_in_blocks": "0", "process_kernel_time": 0.191225, "process_mem_max_rss": "108884", "process_out_blocks": "2056", "process_user_time": 2.457043}
[0m14:49:08.735270 [debug] [MainThread]: Command `dbt run` succeeded at 14:49:08.735166 after 12.12 seconds
[0m14:49:08.735674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a15b06300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a1604ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x783a1589d790>]}
[0m14:49:08.736133 [debug] [MainThread]: Flushing usage events
[0m14:49:10.242762 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:49:42.271368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edf5f4ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edf6e7920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edeb89f10>]}


============================== 14:49:42.274733 | 3c96cd70-9c9d-41b8-b54b-81b3f6ab4356 ==============================
[0m14:49:42.274733 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:49:42.275714 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:49:42.337288 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:49:42.337826 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:49:42.338238 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:49:42.506985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edf5981d0>]}
[0m14:49:42.578324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edeeb0d40>]}
[0m14:49:42.579030 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:49:42.689479 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:49:42.784969 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:49:42.785500 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:49:42.794548 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:49:42.821819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edeaef4a0>]}
[0m14:49:42.885997 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:49:42.888541 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:49:42.899036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ede0e80e0>]}
[0m14:49:42.899513 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:49:42.899892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ede082060>]}
[0m14:49:42.901657 [info ] [MainThread]: 
[0m14:49:42.902077 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:49:42.902439 [info ] [MainThread]: 
[0m14:49:42.903011 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:49:42.909773 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:49:42.921989 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:49:42.922430 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:49:42.922795 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:49:46.108057 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:49:46.108708 [debug] [ThreadPool]: SQL status: OK in 3.186 seconds
[0m14:49:46.172906 [debug] [ThreadPool]: On list_schemas: Close
[0m14:49:46.203045 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:49:46.209613 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:46.210165 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:49:46.210653 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:49:46.211037 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:49:47.004260 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:49:47.004869 [debug] [ThreadPool]: SQL status: OK in 0.794 seconds
[0m14:49:47.012769 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:49:47.013486 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:49:47.014010 [debug] [ThreadPool]: On list_None_default: Close
[0m14:49:47.030656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ee0548cb0>]}
[0m14:49:47.031653 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:47.032395 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:47.035702 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:49:47.036814 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:49:47.037857 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:49:47.038671 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:49:47.052152 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:49:47.053133 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:49:47.079986 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:49:47.080632 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:49:47.081193 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:47.705771 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:47.706426 [debug] [Thread-1 (]: SQL status: OK in 0.625 seconds
[0m14:49:47.756948 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:49:47.758325 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:47.759008 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:49:47.759607 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:49:50.267680 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:50.268404 [debug] [Thread-1 (]: SQL status: OK in 2.508 seconds
[0m14:49:50.302088 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:49:50.302723 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:50.303230 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:49:50.317337 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2eddeabb90>]}
[0m14:49:50.318315 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.28s]
[0m14:49:50.319157 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:49:50.319756 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:49:50.320403 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:49:50.321193 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:49:50.321764 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:49:50.326177 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:49:50.327834 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:49:50.333204 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:49:50.333731 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:49:50.334186 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:50.637770 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:50.638622 [debug] [Thread-1 (]: SQL status: OK in 0.304 seconds
[0m14:49:50.645621 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:49:50.647440 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:50.648877 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:49:50.651763 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:49:51.544575 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:51.545524 [debug] [Thread-1 (]: SQL status: OK in 0.893 seconds
[0m14:49:51.548840 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:49:51.549361 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:51.549975 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:49:51.563159 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2eddf471a0>]}
[0m14:49:51.564077 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 1.24s]
[0m14:49:51.565045 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:49:51.565738 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:49:51.566734 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:49:51.567458 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:49:51.568033 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:49:51.571001 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:49:51.571742 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:49:51.578818 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:49:51.579608 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:49:51.580230 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:51.867487 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:51.868471 [debug] [Thread-1 (]: SQL status: OK in 0.288 seconds
[0m14:49:51.874694 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:49:51.876160 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:51.877039 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:49:51.877930 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:49:52.479915 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:52.480914 [debug] [Thread-1 (]: SQL status: OK in 0.602 seconds
[0m14:49:52.487813 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:49:52.488706 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:52.489524 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:49:52.503610 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edc5fadb0>]}
[0m14:49:52.504889 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.94s]
[0m14:49:52.506245 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:49:52.507268 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:49:52.508734 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:49:52.509819 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:49:52.510780 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:49:52.515638 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:49:52.516786 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:49:52.522799 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:49:52.523701 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:49:52.524695 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:52.687654 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:52.688344 [debug] [Thread-1 (]: SQL status: OK in 0.164 seconds
[0m14:49:52.692279 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:49:52.693274 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:52.693766 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:49:52.694222 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:49:53.318276 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:53.318996 [debug] [Thread-1 (]: SQL status: OK in 0.624 seconds
[0m14:49:53.322273 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:49:53.322993 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:53.323732 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:49:53.335948 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edc5f9df0>]}
[0m14:49:53.336903 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.83s]
[0m14:49:53.337778 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:49:53.338385 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:49:53.339344 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:49:53.340325 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:49:53.341263 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:49:53.345301 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:49:53.346356 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:49:53.352108 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:49:53.352835 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:49:53.353456 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:53.533508 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:53.534585 [debug] [Thread-1 (]: SQL status: OK in 0.181 seconds
[0m14:49:53.540178 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:49:53.541433 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:53.542279 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:49:53.543107 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:49:54.071692 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:54.072338 [debug] [Thread-1 (]: SQL status: OK in 0.529 seconds
[0m14:49:54.076148 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:49:54.076704 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:54.077248 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:49:54.084788 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c96cd70-9c9d-41b8-b54b-81b3f6ab4356', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edc601160>]}
[0m14:49:54.085877 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.74s]
[0m14:49:54.086810 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:49:54.088365 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:54.088916 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:49:54.135283 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:49:54.136160 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:54.136852 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:54.137498 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:54.138141 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:49:54.138741 [debug] [MainThread]: On master: Close
[0m14:49:54.145933 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:49:54.146625 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:49:54.147251 [info ] [MainThread]: 
[0m14:49:54.147873 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 11.24 seconds (11.24s).
[0m14:49:54.149583 [debug] [MainThread]: Command end result
[0m14:49:54.179480 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:49:54.181553 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:49:54.188485 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:49:54.188958 [info ] [MainThread]: 
[0m14:49:54.189523 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:49:54.190221 [info ] [MainThread]: 
[0m14:49:54.190872 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:49:54.192396 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.984396, "process_in_blocks": "8", "process_kernel_time": 0.192322, "process_mem_max_rss": "108916", "process_out_blocks": "2056", "process_user_time": 2.420473}
[0m14:49:54.192994 [debug] [MainThread]: Command `dbt run` succeeded at 14:49:54.192869 after 11.99 seconds
[0m14:49:54.193444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edeaca150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2edfe35880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2ede005460>]}
[0m14:49:54.193915 [debug] [MainThread]: Flushing usage events
[0m14:49:56.214381 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:24.308335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eeffc51f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765ef1d94080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eeff11040>]}


============================== 14:50:24.311566 | afc14e84-a458-4fc8-9ec8-c1679a89f844 ==============================
[0m14:50:24.311566 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:50:24.312296 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:50:24.375677 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:50:24.376265 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:50:24.376693 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:50:24.643860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eef46fda0>]}
[0m14:50:24.760213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eef401b20>]}
[0m14:50:24.761208 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:50:24.882045 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:50:25.019725 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:50:25.020267 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:50:25.028548 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:50:25.053817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eefa47ec0>]}
[0m14:50:25.128570 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:50:25.131640 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:50:25.146429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eef0d0920>]}
[0m14:50:25.147150 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:50:25.147592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eef1c5040>]}
[0m14:50:25.149740 [info ] [MainThread]: 
[0m14:50:25.150219 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:50:25.150652 [info ] [MainThread]: 
[0m14:50:25.151286 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:50:25.167566 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:50:25.187510 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:50:25.188742 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:50:25.190186 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:50:28.793514 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:50:28.794209 [debug] [ThreadPool]: SQL status: OK in 3.604 seconds
[0m14:50:28.872595 [debug] [ThreadPool]: On list_schemas: Close
[0m14:50:28.907946 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:50:28.915060 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:28.915635 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:50:28.916136 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:50:28.916628 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:50:29.821339 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:50:29.822113 [debug] [ThreadPool]: SQL status: OK in 0.905 seconds
[0m14:50:29.829835 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:50:29.830543 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:50:29.831096 [debug] [ThreadPool]: On list_None_default: Close
[0m14:50:29.847224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eef305d60>]}
[0m14:50:29.848310 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:29.849136 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:50:29.852644 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:50:29.853876 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:50:29.854977 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:50:29.855950 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:50:29.871387 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:50:29.872722 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:50:29.907211 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:50:29.907818 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:50:29.908314 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:30.621478 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:30.622867 [debug] [Thread-1 (]: SQL status: OK in 0.714 seconds
[0m14:50:30.718623 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:50:30.719995 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:30.721035 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:50:30.722247 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:50:33.219782 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:33.220595 [debug] [Thread-1 (]: SQL status: OK in 2.497 seconds
[0m14:50:33.249994 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:50:33.250649 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:50:33.251197 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:50:33.264434 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eeef264b0>]}
[0m14:50:33.265588 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.41s]
[0m14:50:33.266583 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:50:33.267275 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:50:33.268159 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:50:33.268883 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:50:33.269472 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:50:33.272815 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:50:33.273684 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:50:33.278739 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:50:33.279361 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:50:33.279924 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:33.558078 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:33.559125 [debug] [Thread-1 (]: SQL status: OK in 0.279 seconds
[0m14:50:33.564854 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:50:33.565770 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:33.566299 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:50:33.566788 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:50:34.112060 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:34.112981 [debug] [Thread-1 (]: SQL status: OK in 0.546 seconds
[0m14:50:34.116955 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:50:34.117669 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:50:34.118338 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:50:34.127525 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eeef271a0>]}
[0m14:50:34.128709 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.86s]
[0m14:50:34.129737 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:50:34.130472 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:50:34.131265 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:50:34.132106 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:50:34.132795 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:50:34.136490 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:50:34.137377 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:50:34.143303 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:50:34.144000 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:50:34.144596 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:34.311470 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:34.312034 [debug] [Thread-1 (]: SQL status: OK in 0.167 seconds
[0m14:50:34.315006 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:50:34.315739 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:34.316207 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:50:34.316667 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:50:34.717440 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:34.718074 [debug] [Thread-1 (]: SQL status: OK in 0.401 seconds
[0m14:50:34.722260 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:50:34.722769 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:50:34.723216 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:50:34.731354 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eecdd5c10>]}
[0m14:50:34.732531 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.60s]
[0m14:50:34.733701 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:50:34.734236 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:50:34.734899 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:50:34.735471 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:50:34.735992 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:50:34.738530 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:50:34.739258 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:50:34.743463 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:50:34.743956 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:50:34.744376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:34.907821 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:34.908726 [debug] [Thread-1 (]: SQL status: OK in 0.164 seconds
[0m14:50:34.912502 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:50:34.913331 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:34.913848 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:50:34.914328 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:50:35.531389 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:35.532587 [debug] [Thread-1 (]: SQL status: OK in 0.618 seconds
[0m14:50:35.537161 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:50:35.537999 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:50:35.538729 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:50:35.549386 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eecdd5af0>]}
[0m14:50:35.551234 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.81s]
[0m14:50:35.553012 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:50:35.554226 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:50:35.555559 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:50:35.556654 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:50:35.557628 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:50:35.562846 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:50:35.564082 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:50:35.571576 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:50:35.572415 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:50:35.573175 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:35.745779 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:35.746448 [debug] [Thread-1 (]: SQL status: OK in 0.173 seconds
[0m14:50:35.749717 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:50:35.750772 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:35.751415 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:50:35.752110 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:50:36.160928 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:50:36.161610 [debug] [Thread-1 (]: SQL status: OK in 0.409 seconds
[0m14:50:36.164726 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:50:36.165281 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:50:36.165801 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:50:36.176335 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'afc14e84-a458-4fc8-9ec8-c1679a89f844', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eeef26750>]}
[0m14:50:36.177619 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.62s]
[0m14:50:36.178842 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:50:36.180407 [debug] [MainThread]: On master: ROLLBACK
[0m14:50:36.180930 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:50:36.225709 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:50:36.226261 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:50:36.226715 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:50:36.227137 [debug] [MainThread]: On master: ROLLBACK
[0m14:50:36.227570 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:50:36.227980 [debug] [MainThread]: On master: Close
[0m14:50:36.234194 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:50:36.234849 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:50:36.235401 [info ] [MainThread]: 
[0m14:50:36.236038 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 11.08 seconds (11.08s).
[0m14:50:36.237808 [debug] [MainThread]: Command end result
[0m14:50:36.260974 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:50:36.262677 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:50:36.270204 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:50:36.270756 [info ] [MainThread]: 
[0m14:50:36.271393 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:50:36.272065 [info ] [MainThread]: 
[0m14:50:36.272693 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:50:36.273845 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.032213, "process_in_blocks": "232", "process_kernel_time": 0.225314, "process_mem_max_rss": "108732", "process_out_blocks": "2080", "process_user_time": 3.130482}
[0m14:50:36.274427 [debug] [MainThread]: Command `dbt run` succeeded at 14:50:36.274299 after 12.03 seconds
[0m14:50:36.274937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765ef332c140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eeefc08f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x765eef0e3aa0>]}
[0m14:50:36.275435 [debug] [MainThread]: Flushing usage events
[0m14:50:37.885723 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:04.445783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed37e2000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed3cd2600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed37e09e0>]}


============================== 14:53:04.448976 | 29ff04bb-303d-4243-89c5-546acd3b5350 ==============================
[0m14:53:04.448976 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:53:04.449785 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:53:04.505721 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:53:04.506389 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:53:04.506993 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:53:04.673205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed3d20050>]}
[0m14:53:04.738475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed3d20740>]}
[0m14:53:04.739121 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:53:04.822085 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:53:04.898704 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:53:04.899140 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:53:04.905517 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:53:04.925505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed323bb90>]}
[0m14:53:04.978157 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:53:04.980867 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:53:04.991123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed26347a0>]}
[0m14:53:04.991794 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:53:04.992181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed3313860>]}
[0m14:53:04.994479 [info ] [MainThread]: 
[0m14:53:04.995109 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:53:04.995547 [info ] [MainThread]: 
[0m14:53:04.996119 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:53:05.002345 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:53:05.013119 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:53:05.013636 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:53:05.013993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:53:07.453469 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:53:07.454051 [debug] [ThreadPool]: SQL status: OK in 2.440 seconds
[0m14:53:07.501247 [debug] [ThreadPool]: On list_schemas: Close
[0m14:53:07.521797 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:53:07.526884 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:07.527264 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:53:07.527584 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:53:07.527891 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:53:08.229983 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:53:08.230687 [debug] [ThreadPool]: SQL status: OK in 0.703 seconds
[0m14:53:08.238604 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:53:08.239238 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:53:08.239808 [debug] [ThreadPool]: On list_None_default: Close
[0m14:53:08.255773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed275da90>]}
[0m14:53:08.256882 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:08.257756 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:53:08.261221 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:53:08.262077 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:53:08.262771 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:53:08.263336 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:53:08.275250 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:53:08.276134 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:53:08.301555 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:53:08.302132 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:53:08.302620 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:08.999053 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:08.999777 [debug] [Thread-1 (]: SQL status: OK in 0.697 seconds
[0m14:53:09.041448 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:53:09.042330 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:09.042804 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:53:09.043324 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:53:11.444548 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:11.445311 [debug] [Thread-1 (]: SQL status: OK in 2.401 seconds
[0m14:53:11.478132 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:53:11.478853 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:53:11.479451 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:53:11.491454 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed4e13140>]}
[0m14:53:11.492557 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.23s]
[0m14:53:11.493504 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:53:11.494349 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:53:11.495603 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:53:11.496675 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:53:11.497636 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:53:11.501794 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:53:11.502723 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:53:11.508420 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:53:11.509157 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:53:11.509786 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:11.762615 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:11.763763 [debug] [Thread-1 (]: SQL status: OK in 0.254 seconds
[0m14:53:11.770524 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:53:11.772167 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:11.773202 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:53:11.774275 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:53:12.487555 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:12.488697 [debug] [Thread-1 (]: SQL status: OK in 0.713 seconds
[0m14:53:12.492249 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:53:12.492900 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:53:12.493599 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:53:12.506243 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed26d23f0>]}
[0m14:53:12.507617 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 1.01s]
[0m14:53:12.509003 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:53:12.510127 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:53:12.511549 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:53:12.512558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:53:12.513321 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:53:12.517240 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:53:12.518118 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:53:12.523163 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:53:12.523806 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:53:12.524559 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:12.774360 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:12.775415 [debug] [Thread-1 (]: SQL status: OK in 0.251 seconds
[0m14:53:12.779082 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:53:12.779941 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:12.780450 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:53:12.781210 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:53:13.313397 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:13.314457 [debug] [Thread-1 (]: SQL status: OK in 0.532 seconds
[0m14:53:13.325938 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:53:13.327073 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:53:13.328206 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:53:13.343393 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed0584c50>]}
[0m14:53:13.345191 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.83s]
[0m14:53:13.346945 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:53:13.348124 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:53:13.349436 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:53:13.350855 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:53:13.352497 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:53:13.358598 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:53:13.361105 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:53:13.371100 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:53:13.372363 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:53:13.373573 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:13.625776 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:13.626638 [debug] [Thread-1 (]: SQL status: OK in 0.253 seconds
[0m14:53:13.630797 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:53:13.631939 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:13.632590 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:53:13.633212 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:53:14.154966 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:14.156024 [debug] [Thread-1 (]: SQL status: OK in 0.522 seconds
[0m14:53:14.160560 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:53:14.161393 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:53:14.162270 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:53:14.172521 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed0585730>]}
[0m14:53:14.173938 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.82s]
[0m14:53:14.175267 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:53:14.176361 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:53:14.177810 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:53:14.178943 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:53:14.179698 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:53:14.184482 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:53:14.185649 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:53:14.193482 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:53:14.194486 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:53:14.195417 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:53:14.388319 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:14.389061 [debug] [Thread-1 (]: SQL status: OK in 0.194 seconds
[0m14:53:14.392821 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:53:14.393991 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:14.394704 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:53:14.395362 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:53:14.777678 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:53:14.778441 [debug] [Thread-1 (]: SQL status: OK in 0.382 seconds
[0m14:53:14.782409 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:53:14.783075 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:53:14.784028 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:53:14.794522 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '29ff04bb-303d-4243-89c5-546acd3b5350', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed26d2de0>]}
[0m14:53:14.796266 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.62s]
[0m14:53:14.797845 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:53:14.799817 [debug] [MainThread]: On master: ROLLBACK
[0m14:53:14.800846 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:53:14.860766 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:53:14.861361 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:53:14.861894 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:53:14.862404 [debug] [MainThread]: On master: ROLLBACK
[0m14:53:14.862954 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:53:14.863417 [debug] [MainThread]: On master: Close
[0m14:53:14.870409 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:53:14.871135 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:53:14.871756 [info ] [MainThread]: 
[0m14:53:14.872382 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 9.88 seconds (9.88s).
[0m14:53:14.874344 [debug] [MainThread]: Command end result
[0m14:53:14.905311 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:53:14.907490 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:53:14.919680 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:53:14.920459 [info ] [MainThread]: 
[0m14:53:14.921369 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:53:14.922271 [info ] [MainThread]: 
[0m14:53:14.923204 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:53:14.925209 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.536183, "process_in_blocks": "0", "process_kernel_time": 0.145417, "process_mem_max_rss": "108720", "process_out_blocks": "2040", "process_user_time": 2.159344}
[0m14:53:14.926301 [debug] [MainThread]: Command `dbt run` succeeded at 14:53:14.926048 after 10.54 seconds
[0m14:53:14.927403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed5190950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed3cfcad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x743ed2733470>]}
[0m14:53:14.928644 [debug] [MainThread]: Flushing usage events
[0m14:53:16.695446 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:55:44.967373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f457b2c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f455f5640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f47210770>]}


============================== 14:55:44.970316 | 7d2b377c-b97f-49d9-b1fd-ce815be107d4 ==============================
[0m14:55:44.970316 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:55:44.971002 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m14:55:45.030428 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:55:45.030895 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:55:45.031263 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:55:45.199493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f45d19ac0>]}
[0m14:55:45.289418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f44a6dd00>]}
[0m14:55:45.290165 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:55:45.397938 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:55:45.495219 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:55:45.495987 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:55:45.503804 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:55:45.533209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f44ac5f10>]}
[0m14:55:45.623825 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:55:45.629006 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:55:45.646478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f44891d30>]}
[0m14:55:45.647281 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:55:45.647978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f44850260>]}
[0m14:55:45.651043 [info ] [MainThread]: 
[0m14:55:45.651703 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:55:45.652313 [info ] [MainThread]: 
[0m14:55:45.653093 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:55:45.666454 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:55:45.683305 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:55:45.683854 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:55:45.684254 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:55:49.317264 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:55:49.317878 [debug] [ThreadPool]: SQL status: OK in 3.634 seconds
[0m14:55:49.379187 [debug] [ThreadPool]: On list_schemas: Close
[0m14:55:49.408260 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:55:49.413939 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:49.414347 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:55:49.414704 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:55:49.415040 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:55:50.143726 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:55:50.144525 [debug] [ThreadPool]: SQL status: OK in 0.729 seconds
[0m14:55:50.153900 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:55:50.154497 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:55:50.154983 [debug] [ThreadPool]: On list_None_default: Close
[0m14:55:50.172365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f44720680>]}
[0m14:55:50.173027 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:50.173472 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:55:50.176635 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:55:50.177483 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:55:50.178326 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:55:50.178966 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:55:50.188257 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:55:50.189088 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:55:50.218403 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:55:50.218954 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:55:50.219384 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:51.112727 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:51.113515 [debug] [Thread-1 (]: SQL status: OK in 0.894 seconds
[0m14:55:51.203349 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:55:51.204793 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:51.205582 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:55:51.206290 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:55:54.347739 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:54.348432 [debug] [Thread-1 (]: SQL status: OK in 3.141 seconds
[0m14:55:54.380037 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:55:54.380794 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:55:54.381445 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:55:54.396309 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f44918bf0>]}
[0m14:55:54.397338 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 4.22s]
[0m14:55:54.398806 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:55:54.399583 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:55:54.400431 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:55:54.401236 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:55:54.401930 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:55:54.405630 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:55:54.406908 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:55:54.413368 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:55:54.414207 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:55:54.415010 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:54.655144 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:54.655912 [debug] [Thread-1 (]: SQL status: OK in 0.241 seconds
[0m14:55:54.662920 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:55:54.664030 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:54.664746 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:55:54.665478 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:55:55.312018 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:55.313305 [debug] [Thread-1 (]: SQL status: OK in 0.647 seconds
[0m14:55:55.317189 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:55:55.317930 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:55:55.318655 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:55:55.334325 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f42518fe0>]}
[0m14:55:55.335394 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.93s]
[0m14:55:55.336644 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:55:55.337458 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:55:55.338298 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:55:55.339048 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:55:55.339699 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:55:55.345857 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:55:55.347117 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:55:55.352601 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:55:55.353311 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:55:55.353916 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:55.601171 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:55.602140 [debug] [Thread-1 (]: SQL status: OK in 0.248 seconds
[0m14:55:55.608042 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:55:55.609330 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:55.610157 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:55:55.611532 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:55:56.207812 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:56.208629 [debug] [Thread-1 (]: SQL status: OK in 0.596 seconds
[0m14:55:56.213216 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:55:56.213805 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:55:56.214270 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:55:56.221393 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f425560f0>]}
[0m14:55:56.222328 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.88s]
[0m14:55:56.223449 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:55:56.224304 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:55:56.225351 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:55:56.226145 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:55:56.226751 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:55:56.229396 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:55:56.230204 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:55:56.234464 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:55:56.235054 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:55:56.235572 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:56.429814 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:56.430444 [debug] [Thread-1 (]: SQL status: OK in 0.195 seconds
[0m14:55:56.434354 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:55:56.435378 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:56.435998 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:55:56.436620 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:55:56.915668 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:56.916383 [debug] [Thread-1 (]: SQL status: OK in 0.479 seconds
[0m14:55:56.919662 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:55:56.920245 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:55:56.920761 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:55:56.930507 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f425560f0>]}
[0m14:55:56.931565 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.70s]
[0m14:55:56.932466 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:55:56.933118 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:55:56.933937 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:55:56.934656 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:55:56.935351 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:55:56.938697 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:55:56.939561 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:55:56.945347 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:55:56.946094 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:55:56.946646 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:55:57.147164 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:57.147860 [debug] [Thread-1 (]: SQL status: OK in 0.201 seconds
[0m14:55:57.151760 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:55:57.152668 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:57.153186 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:55:57.153676 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:55:57.688002 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:55:57.689196 [debug] [Thread-1 (]: SQL status: OK in 0.535 seconds
[0m14:55:57.694827 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:55:57.695791 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:55:57.696838 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:55:57.709651 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7d2b377c-b97f-49d9-b1fd-ce815be107d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f446a6300>]}
[0m14:55:57.711476 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.77s]
[0m14:55:57.713933 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:55:57.716035 [debug] [MainThread]: On master: ROLLBACK
[0m14:55:57.716681 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:55:57.794660 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:55:57.795831 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:55:57.796782 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:55:57.797627 [debug] [MainThread]: On master: ROLLBACK
[0m14:55:57.798599 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:55:57.799361 [debug] [MainThread]: On master: Close
[0m14:55:57.810205 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:55:57.811622 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:55:57.813047 [info ] [MainThread]: 
[0m14:55:57.814253 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 12.16 seconds (12.16s).
[0m14:55:57.817901 [debug] [MainThread]: Command end result
[0m14:55:57.859548 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:55:57.862528 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:55:57.869687 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:55:57.870161 [info ] [MainThread]: 
[0m14:55:57.870634 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:55:57.871056 [info ] [MainThread]: 
[0m14:55:57.871499 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:55:57.872598 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.970707, "process_in_blocks": "1632", "process_kernel_time": 0.193282, "process_mem_max_rss": "108588", "process_out_blocks": "2072", "process_user_time": 2.485774}
[0m14:55:57.873343 [debug] [MainThread]: Command `dbt run` succeeded at 14:55:57.873159 after 12.97 seconds
[0m14:55:57.874128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f44b1ac30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f4524ac90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x780f45308dd0>]}
[0m14:55:57.874794 [debug] [MainThread]: Flushing usage events
[0m14:55:59.607032 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:56:26.481997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c226480a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c225d9ca40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c225d9e9c0>]}


============================== 14:56:26.486232 | 5d3fd275-5617-4aa6-9267-6424774ea53c ==============================
[0m14:56:26.486232 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:56:26.487097 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:56:26.555231 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:56:26.555859 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:56:26.556368 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:56:26.763971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c22518e120>]}
[0m14:56:26.857862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c2252ac6b0>]}
[0m14:56:26.859197 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:56:27.004275 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:56:27.112855 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:56:27.113652 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:56:27.123148 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:56:27.150061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c2250657c0>]}
[0m14:56:27.238197 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:56:27.241599 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:56:27.260621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c22518d010>]}
[0m14:56:27.261327 [info ] [MainThread]: Found 5 models, 473 macros
[0m14:56:27.261940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c2252ba8a0>]}
[0m14:56:27.264790 [info ] [MainThread]: 
[0m14:56:27.265425 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:56:27.265996 [info ] [MainThread]: 
[0m14:56:27.266982 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:56:27.278898 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:56:27.305579 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:56:27.306179 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:56:27.306661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:56:31.051208 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:56:31.051835 [debug] [ThreadPool]: SQL status: OK in 3.745 seconds
[0m14:56:31.142681 [debug] [ThreadPool]: On list_schemas: Close
[0m14:56:31.171815 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:56:31.178324 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:31.178858 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:56:31.179319 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:56:31.179776 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:56:31.908306 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:56:31.909353 [debug] [ThreadPool]: SQL status: OK in 0.729 seconds
[0m14:56:31.918128 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:56:31.918912 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:56:31.919423 [debug] [ThreadPool]: On list_None_default: Close
[0m14:56:31.934233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c224e700e0>]}
[0m14:56:31.934877 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:31.935340 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:56:31.938340 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:56:31.939173 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:56:31.939874 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:56:31.940381 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:56:31.949291 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:56:31.950052 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:56:31.979910 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:56:31.980849 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:56:31.981386 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:32.776289 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:32.777289 [debug] [Thread-1 (]: SQL status: OK in 0.796 seconds
[0m14:56:32.838992 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:56:32.840189 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:32.840914 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:56:32.841596 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:56:36.263311 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:36.264043 [debug] [Thread-1 (]: SQL status: OK in 3.422 seconds
[0m14:56:36.288366 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:56:36.289088 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:56:36.289672 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:56:36.308560 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c22503e780>]}
[0m14:56:36.310079 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 4.37s]
[0m14:56:36.311548 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:56:36.312588 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:56:36.314258 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:56:36.316018 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:56:36.317053 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:56:36.322984 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:56:36.324372 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:56:36.334076 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:56:36.335341 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:56:36.336584 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:36.552411 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:36.552998 [debug] [Thread-1 (]: SQL status: OK in 0.217 seconds
[0m14:56:36.556271 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:56:36.557056 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:36.557523 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:56:36.557972 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:56:37.039295 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:37.039890 [debug] [Thread-1 (]: SQL status: OK in 0.481 seconds
[0m14:56:37.042431 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:56:37.042953 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:56:37.043387 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:56:37.052708 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c224dd6030>]}
[0m14:56:37.053566 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.74s]
[0m14:56:37.054346 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:56:37.054925 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:56:37.055669 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:56:37.056326 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:56:37.056957 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:56:37.061787 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:56:37.062687 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:56:37.067567 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:56:37.068273 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:56:37.068776 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:37.280489 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:37.281096 [debug] [Thread-1 (]: SQL status: OK in 0.212 seconds
[0m14:56:37.284160 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:56:37.284910 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:37.285354 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:56:37.285825 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:56:37.746292 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:37.746872 [debug] [Thread-1 (]: SQL status: OK in 0.461 seconds
[0m14:56:37.750797 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:56:37.751271 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:56:37.751712 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:56:37.758491 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c22448f8f0>]}
[0m14:56:37.759412 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.70s]
[0m14:56:37.760235 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:56:37.760934 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:56:37.761696 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:56:37.762344 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:56:37.762880 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:56:37.765432 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:56:37.766209 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:56:37.770367 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:56:37.770925 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:56:37.771409 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:37.933278 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:37.933936 [debug] [Thread-1 (]: SQL status: OK in 0.163 seconds
[0m14:56:37.937419 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:56:37.938183 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:37.938733 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:56:37.939260 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:56:38.379969 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:38.380794 [debug] [Thread-1 (]: SQL status: OK in 0.441 seconds
[0m14:56:38.384043 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:56:38.384660 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:56:38.385243 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:56:38.395405 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c22448cfe0>]}
[0m14:56:38.396496 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.63s]
[0m14:56:38.397692 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:56:38.398816 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:56:38.399711 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:56:38.400373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:56:38.400967 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:56:38.404293 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:56:38.405045 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:56:38.411164 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:56:38.411885 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:56:38.412582 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:56:38.586509 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:38.587468 [debug] [Thread-1 (]: SQL status: OK in 0.175 seconds
[0m14:56:38.590464 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:56:38.591171 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:38.591652 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:56:38.592118 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:56:39.013774 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:56:39.014425 [debug] [Thread-1 (]: SQL status: OK in 0.422 seconds
[0m14:56:39.017479 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:56:39.018026 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:56:39.018522 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:56:39.025677 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d3fd275-5617-4aa6-9267-6424774ea53c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c224dd6480>]}
[0m14:56:39.026625 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.63s]
[0m14:56:39.027579 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:56:39.029066 [debug] [MainThread]: On master: ROLLBACK
[0m14:56:39.029625 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:56:39.072860 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:56:39.073468 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:56:39.073963 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:56:39.074416 [debug] [MainThread]: On master: ROLLBACK
[0m14:56:39.074909 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:56:39.075366 [debug] [MainThread]: On master: Close
[0m14:56:39.081389 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:56:39.081944 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:56:39.082544 [info ] [MainThread]: 
[0m14:56:39.083103 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 11.82 seconds (11.82s).
[0m14:56:39.084821 [debug] [MainThread]: Command end result
[0m14:56:39.108880 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:56:39.110610 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:56:39.117152 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:56:39.117820 [info ] [MainThread]: 
[0m14:56:39.118447 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:56:39.119048 [info ] [MainThread]: 
[0m14:56:39.119878 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:56:39.121224 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.71546, "process_in_blocks": "0", "process_kernel_time": 0.176417, "process_mem_max_rss": "108684", "process_out_blocks": "2040", "process_user_time": 2.989125}
[0m14:56:39.121868 [debug] [MainThread]: Command `dbt run` succeeded at 14:56:39.121737 after 12.72 seconds
[0m14:56:39.122351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c226c9a060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c22502b410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71c225028590>]}
[0m14:56:39.122845 [debug] [MainThread]: Flushing usage events
[0m14:56:40.992480 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:00:23.932823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714274464d40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714273bed970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714274467470>]}


============================== 15:00:23.935690 | 909c4f4e-7b27-4e1d-a815-9fe4e4441739 ==============================
[0m15:00:23.935690 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:00:23.936241 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:00:23.989596 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:00:23.990117 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:00:23.990487 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:00:24.136868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7142754a40b0>]}
[0m15:00:24.202625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714273a78440>]}
[0m15:00:24.203270 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m15:00:24.275725 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:00:24.358596 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:00:24.359194 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:00:24.366142 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m15:00:24.386009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714273d3dd60>]}
[0m15:00:24.440933 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:00:24.444857 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:00:24.455717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714272e64080>]}
[0m15:00:24.456209 [info ] [MainThread]: Found 5 models, 473 macros
[0m15:00:24.456577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714272c09be0>]}
[0m15:00:24.458651 [info ] [MainThread]: 
[0m15:00:24.459201 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:00:24.459638 [info ] [MainThread]: 
[0m15:00:24.460221 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:00:24.466682 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:00:24.478147 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:00:24.478677 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:00:24.479079 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:00:27.165387 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:00:27.165914 [debug] [ThreadPool]: SQL status: OK in 2.687 seconds
[0m15:00:27.238308 [debug] [ThreadPool]: On list_schemas: Close
[0m15:00:27.267254 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m15:00:27.273199 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:27.273705 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m15:00:27.274162 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m15:00:27.274610 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:00:27.897634 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:00:27.898221 [debug] [ThreadPool]: SQL status: OK in 0.624 seconds
[0m15:00:27.905276 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m15:00:27.905728 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:00:27.906062 [debug] [ThreadPool]: On list_None_default: Close
[0m15:00:27.920079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714275b18620>]}
[0m15:00:27.920681 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:27.921062 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:00:27.923340 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m15:00:27.924012 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m15:00:27.924714 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m15:00:27.925351 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m15:00:27.933464 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m15:00:27.934283 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m15:00:27.956644 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:00:27.957228 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m15:00:27.957707 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:28.632275 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:28.633260 [debug] [Thread-1 (]: SQL status: OK in 0.675 seconds
[0m15:00:28.705087 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m15:00:28.706077 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:28.706759 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:00:28.707501 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m15:00:31.776108 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:31.777316 [debug] [Thread-1 (]: SQL status: OK in 3.069 seconds
[0m15:00:31.812590 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m15:00:31.813137 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:00:31.813607 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m15:00:31.822780 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71427546afc0>]}
[0m15:00:31.823688 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.90s]
[0m15:00:31.824659 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m15:00:31.825618 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m15:00:31.826447 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m15:00:31.827129 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m15:00:31.827723 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m15:00:31.830464 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m15:00:31.831501 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m15:00:31.835345 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:00:31.835906 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m15:00:31.836411 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:32.027698 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:32.028705 [debug] [Thread-1 (]: SQL status: OK in 0.192 seconds
[0m15:00:32.035048 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m15:00:32.036272 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:32.037061 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:00:32.037826 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m15:00:32.670449 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:32.671015 [debug] [Thread-1 (]: SQL status: OK in 0.633 seconds
[0m15:00:32.674008 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m15:00:32.674701 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:00:32.675401 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m15:00:32.684304 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714272cc31a0>]}
[0m15:00:32.685184 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.86s]
[0m15:00:32.685899 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m15:00:32.686393 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m15:00:32.686970 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m15:00:32.687522 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m15:00:32.687997 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m15:00:32.690714 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m15:00:32.691645 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m15:00:32.696584 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:00:32.697096 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m15:00:32.697551 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:32.866584 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:32.867174 [debug] [Thread-1 (]: SQL status: OK in 0.170 seconds
[0m15:00:32.870424 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m15:00:32.871142 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:32.871611 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:00:32.872057 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m15:00:33.272345 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:33.272897 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m15:00:33.277265 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m15:00:33.277764 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:00:33.278207 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m15:00:33.284979 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714270b77b90>]}
[0m15:00:33.286230 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.60s]
[0m15:00:33.287383 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m15:00:33.288221 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m15:00:33.289215 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m15:00:33.290134 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m15:00:33.290973 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m15:00:33.294872 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m15:00:33.295618 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m15:00:33.299647 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:00:33.300119 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m15:00:33.300531 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:33.485025 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:33.485939 [debug] [Thread-1 (]: SQL status: OK in 0.185 seconds
[0m15:00:33.491218 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m15:00:33.492501 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:33.493198 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:00:33.493893 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m15:00:33.931200 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:33.931800 [debug] [Thread-1 (]: SQL status: OK in 0.437 seconds
[0m15:00:33.934203 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m15:00:33.934632 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:00:33.935017 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m15:00:33.942045 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714270b77b90>]}
[0m15:00:33.943074 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.65s]
[0m15:00:33.944005 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m15:00:33.944653 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m15:00:33.945474 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m15:00:33.946183 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m15:00:33.946696 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m15:00:33.949419 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m15:00:33.950227 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m15:00:33.954088 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:00:33.954520 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m15:00:33.954930 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:00:34.113700 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:34.114263 [debug] [Thread-1 (]: SQL status: OK in 0.159 seconds
[0m15:00:34.117221 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m15:00:34.117974 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:34.118456 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:00:34.118944 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m15:00:34.479448 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:00:34.480036 [debug] [Thread-1 (]: SQL status: OK in 0.361 seconds
[0m15:00:34.482713 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m15:00:34.483200 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:00:34.483628 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m15:00:34.489510 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '909c4f4e-7b27-4e1d-a815-9fe4e4441739', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714272cc2a80>]}
[0m15:00:34.490466 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.54s]
[0m15:00:34.491258 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m15:00:34.492764 [debug] [MainThread]: On master: ROLLBACK
[0m15:00:34.493260 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:00:34.529753 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:00:34.530266 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:00:34.530681 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:00:34.531053 [debug] [MainThread]: On master: ROLLBACK
[0m15:00:34.531441 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:00:34.531815 [debug] [MainThread]: On master: Close
[0m15:00:34.537013 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:00:34.537481 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m15:00:34.537955 [info ] [MainThread]: 
[0m15:00:34.538367 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 10.08 seconds (10.08s).
[0m15:00:34.539705 [debug] [MainThread]: Command end result
[0m15:00:34.563548 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:00:34.565601 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:00:34.572110 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m15:00:34.572571 [info ] [MainThread]: 
[0m15:00:34.573046 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:00:34.573598 [info ] [MainThread]: 
[0m15:00:34.574120 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:00:34.575347 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.709811, "process_in_blocks": "808", "process_kernel_time": 0.170217, "process_mem_max_rss": "108740", "process_out_blocks": "2048", "process_user_time": 2.221833}
[0m15:00:34.576026 [debug] [MainThread]: Command `dbt run` succeeded at 15:00:34.575884 after 10.71 seconds
[0m15:00:34.576525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71427429c6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714273093fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x714273093aa0>]}
[0m15:00:34.577049 [debug] [MainThread]: Flushing usage events
[0m15:00:36.288006 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:03:41.283593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098ee24da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b099091c5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b09907e9a00>]}


============================== 15:03:41.287236 | e92e1fc9-73df-435f-939f-9328b3201d1a ==============================
[0m15:03:41.287236 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:03:41.287938 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:03:41.365701 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:03:41.366236 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:03:41.366658 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:03:41.514293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098e17fce0>]}
[0m15:03:41.577302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098e29f650>]}
[0m15:03:41.578023 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m15:03:41.658896 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:03:41.757384 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:03:41.757912 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:03:41.764425 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m15:03:41.784962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098e0549b0>]}
[0m15:03:41.840819 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:03:41.843675 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:03:41.858362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098df6be60>]}
[0m15:03:41.859283 [info ] [MainThread]: Found 5 models, 473 macros
[0m15:03:41.859780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098defbda0>]}
[0m15:03:41.861763 [info ] [MainThread]: 
[0m15:03:41.862148 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:03:41.862484 [info ] [MainThread]: 
[0m15:03:41.863057 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:03:41.869208 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:03:41.880510 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:03:41.881013 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:03:41.881369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:03:44.682446 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:03:44.683051 [debug] [ThreadPool]: SQL status: OK in 2.802 seconds
[0m15:03:44.743292 [debug] [ThreadPool]: On list_schemas: Close
[0m15:03:44.780530 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m15:03:44.787852 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:44.788348 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m15:03:44.788778 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m15:03:44.789146 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:03:45.553655 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:03:45.554610 [debug] [ThreadPool]: SQL status: OK in 0.765 seconds
[0m15:03:45.565979 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m15:03:45.567152 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:03:45.567992 [debug] [ThreadPool]: On list_None_default: Close
[0m15:03:45.595889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098eeca7e0>]}
[0m15:03:45.597023 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:45.597864 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:03:45.601571 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m15:03:45.602919 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m15:03:45.603967 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m15:03:45.604926 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m15:03:45.620469 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m15:03:45.621709 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m15:03:45.665474 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:03:45.666178 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m15:03:45.666774 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:03:46.553055 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:46.553834 [debug] [Thread-1 (]: SQL status: OK in 0.887 seconds
[0m15:03:46.611416 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m15:03:46.612749 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:46.613479 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:03:46.614244 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m15:03:48.798843 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:48.799482 [debug] [Thread-1 (]: SQL status: OK in 2.185 seconds
[0m15:03:48.825474 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m15:03:48.826030 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:03:48.826471 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m15:03:48.837414 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098e17e120>]}
[0m15:03:48.838275 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.23s]
[0m15:03:48.839000 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m15:03:48.839619 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m15:03:48.840643 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m15:03:48.841444 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m15:03:48.842029 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m15:03:48.844955 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m15:03:48.845670 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m15:03:48.850726 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:03:48.851361 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m15:03:48.851958 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:03:49.034321 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:49.035073 [debug] [Thread-1 (]: SQL status: OK in 0.183 seconds
[0m15:03:49.038954 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m15:03:49.040046 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:49.040956 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:03:49.041680 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m15:03:49.714572 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:49.715586 [debug] [Thread-1 (]: SQL status: OK in 0.673 seconds
[0m15:03:49.720441 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m15:03:49.721270 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:03:49.722034 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m15:03:49.732893 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098df24aa0>]}
[0m15:03:49.734285 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.89s]
[0m15:03:49.735572 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m15:03:49.736457 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m15:03:49.737542 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m15:03:49.738465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m15:03:49.739245 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m15:03:49.744109 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m15:03:49.745210 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m15:03:49.752395 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:03:49.753331 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m15:03:49.754067 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:03:49.951113 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:49.952149 [debug] [Thread-1 (]: SQL status: OK in 0.198 seconds
[0m15:03:49.957967 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m15:03:49.959366 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:49.960149 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:03:49.960979 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m15:03:50.489450 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:50.490288 [debug] [Thread-1 (]: SQL status: OK in 0.528 seconds
[0m15:03:50.496333 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m15:03:50.497105 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:03:50.497742 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m15:03:50.507833 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098c47b1d0>]}
[0m15:03:50.509799 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.77s]
[0m15:03:50.511332 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m15:03:50.512299 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m15:03:50.513852 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m15:03:50.514985 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m15:03:50.516092 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m15:03:50.521754 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m15:03:50.523250 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m15:03:50.532355 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:03:50.533327 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m15:03:50.534340 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:03:50.764157 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:50.764850 [debug] [Thread-1 (]: SQL status: OK in 0.231 seconds
[0m15:03:50.768642 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m15:03:50.769676 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:50.770247 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:03:50.770873 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m15:03:51.315835 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:51.316662 [debug] [Thread-1 (]: SQL status: OK in 0.545 seconds
[0m15:03:51.319821 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m15:03:51.320381 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:03:51.320943 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m15:03:51.329716 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098c478f80>]}
[0m15:03:51.330672 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.81s]
[0m15:03:51.331700 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m15:03:51.332312 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m15:03:51.333300 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m15:03:51.334208 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m15:03:51.334888 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m15:03:51.338110 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m15:03:51.339098 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m15:03:51.344638 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:03:51.345261 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m15:03:51.345818 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:03:51.530359 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:51.531125 [debug] [Thread-1 (]: SQL status: OK in 0.185 seconds
[0m15:03:51.535100 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m15:03:51.536029 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:51.536554 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:03:51.537049 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m15:03:51.973713 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:03:51.974422 [debug] [Thread-1 (]: SQL status: OK in 0.437 seconds
[0m15:03:51.977211 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m15:03:51.977691 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:03:51.978113 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m15:03:51.984886 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e92e1fc9-73df-435f-939f-9328b3201d1a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098ddc6ab0>]}
[0m15:03:51.985910 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.65s]
[0m15:03:51.986818 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m15:03:51.988278 [debug] [MainThread]: On master: ROLLBACK
[0m15:03:51.988705 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:03:52.028838 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:03:52.029413 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:03:52.029831 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:03:52.030218 [debug] [MainThread]: On master: ROLLBACK
[0m15:03:52.030626 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:03:52.031000 [debug] [MainThread]: On master: Close
[0m15:03:52.036911 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:03:52.037454 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m15:03:52.038024 [info ] [MainThread]: 
[0m15:03:52.038554 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 10.18 seconds (10.18s).
[0m15:03:52.040496 [debug] [MainThread]: Command end result
[0m15:03:52.063039 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:03:52.064617 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:03:52.071088 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m15:03:52.071599 [info ] [MainThread]: 
[0m15:03:52.072163 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:03:52.072720 [info ] [MainThread]: 
[0m15:03:52.073308 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:03:52.074495 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.865661, "process_in_blocks": "7640", "process_kernel_time": 0.209288, "process_mem_max_rss": "108536", "process_out_blocks": "2048", "process_user_time": 2.336225}
[0m15:03:52.075132 [debug] [MainThread]: Command `dbt run` succeeded at 15:03:52.074995 after 10.87 seconds
[0m15:03:52.075607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b099091c5c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b098dfaec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b09907ea000>]}
[0m15:03:52.076092 [debug] [MainThread]: Flushing usage events
[0m15:03:54.255007 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:10:06.253407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74677ae37170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746779321a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74677986d040>]}


============================== 15:10:06.260922 | 31795106-cd10-4b3c-95c4-2d1861932255 ==============================
[0m15:10:06.260922 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:10:06.262045 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:10:06.408601 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:10:06.409718 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:10:06.410656 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:10:06.707914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7467788c3ce0>]}
[0m15:10:06.824987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7467788a8a40>]}
[0m15:10:06.826191 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m15:10:06.962349 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:10:07.234738 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:10:07.235908 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:10:07.250801 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m15:10:07.291438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746778ca7b60>]}
[0m15:10:07.393994 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:10:07.398624 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:10:07.429888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746778697ef0>]}
[0m15:10:07.430838 [info ] [MainThread]: Found 5 models, 473 macros
[0m15:10:07.431679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746778785040>]}
[0m15:10:07.435223 [info ] [MainThread]: 
[0m15:10:07.436037 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:10:07.436769 [info ] [MainThread]: 
[0m15:10:07.437826 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:10:07.449928 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:10:07.470287 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:10:07.471071 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:10:07.471718 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:12.633050 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:10:12.634368 [debug] [ThreadPool]: SQL status: OK in 5.163 seconds
[0m15:10:12.762795 [debug] [ThreadPool]: On list_schemas: Close
[0m15:10:12.812593 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m15:10:12.822394 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:12.823262 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m15:10:12.824067 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m15:10:12.824846 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:10:13.930564 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:10:13.931835 [debug] [ThreadPool]: SQL status: OK in 1.107 seconds
[0m15:10:13.942274 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m15:10:13.943117 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:10:13.943842 [debug] [ThreadPool]: On list_None_default: Close
[0m15:10:13.964996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74677ab14ef0>]}
[0m15:10:13.965990 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:13.966707 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:10:13.971047 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m15:10:13.972120 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m15:10:13.973138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m15:10:13.974215 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m15:10:13.988009 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m15:10:13.990715 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m15:10:14.032159 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:10:14.033084 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m15:10:14.033895 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:10:14.246767 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:14.247804 [debug] [Thread-1 (]: SQL status: OK in 0.214 seconds
[0m15:10:14.332138 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m15:10:14.334013 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:14.334893 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:10:14.335721 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m15:10:18.208988 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:18.210034 [debug] [Thread-1 (]: SQL status: OK in 3.873 seconds
[0m15:10:18.245435 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m15:10:18.246361 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:10:18.247256 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m15:10:18.263218 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746778513ce0>]}
[0m15:10:18.264595 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 4.29s]
[0m15:10:18.265941 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m15:10:18.267199 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m15:10:18.268309 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m15:10:18.269293 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m15:10:18.270144 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m15:10:18.274703 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m15:10:18.275833 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m15:10:18.282519 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:10:18.283357 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m15:10:18.284140 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:10:18.430705 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:18.431585 [debug] [Thread-1 (]: SQL status: OK in 0.147 seconds
[0m15:10:18.436957 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m15:10:18.438131 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:18.438974 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:10:18.439825 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m15:10:19.233439 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:19.234434 [debug] [Thread-1 (]: SQL status: OK in 0.794 seconds
[0m15:10:19.238189 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m15:10:19.239034 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:10:19.239835 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m15:10:19.259203 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746778554260>]}
[0m15:10:19.260650 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.99s]
[0m15:10:19.261896 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m15:10:19.262778 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m15:10:19.263866 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m15:10:19.264852 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m15:10:19.265677 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m15:10:19.270018 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m15:10:19.271109 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m15:10:19.278687 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:10:19.279507 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m15:10:19.280266 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:10:19.432277 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:19.433834 [debug] [Thread-1 (]: SQL status: OK in 0.153 seconds
[0m15:10:19.443270 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m15:10:19.445107 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:19.446346 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:10:19.447619 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m15:10:20.199023 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:20.200413 [debug] [Thread-1 (]: SQL status: OK in 0.751 seconds
[0m15:10:20.205964 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m15:10:20.207208 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:10:20.208334 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m15:10:20.219695 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746778513ce0>]}
[0m15:10:20.221604 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.95s]
[0m15:10:20.222961 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m15:10:20.224053 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m15:10:20.225461 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m15:10:20.226484 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m15:10:20.227411 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m15:10:20.234055 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m15:10:20.235187 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m15:10:20.242356 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:10:20.243301 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m15:10:20.244049 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:10:20.363099 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:20.363987 [debug] [Thread-1 (]: SQL status: OK in 0.120 seconds
[0m15:10:20.369391 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m15:10:20.370727 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:20.371737 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:10:20.372685 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m15:10:21.017201 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:21.018175 [debug] [Thread-1 (]: SQL status: OK in 0.645 seconds
[0m15:10:21.022680 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m15:10:21.023654 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:10:21.024488 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m15:10:21.035080 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746776374560>]}
[0m15:10:21.036402 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.81s]
[0m15:10:21.037626 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m15:10:21.038813 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m15:10:21.040111 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m15:10:21.041175 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m15:10:21.042051 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m15:10:21.046435 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m15:10:21.047532 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m15:10:21.054386 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:10:21.055346 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m15:10:21.056446 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:10:21.184027 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:21.184954 [debug] [Thread-1 (]: SQL status: OK in 0.128 seconds
[0m15:10:21.190085 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m15:10:21.191361 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:21.192161 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:10:21.192920 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m15:10:21.767211 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:10:21.768646 [debug] [Thread-1 (]: SQL status: OK in 0.575 seconds
[0m15:10:21.775080 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m15:10:21.776450 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:10:21.777721 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m15:10:21.789010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31795106-cd10-4b3c-95c4-2d1861932255', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7467763756d0>]}
[0m15:10:21.790961 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.75s]
[0m15:10:21.792854 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m15:10:21.795168 [debug] [MainThread]: On master: ROLLBACK
[0m15:10:21.795892 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:10:21.860166 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:10:21.861013 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:10:21.861751 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:10:21.862441 [debug] [MainThread]: On master: ROLLBACK
[0m15:10:21.863160 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:10:21.863856 [debug] [MainThread]: On master: Close
[0m15:10:21.872604 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:10:21.873455 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m15:10:21.874429 [info ] [MainThread]: 
[0m15:10:21.875258 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 14.44 seconds (14.44s).
[0m15:10:21.877640 [debug] [MainThread]: Command end result
[0m15:10:21.913705 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:10:21.916227 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:10:21.927555 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m15:10:21.928279 [info ] [MainThread]: 
[0m15:10:21.929060 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:10:21.929772 [info ] [MainThread]: 
[0m15:10:21.930507 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:10:21.932223 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 15.787733, "process_in_blocks": "68160", "process_kernel_time": 0.454279, "process_mem_max_rss": "108232", "process_out_blocks": "2056", "process_user_time": 4.397705}
[0m15:10:21.933108 [debug] [MainThread]: Command `dbt run` succeeded at 15:10:21.932917 after 15.79 seconds
[0m15:10:21.933874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74677d3b3b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x746779060fb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7467790ddc10>]}
[0m15:10:21.934663 [debug] [MainThread]: Flushing usage events
[0m15:10:24.275361 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:28:43.917303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef8477a4470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef848951010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef847034980>]}


============================== 15:28:43.922394 | e02278dc-80b3-4f4f-a485-e8108690da73 ==============================
[0m15:28:43.922394 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:28:43.923076 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:28:44.064230 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:28:44.064751 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:28:44.065146 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:28:44.224369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef846302c60>]}
[0m15:28:44.281781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef8465a8530>]}
[0m15:28:44.282483 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m15:28:44.385741 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:28:44.500895 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:28:44.501334 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:28:44.507485 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m15:28:44.534283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef846342480>]}
[0m15:28:44.592425 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:28:44.596281 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:28:44.617741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef846090f20>]}
[0m15:28:44.618404 [info ] [MainThread]: Found 5 models, 473 macros
[0m15:28:44.618919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef846501340>]}
[0m15:28:44.621222 [info ] [MainThread]: 
[0m15:28:44.621756 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:28:44.622171 [info ] [MainThread]: 
[0m15:28:44.622797 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:28:44.631358 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:28:44.646866 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:28:44.647333 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:28:44.647715 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:28:47.464342 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:28:47.464953 [debug] [ThreadPool]: SQL status: OK in 2.817 seconds
[0m15:28:47.541480 [debug] [ThreadPool]: On list_schemas: Close
[0m15:28:47.580474 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m15:28:47.588402 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:47.589080 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m15:28:47.589655 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m15:28:47.590234 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:28:48.027661 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:28:48.028222 [debug] [ThreadPool]: SQL status: OK in 0.438 seconds
[0m15:28:48.033652 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m15:28:48.034159 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:28:48.034565 [debug] [ThreadPool]: On list_None_default: Close
[0m15:28:48.044900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef8487d8f50>]}
[0m15:28:48.045525 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:48.045978 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:28:48.048595 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m15:28:48.049162 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m15:28:48.049687 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m15:28:48.050148 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m15:28:48.058448 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m15:28:48.061046 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m15:28:48.087758 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:28:48.088272 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m15:28:48.088701 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:28:48.200098 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:48.200812 [debug] [Thread-1 (]: SQL status: OK in 0.112 seconds
[0m15:28:48.250301 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m15:28:48.251844 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:48.252347 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:28:48.252826 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m15:28:51.061718 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:51.062420 [debug] [Thread-1 (]: SQL status: OK in 2.809 seconds
[0m15:28:51.087469 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m15:28:51.088134 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:28:51.088677 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m15:28:51.100397 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef845f084a0>]}
[0m15:28:51.101286 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.05s]
[0m15:28:51.102168 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m15:28:51.102827 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m15:28:51.103652 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m15:28:51.104380 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m15:28:51.104999 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m15:28:51.109353 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m15:28:51.112444 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m15:28:51.120584 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:28:51.121707 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m15:28:51.122751 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:28:51.259403 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:51.260124 [debug] [Thread-1 (]: SQL status: OK in 0.137 seconds
[0m15:28:51.264483 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m15:28:51.265453 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:51.266045 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:28:51.266633 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m15:28:51.798370 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:51.799253 [debug] [Thread-1 (]: SQL status: OK in 0.532 seconds
[0m15:28:51.802736 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m15:28:51.803459 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:28:51.804149 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m15:28:51.812186 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef845f9c890>]}
[0m15:28:51.813458 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.71s]
[0m15:28:51.814636 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m15:28:51.815480 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m15:28:51.816485 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m15:28:51.817386 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m15:28:51.818131 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m15:28:51.822504 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m15:28:51.823635 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m15:28:51.831564 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:28:51.832640 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m15:28:51.833621 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:28:51.938610 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:51.939329 [debug] [Thread-1 (]: SQL status: OK in 0.106 seconds
[0m15:28:51.943402 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m15:28:51.944326 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:51.944914 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:28:51.945441 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m15:28:52.440451 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:52.441081 [debug] [Thread-1 (]: SQL status: OK in 0.495 seconds
[0m15:28:52.443439 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m15:28:52.443962 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:28:52.444423 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m15:28:52.451200 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef8445e3470>]}
[0m15:28:52.452116 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.63s]
[0m15:28:52.453005 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m15:28:52.453555 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m15:28:52.454170 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m15:28:52.454785 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m15:28:52.455289 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m15:28:52.459926 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m15:28:52.460931 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m15:28:52.467865 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:28:52.468985 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m15:28:52.470067 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:28:52.563041 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:52.563739 [debug] [Thread-1 (]: SQL status: OK in 0.094 seconds
[0m15:28:52.567560 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m15:28:52.568409 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:52.568930 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:28:52.569421 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m15:28:52.960853 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:52.961640 [debug] [Thread-1 (]: SQL status: OK in 0.392 seconds
[0m15:28:52.964455 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m15:28:52.965050 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:28:52.965496 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m15:28:52.972765 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef845fb2e10>]}
[0m15:28:52.973479 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.52s]
[0m15:28:52.974159 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m15:28:52.974690 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m15:28:52.975393 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m15:28:52.976013 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m15:28:52.976484 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m15:28:52.979235 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m15:28:52.980278 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m15:28:52.987704 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:28:52.988817 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m15:28:52.989807 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:28:53.074356 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:53.075018 [debug] [Thread-1 (]: SQL status: OK in 0.085 seconds
[0m15:28:53.078685 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m15:28:53.079513 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:53.080010 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:28:53.080442 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m15:28:53.504753 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:28:53.505891 [debug] [Thread-1 (]: SQL status: OK in 0.425 seconds
[0m15:28:53.510280 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m15:28:53.511249 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:28:53.512194 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m15:28:53.520417 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e02278dc-80b3-4f4f-a485-e8108690da73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef845f89340>]}
[0m15:28:53.521974 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.54s]
[0m15:28:53.523670 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m15:28:53.525705 [debug] [MainThread]: On master: ROLLBACK
[0m15:28:53.526704 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:28:53.576625 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:28:53.577180 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:28:53.577628 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:28:53.578041 [debug] [MainThread]: On master: ROLLBACK
[0m15:28:53.578428 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:28:53.578840 [debug] [MainThread]: On master: Close
[0m15:28:53.584491 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:28:53.585036 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m15:28:53.585509 [info ] [MainThread]: 
[0m15:28:53.585976 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 8.96 seconds (8.96s).
[0m15:28:53.587435 [debug] [MainThread]: Command end result
[0m15:28:53.619323 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:28:53.621078 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:28:53.627277 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m15:28:53.627797 [info ] [MainThread]: 
[0m15:28:53.628275 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:28:53.628730 [info ] [MainThread]: 
[0m15:28:53.629180 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:28:53.630989 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.768778, "process_in_blocks": "55552", "process_kernel_time": 0.242355, "process_mem_max_rss": "108344", "process_out_blocks": "2048", "process_user_time": 2.232276}
[0m15:28:53.631605 [debug] [MainThread]: Command `dbt run` succeeded at 15:28:53.631475 after 9.77 seconds
[0m15:28:53.632111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef846f2bd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef846a88770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef848951010>]}
[0m15:28:53.632603 [debug] [MainThread]: Flushing usage events
[0m15:28:55.577095 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:31:08.826697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f84754dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f8330c7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f818c55e0>]}


============================== 15:31:08.830337 | 9348299f-751f-4c37-afb4-8b11da66c545 ==============================
[0m15:31:08.830337 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:31:08.831373 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:31:08.901077 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:31:08.901688 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:31:08.902181 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:31:09.097337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f80b12150>]}
[0m15:31:09.184737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f80f5d100>]}
[0m15:31:09.185738 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m15:31:09.322335 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:31:09.490603 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:09.491827 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:09.503650 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m15:31:09.539877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f81345c40>]}
[0m15:31:09.640996 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:31:09.645269 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:31:09.662507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f809bda00>]}
[0m15:31:09.663273 [info ] [MainThread]: Found 5 models, 473 macros
[0m15:31:09.663909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f80a3b860>]}
[0m15:31:09.666891 [info ] [MainThread]: 
[0m15:31:09.667627 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:31:09.668603 [info ] [MainThread]: 
[0m15:31:09.669937 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:31:09.682675 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:31:09.703895 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:31:09.704642 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:31:09.705247 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:31:13.019041 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:31:13.019487 [debug] [ThreadPool]: SQL status: OK in 3.314 seconds
[0m15:31:13.074626 [debug] [ThreadPool]: On list_schemas: Close
[0m15:31:13.099878 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m15:31:13.105182 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:13.105645 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m15:31:13.106028 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m15:31:13.106362 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:31:13.537806 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:31:13.538509 [debug] [ThreadPool]: SQL status: OK in 0.432 seconds
[0m15:31:13.545251 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m15:31:13.545722 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:31:13.546084 [debug] [ThreadPool]: On list_None_default: Close
[0m15:31:13.561095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f80b935f0>]}
[0m15:31:13.561709 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:13.562111 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:31:13.564320 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m15:31:13.564965 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m15:31:13.565499 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m15:31:13.565962 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m15:31:13.574551 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m15:31:13.575269 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m15:31:13.598171 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:31:13.598705 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m15:31:13.599132 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:13.713789 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:13.714406 [debug] [Thread-1 (]: SQL status: OK in 0.115 seconds
[0m15:31:13.764579 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m15:31:13.765361 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:13.765822 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:31:13.766250 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m15:31:16.014768 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:16.015947 [debug] [Thread-1 (]: SQL status: OK in 2.249 seconds
[0m15:31:16.047231 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m15:31:16.047769 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:31:16.048197 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m15:31:16.059814 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f82f8f170>]}
[0m15:31:16.060686 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 2.49s]
[0m15:31:16.061421 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m15:31:16.061941 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m15:31:16.062509 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m15:31:16.063101 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m15:31:16.063586 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m15:31:16.066251 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m15:31:16.066999 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m15:31:16.071003 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:31:16.071646 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m15:31:16.072235 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:16.163941 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:16.164593 [debug] [Thread-1 (]: SQL status: OK in 0.092 seconds
[0m15:31:16.168083 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m15:31:16.168894 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:16.169374 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:31:16.169846 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m15:31:16.660576 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:16.661148 [debug] [Thread-1 (]: SQL status: OK in 0.491 seconds
[0m15:31:16.663371 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m15:31:16.663823 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:31:16.664217 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m15:31:16.674119 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f7e6376e0>]}
[0m15:31:16.675415 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.61s]
[0m15:31:16.676890 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m15:31:16.677722 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m15:31:16.678637 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m15:31:16.679420 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m15:31:16.680108 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m15:31:16.683628 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m15:31:16.684254 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m15:31:16.688441 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:31:16.689205 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m15:31:16.689793 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:16.784436 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:16.785204 [debug] [Thread-1 (]: SQL status: OK in 0.095 seconds
[0m15:31:16.789505 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m15:31:16.790442 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:16.791007 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:31:16.791531 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m15:31:17.300452 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:17.301034 [debug] [Thread-1 (]: SQL status: OK in 0.509 seconds
[0m15:31:17.303210 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m15:31:17.303685 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:31:17.304188 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m15:31:17.311574 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f7e687cb0>]}
[0m15:31:17.312527 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.63s]
[0m15:31:17.313410 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m15:31:17.313930 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m15:31:17.314497 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m15:31:17.315216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m15:31:17.315698 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m15:31:17.320093 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m15:31:17.320992 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m15:31:17.326107 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:31:17.326642 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m15:31:17.327075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:17.416511 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:17.417258 [debug] [Thread-1 (]: SQL status: OK in 0.090 seconds
[0m15:31:17.421228 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m15:31:17.422406 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:17.423150 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:31:17.423905 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m15:31:17.847114 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:17.848125 [debug] [Thread-1 (]: SQL status: OK in 0.423 seconds
[0m15:31:17.852201 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m15:31:17.852960 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:31:17.853677 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m15:31:17.860754 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f7e65dbb0>]}
[0m15:31:17.861914 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.55s]
[0m15:31:17.863060 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m15:31:17.863866 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m15:31:17.864711 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m15:31:17.865582 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m15:31:17.866278 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m15:31:17.870265 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m15:31:17.871312 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m15:31:17.877689 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:31:17.878411 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m15:31:17.879063 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:31:17.958336 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:17.958939 [debug] [Thread-1 (]: SQL status: OK in 0.080 seconds
[0m15:31:17.962100 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m15:31:17.963298 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:17.963807 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:31:17.964264 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m15:31:18.434516 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:31:18.435229 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m15:31:18.438366 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m15:31:18.439039 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:31:18.439594 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m15:31:18.446905 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9348299f-751f-4c37-afb4-8b11da66c545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f7e646120>]}
[0m15:31:18.447742 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.58s]
[0m15:31:18.448422 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m15:31:18.449869 [debug] [MainThread]: On master: ROLLBACK
[0m15:31:18.450350 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:31:18.494695 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:31:18.495214 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:31:18.495633 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:31:18.496025 [debug] [MainThread]: On master: ROLLBACK
[0m15:31:18.496410 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:31:18.496805 [debug] [MainThread]: On master: Close
[0m15:31:18.503291 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:31:18.503919 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m15:31:18.504527 [info ] [MainThread]: 
[0m15:31:18.505164 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 8.83 seconds (8.83s).
[0m15:31:18.506926 [debug] [MainThread]: Command end result
[0m15:31:18.529656 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:31:18.531240 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:31:18.539350 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m15:31:18.540037 [info ] [MainThread]: 
[0m15:31:18.540983 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:31:18.541614 [info ] [MainThread]: 
[0m15:31:18.542312 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:31:18.543637 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.789499, "process_in_blocks": "0", "process_kernel_time": 0.178995, "process_mem_max_rss": "108580", "process_out_blocks": "2064", "process_user_time": 2.46243}
[0m15:31:18.544290 [debug] [MainThread]: Command `dbt run` succeeded at 15:31:18.544153 after 9.79 seconds
[0m15:31:18.544913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f84350e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f82e34e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e6f813c05f0>]}
[0m15:31:18.545617 [debug] [MainThread]: Flushing usage events
[0m15:31:20.130879 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:24:19.757378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef202aecfb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef202aef560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef202aef680>]}


============================== 11:24:19.761434 | db6a453f-97df-4d27-944a-295a8fa5a9f3 ==============================
[0m11:24:19.761434 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:24:19.762216 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m11:24:19.859554 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:24:19.860115 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:24:19.860598 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:24:20.020865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef2019ee720>]}
[0m11:24:20.087690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef201d884a0>]}
[0m11:24:20.088505 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:24:20.199010 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:24:20.313760 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:24:20.314426 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:24:20.324054 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:24:20.360130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef201a34f20>]}
[0m11:24:20.439066 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:24:20.443044 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:24:20.464446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef201770ce0>]}
[0m11:24:20.465129 [info ] [MainThread]: Found 5 models, 473 macros
[0m11:24:20.465746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef201767b60>]}
[0m11:24:20.468158 [info ] [MainThread]: 
[0m11:24:20.468840 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:24:20.469393 [info ] [MainThread]: 
[0m11:24:20.470135 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:24:20.479054 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:24:20.497689 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:24:20.498266 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:24:20.498742 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:24:20.697804 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:24:20.698429 [debug] [ThreadPool]: SQL status: OK in 0.200 seconds
[0m11:24:20.703461 [debug] [ThreadPool]: On list_schemas: Close
[0m11:24:20.729180 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:24:20.734626 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:20.735129 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:24:20.735597 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:24:20.736023 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:24:20.915801 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:24:20.916478 [debug] [ThreadPool]: SQL status: OK in 0.180 seconds
[0m11:24:20.922071 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:24:20.922711 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:24:20.923340 [debug] [ThreadPool]: On list_None_default: Close
[0m11:24:20.931966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef201de2f60>]}
[0m11:24:20.932528 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:20.932912 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:24:20.936445 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m11:24:20.936999 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m11:24:20.937506 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m11:24:20.937924 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m11:24:20.944944 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m11:24:20.945787 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m11:24:20.977755 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m11:24:20.978444 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m11:24:20.979053 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:24:21.091766 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:21.092443 [debug] [Thread-1 (]: SQL status: OK in 0.113 seconds
[0m11:24:21.132825 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m11:24:21.133966 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:21.134505 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m11:24:21.135014 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m11:24:22.802907 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:22.803582 [debug] [Thread-1 (]: SQL status: OK in 1.668 seconds
[0m11:24:22.824050 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m11:24:22.824647 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:24:22.825112 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m11:24:22.834334 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef203d874d0>]}
[0m11:24:22.835128 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.89s]
[0m11:24:22.835836 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m11:24:22.836420 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m11:24:22.837141 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m11:24:22.837723 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m11:24:22.838222 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m11:24:22.840918 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m11:24:22.841857 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m11:24:22.848562 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m11:24:22.849319 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m11:24:22.849924 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:24:22.935539 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:22.936210 [debug] [Thread-1 (]: SQL status: OK in 0.086 seconds
[0m11:24:22.939815 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m11:24:22.940644 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:22.941128 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m11:24:22.941616 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m11:24:23.443842 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:23.444460 [debug] [Thread-1 (]: SQL status: OK in 0.502 seconds
[0m11:24:23.446733 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m11:24:23.447220 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:24:23.447674 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m11:24:23.454652 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef200484b00>]}
[0m11:24:23.455527 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.62s]
[0m11:24:23.456289 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m11:24:23.456822 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m11:24:23.457474 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m11:24:23.458212 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m11:24:23.458710 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m11:24:23.461304 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m11:24:23.462312 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m11:24:23.469143 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m11:24:23.470186 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m11:24:23.471105 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:24:23.558742 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:23.559675 [debug] [Thread-1 (]: SQL status: OK in 0.089 seconds
[0m11:24:23.564966 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m11:24:23.565964 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:23.566641 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m11:24:23.567354 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m11:24:23.988566 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:23.989164 [debug] [Thread-1 (]: SQL status: OK in 0.421 seconds
[0m11:24:23.991525 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m11:24:23.992017 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:24:23.992477 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m11:24:23.999337 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef2004bc140>]}
[0m11:24:24.000173 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.54s]
[0m11:24:24.000925 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m11:24:24.001473 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m11:24:24.002071 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m11:24:24.002788 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m11:24:24.003291 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m11:24:24.008270 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m11:24:24.009152 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m11:24:24.016069 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m11:24:24.017177 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m11:24:24.018265 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:24:24.097832 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:24.098686 [debug] [Thread-1 (]: SQL status: OK in 0.080 seconds
[0m11:24:24.102545 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m11:24:24.103385 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:24.103934 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m11:24:24.104461 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m11:24:24.547778 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:24.548559 [debug] [Thread-1 (]: SQL status: OK in 0.443 seconds
[0m11:24:24.551702 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m11:24:24.552373 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:24:24.552966 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m11:24:24.560948 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef2004ad610>]}
[0m11:24:24.562069 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.56s]
[0m11:24:24.562988 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m11:24:24.563683 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m11:24:24.564549 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m11:24:24.565314 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m11:24:24.566015 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m11:24:24.570660 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m11:24:24.571953 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m11:24:24.579448 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m11:24:24.580531 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m11:24:24.581580 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:24:24.661394 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:24.662320 [debug] [Thread-1 (]: SQL status: OK in 0.081 seconds
[0m11:24:24.666491 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m11:24:24.667278 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:24.667777 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m11:24:24.668283 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m11:24:25.032837 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:24:25.033621 [debug] [Thread-1 (]: SQL status: OK in 0.365 seconds
[0m11:24:25.036167 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m11:24:25.036726 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:24:25.037223 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m11:24:25.044103 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db6a453f-97df-4d27-944a-295a8fa5a9f3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef2004a59d0>]}
[0m11:24:25.045013 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.48s]
[0m11:24:25.045790 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m11:24:25.047133 [debug] [MainThread]: On master: ROLLBACK
[0m11:24:25.047614 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:24:25.092254 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:24:25.092910 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:24:25.093405 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:24:25.093898 [debug] [MainThread]: On master: ROLLBACK
[0m11:24:25.094397 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:24:25.094884 [debug] [MainThread]: On master: Close
[0m11:24:25.100682 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:24:25.101248 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m11:24:25.101801 [info ] [MainThread]: 
[0m11:24:25.102336 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 4.63 seconds (4.63s).
[0m11:24:25.103957 [debug] [MainThread]: Command end result
[0m11:24:25.139191 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:24:25.140880 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:24:25.146975 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:24:25.147479 [info ] [MainThread]: 
[0m11:24:25.148018 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:24:25.148501 [info ] [MainThread]: 
[0m11:24:25.149047 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m11:24:25.150130 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.4521084, "process_in_blocks": "9080", "process_kernel_time": 0.223859, "process_mem_max_rss": "108612", "process_out_blocks": "2040", "process_user_time": 2.467425}
[0m11:24:25.150767 [debug] [MainThread]: Command `dbt run` succeeded at 11:24:25.150640 after 5.45 seconds
[0m11:24:25.151305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef202dfdeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef201a9bc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ef2023e53a0>]}
[0m11:24:25.151825 [debug] [MainThread]: Flushing usage events
[0m11:24:26.680791 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:13:20.867282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcf857fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcd6ddd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcd6dc7a0>]}


============================== 09:13:20.895730 | e393089a-3ff9-4a65-a764-6cd31ee25c40 ==============================
[0m09:13:20.895730 [info ] [MainThread]: Running with dbt=1.9.3
[0m09:13:20.898651 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m09:13:21.009881 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:13:21.010676 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:13:21.011299 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:13:21.180030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcd531a90>]}
[0m09:13:21.243179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcc693e60>]}
[0m09:13:21.243908 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m09:13:21.324794 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m09:13:21.438315 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:13:21.438735 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:13:21.445055 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m09:13:21.463507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcc16d790>]}
[0m09:13:21.514213 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:13:21.518106 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:13:21.566312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcc0801a0>]}
[0m09:13:21.566893 [info ] [MainThread]: Found 5 models, 473 macros
[0m09:13:21.567381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcdda1fd0>]}
[0m09:13:21.570474 [info ] [MainThread]: 
[0m09:13:21.571149 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:13:21.571680 [info ] [MainThread]: 
[0m09:13:21.572360 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:13:21.579099 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:13:21.590020 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:13:21.590443 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:13:21.590771 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:13:24.691026 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:13:24.691534 [debug] [ThreadPool]: SQL status: OK in 3.101 seconds
[0m09:13:24.782669 [debug] [ThreadPool]: On list_schemas: Close
[0m09:13:24.832118 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m09:13:24.838486 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:24.838960 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m09:13:24.839351 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m09:13:24.839736 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:13:25.416450 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:13:25.417244 [debug] [ThreadPool]: SQL status: OK in 0.577 seconds
[0m09:13:25.424793 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m09:13:25.425250 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:13:25.425612 [debug] [ThreadPool]: On list_None_default: Close
[0m09:13:25.437741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcc412cc0>]}
[0m09:13:25.438325 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:25.438690 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:13:25.442284 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m09:13:25.442912 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m09:13:25.443447 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m09:13:25.443877 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m09:13:25.451227 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m09:13:25.452999 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m09:13:25.473163 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:13:25.473600 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m09:13:25.474031 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:13:26.487391 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:26.489049 [debug] [Thread-1 (]: SQL status: OK in 1.014 seconds
[0m09:13:26.572156 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m09:13:26.573863 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:26.574451 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:13:26.575042 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m09:13:30.119366 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:30.120011 [debug] [Thread-1 (]: SQL status: OK in 3.544 seconds
[0m09:13:30.152207 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m09:13:30.152811 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:13:30.153275 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m09:13:30.169190 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fce63eff0>]}
[0m09:13:30.170181 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 4.72s]
[0m09:13:30.171039 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m09:13:30.171595 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m09:13:30.172351 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m09:13:30.173006 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m09:13:30.173556 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m09:13:30.178249 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m09:13:30.179367 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m09:13:30.185007 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:13:30.185659 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m09:13:30.186210 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:13:30.448917 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:30.450036 [debug] [Thread-1 (]: SQL status: OK in 0.264 seconds
[0m09:13:30.457117 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m09:13:30.458325 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:30.458951 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:13:30.459528 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m09:13:30.997776 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:30.998351 [debug] [Thread-1 (]: SQL status: OK in 0.538 seconds
[0m09:13:31.001050 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m09:13:31.001524 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:13:31.001933 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m09:13:31.010393 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcbedf020>]}
[0m09:13:31.011220 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.84s]
[0m09:13:31.011911 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m09:13:31.012385 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m09:13:31.012929 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m09:13:31.013450 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m09:13:31.013862 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m09:13:31.016406 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m09:13:31.017191 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m09:13:31.021189 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:13:31.021638 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m09:13:31.022008 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:13:31.180018 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:31.180642 [debug] [Thread-1 (]: SQL status: OK in 0.159 seconds
[0m09:13:31.184375 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m09:13:31.185197 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:31.185619 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:13:31.186018 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m09:13:31.581959 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:31.582516 [debug] [Thread-1 (]: SQL status: OK in 0.396 seconds
[0m09:13:31.586305 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m09:13:31.586758 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:13:31.587148 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m09:13:31.593319 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fc9d92510>]}
[0m09:13:31.594143 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.58s]
[0m09:13:31.594839 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m09:13:31.595344 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m09:13:31.596015 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m09:13:31.596602 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m09:13:31.597057 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m09:13:31.599514 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m09:13:31.600161 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m09:13:31.604082 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:13:31.604574 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m09:13:31.604981 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:13:31.745649 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:31.746270 [debug] [Thread-1 (]: SQL status: OK in 0.141 seconds
[0m09:13:31.750143 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m09:13:31.750866 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:31.751290 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:13:31.751718 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m09:13:32.136853 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:32.137725 [debug] [Thread-1 (]: SQL status: OK in 0.385 seconds
[0m09:13:32.141922 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m09:13:32.142471 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:13:32.142946 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m09:13:32.149505 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fc9d92600>]}
[0m09:13:32.150352 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.55s]
[0m09:13:32.151062 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m09:13:32.151549 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m09:13:32.152339 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m09:13:32.152996 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m09:13:32.153551 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m09:13:32.157397 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m09:13:32.158263 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m09:13:32.162284 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:13:32.162803 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m09:13:32.163274 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:13:32.318027 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:32.318656 [debug] [Thread-1 (]: SQL status: OK in 0.155 seconds
[0m09:13:32.321929 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m09:13:32.322754 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:32.323187 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:13:32.323620 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m09:13:32.807417 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:13:32.808539 [debug] [Thread-1 (]: SQL status: OK in 0.484 seconds
[0m09:13:32.814266 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m09:13:32.815522 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:13:32.816841 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m09:13:32.827519 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e393089a-3ff9-4a65-a764-6cd31ee25c40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcbede990>]}
[0m09:13:32.828587 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.67s]
[0m09:13:32.829556 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m09:13:32.831461 [debug] [MainThread]: On master: ROLLBACK
[0m09:13:32.832269 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:13:32.888606 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:13:32.890002 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:13:32.891050 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:13:32.891882 [debug] [MainThread]: On master: ROLLBACK
[0m09:13:32.892683 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:13:32.893445 [debug] [MainThread]: On master: Close
[0m09:13:32.904251 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:13:32.905342 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m09:13:32.906719 [info ] [MainThread]: 
[0m09:13:32.907697 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 11.33 seconds (11.33s).
[0m09:13:32.910530 [debug] [MainThread]: Command end result
[0m09:13:32.950885 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:13:32.953330 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:13:32.962190 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m09:13:32.962805 [info ] [MainThread]: 
[0m09:13:32.963412 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:13:32.963947 [info ] [MainThread]: 
[0m09:13:32.964535 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m09:13:32.966404 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.203745, "process_in_blocks": "79848", "process_kernel_time": 0.239458, "process_mem_max_rss": "108476", "process_out_blocks": "2056", "process_user_time": 2.326734}
[0m09:13:32.967403 [debug] [MainThread]: Command `dbt run` succeeded at 09:13:32.967183 after 12.21 seconds
[0m09:13:32.968188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcd4bfce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcc412510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9fcd50cb30>]}
[0m09:13:32.968916 [debug] [MainThread]: Flushing usage events
[0m09:13:34.784284 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:14:39.545300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b9b81040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b9f59a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b9f5b0b0>]}


============================== 09:14:39.548538 | 995887ae-2a50-417d-86ac-c77242199479 ==============================
[0m09:14:39.548538 [info ] [MainThread]: Running with dbt=1.9.3
[0m09:14:39.549144 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:14:39.606187 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:14:39.606747 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:14:39.607185 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:14:39.763526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34ba0e85c0>]}
[0m09:14:39.829957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b90882f0>]}
[0m09:14:39.830680 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m09:14:39.904849 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m09:14:39.992470 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:14:39.992926 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:14:40.000133 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m09:14:40.025410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b8f19f40>]}
[0m09:14:40.084092 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:14:40.086807 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:14:40.098269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b8c7f920>]}
[0m09:14:40.098819 [info ] [MainThread]: Found 5 models, 473 macros
[0m09:14:40.099208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b8c3cda0>]}
[0m09:14:40.101506 [info ] [MainThread]: 
[0m09:14:40.101968 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:14:40.102338 [info ] [MainThread]: 
[0m09:14:40.102963 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:14:40.109857 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:14:40.121949 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:14:40.122448 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:14:40.122818 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:14:42.683988 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:14:42.684533 [debug] [ThreadPool]: SQL status: OK in 2.562 seconds
[0m09:14:42.768460 [debug] [ThreadPool]: On list_schemas: Close
[0m09:14:42.803196 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m09:14:42.809998 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:42.810529 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m09:14:42.810947 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m09:14:42.811337 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:14:43.378861 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:14:43.379395 [debug] [ThreadPool]: SQL status: OK in 0.568 seconds
[0m09:14:43.385412 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m09:14:43.385880 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:14:43.386229 [debug] [ThreadPool]: On list_None_default: Close
[0m09:14:43.398713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b8f72540>]}
[0m09:14:43.399314 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:43.399755 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:14:43.402021 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m09:14:43.402696 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m09:14:43.403263 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m09:14:43.403749 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m09:14:43.412197 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m09:14:43.412990 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m09:14:43.436757 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:14:43.437313 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m09:14:43.437769 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:14:44.007054 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:44.007734 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m09:14:44.052695 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m09:14:44.053450 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:44.053895 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:14:44.054325 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m09:14:46.012315 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:46.013321 [debug] [Thread-1 (]: SQL status: OK in 1.958 seconds
[0m09:14:46.046701 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m09:14:46.047198 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:14:46.047673 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m09:14:46.057040 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34bb1db3b0>]}
[0m09:14:46.057805 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 2.65s]
[0m09:14:46.058496 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m09:14:46.058992 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m09:14:46.059561 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m09:14:46.060122 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m09:14:46.060581 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m09:14:46.063196 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m09:14:46.063963 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m09:14:46.067998 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:14:46.068505 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m09:14:46.068937 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:14:46.254605 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:46.255285 [debug] [Thread-1 (]: SQL status: OK in 0.186 seconds
[0m09:14:46.259516 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m09:14:46.260451 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:46.261046 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:14:46.261626 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m09:14:46.761710 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:46.762253 [debug] [Thread-1 (]: SQL status: OK in 0.500 seconds
[0m09:14:46.764666 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m09:14:46.765120 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:14:46.765522 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m09:14:46.773349 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b8a9af90>]}
[0m09:14:46.774209 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.71s]
[0m09:14:46.775044 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m09:14:46.775527 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m09:14:46.776041 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m09:14:46.776685 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m09:14:46.777102 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m09:14:46.779570 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m09:14:46.780203 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m09:14:46.786179 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:14:46.786722 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m09:14:46.787184 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:14:46.940476 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:46.941063 [debug] [Thread-1 (]: SQL status: OK in 0.154 seconds
[0m09:14:46.944107 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m09:14:46.944874 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:46.945310 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:14:46.945752 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m09:14:47.411214 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:47.412066 [debug] [Thread-1 (]: SQL status: OK in 0.466 seconds
[0m09:14:47.417564 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m09:14:47.418248 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:14:47.418880 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m09:14:47.426453 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b694fce0>]}
[0m09:14:47.427640 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.65s]
[0m09:14:47.428764 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m09:14:47.429554 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m09:14:47.430410 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m09:14:47.431163 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m09:14:47.431811 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m09:14:47.435343 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m09:14:47.436200 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m09:14:47.442210 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:14:47.442938 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m09:14:47.443585 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:14:47.620120 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:47.621118 [debug] [Thread-1 (]: SQL status: OK in 0.177 seconds
[0m09:14:47.626763 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m09:14:47.627808 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:47.628591 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:14:47.629372 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m09:14:48.085028 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:48.086050 [debug] [Thread-1 (]: SQL status: OK in 0.456 seconds
[0m09:14:48.090766 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m09:14:48.091678 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:14:48.092511 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m09:14:48.103097 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b694f9b0>]}
[0m09:14:48.104666 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.67s]
[0m09:14:48.106095 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m09:14:48.107172 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m09:14:48.108273 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m09:14:48.109233 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m09:14:48.110180 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m09:14:48.115102 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m09:14:48.116278 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m09:14:48.124105 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:14:48.125026 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m09:14:48.125885 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:14:48.335470 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:48.336637 [debug] [Thread-1 (]: SQL status: OK in 0.211 seconds
[0m09:14:48.343317 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m09:14:48.344758 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:48.345728 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:14:48.346645 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m09:14:48.988409 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:14:48.989156 [debug] [Thread-1 (]: SQL status: OK in 0.642 seconds
[0m09:14:48.992483 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m09:14:48.993097 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:14:48.993633 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m09:14:49.003278 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '995887ae-2a50-417d-86ac-c77242199479', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b8a9a570>]}
[0m09:14:49.004394 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.89s]
[0m09:14:49.005534 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m09:14:49.007304 [debug] [MainThread]: On master: ROLLBACK
[0m09:14:49.008121 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:14:49.053828 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:14:49.054758 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:14:49.055519 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:14:49.056233 [debug] [MainThread]: On master: ROLLBACK
[0m09:14:49.057002 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:14:49.057749 [debug] [MainThread]: On master: Close
[0m09:14:49.064608 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:14:49.065478 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m09:14:49.066425 [info ] [MainThread]: 
[0m09:14:49.067304 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 8.96 seconds (8.96s).
[0m09:14:49.070216 [debug] [MainThread]: Command end result
[0m09:14:49.116448 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:14:49.119212 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:14:49.131055 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m09:14:49.131619 [info ] [MainThread]: 
[0m09:14:49.132232 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:14:49.132791 [info ] [MainThread]: 
[0m09:14:49.133309 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m09:14:49.134390 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 9.648017, "process_in_blocks": "0", "process_kernel_time": 0.166528, "process_mem_max_rss": "108756", "process_out_blocks": "2056", "process_user_time": 2.220709}
[0m09:14:49.134981 [debug] [MainThread]: Command `dbt run` succeeded at 09:14:49.134861 after 9.65 seconds
[0m09:14:49.135442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34bdb4fb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34b8c4e840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e34bb275250>]}
[0m09:14:49.135955 [debug] [MainThread]: Flushing usage events
[0m09:14:50.700144 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:15:43.491637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba219e6edb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba21a75ae70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba21a4ec980>]}


============================== 09:15:43.494558 | 4fc826c5-0497-4c29-8863-6bf2540d01d1 ==============================
[0m09:15:43.494558 [info ] [MainThread]: Running with dbt=1.9.3
[0m09:15:43.495083 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m09:15:43.548072 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:15:43.548578 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:15:43.548933 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:15:43.713884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba21a8999d0>]}
[0m09:15:43.808930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba21980cef0>]}
[0m09:15:43.809856 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m09:15:43.898508 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m09:15:44.011331 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:15:44.011739 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:15:44.019093 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m09:15:44.041906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba21a094e30>]}
[0m09:15:44.099362 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:15:44.102526 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:15:44.112614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba219388ad0>]}
[0m09:15:44.113073 [info ] [MainThread]: Found 5 models, 473 macros
[0m09:15:44.113419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba2193e4a70>]}
[0m09:15:44.115212 [info ] [MainThread]: 
[0m09:15:44.115629 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:15:44.115990 [info ] [MainThread]: 
[0m09:15:44.116684 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:15:44.124827 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:15:44.139294 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:15:44.139743 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:15:44.140088 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:15:46.613294 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:15:46.613878 [debug] [ThreadPool]: SQL status: OK in 2.474 seconds
[0m09:15:46.687990 [debug] [ThreadPool]: On list_schemas: Close
[0m09:15:46.737820 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m09:15:46.745578 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:46.746208 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m09:15:46.746761 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m09:15:46.747224 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:15:47.529932 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:15:47.531046 [debug] [ThreadPool]: SQL status: OK in 0.784 seconds
[0m09:15:47.545249 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m09:15:47.546188 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:15:47.547250 [debug] [ThreadPool]: On list_None_default: Close
[0m09:15:47.574421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba2193e1b80>]}
[0m09:15:47.575119 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:47.575675 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:15:47.578189 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m09:15:47.578972 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m09:15:47.579840 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m09:15:47.580398 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m09:15:47.592011 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m09:15:47.592974 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m09:15:47.621088 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:15:47.621672 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m09:15:47.622154 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:15:48.495956 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:48.496703 [debug] [Thread-1 (]: SQL status: OK in 0.874 seconds
[0m09:15:48.554234 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m09:15:48.555660 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:48.556520 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:15:48.557413 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m09:15:50.900157 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:50.901147 [debug] [Thread-1 (]: SQL status: OK in 2.343 seconds
[0m09:15:50.929049 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m09:15:50.929720 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:15:50.930256 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m09:15:50.943770 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba219646360>]}
[0m09:15:50.944676 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 3.36s]
[0m09:15:50.945563 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m09:15:50.946193 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m09:15:50.946872 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m09:15:50.947482 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m09:15:50.948192 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m09:15:50.951516 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m09:15:50.952289 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m09:15:50.956963 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:15:50.957540 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m09:15:50.958030 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:15:51.173263 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:51.173947 [debug] [Thread-1 (]: SQL status: OK in 0.216 seconds
[0m09:15:51.177781 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m09:15:51.178733 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:51.179275 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:15:51.179809 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m09:15:51.662541 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:51.663121 [debug] [Thread-1 (]: SQL status: OK in 0.483 seconds
[0m09:15:51.665750 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m09:15:51.666248 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:15:51.666677 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m09:15:51.675210 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba2192274a0>]}
[0m09:15:51.676032 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.73s]
[0m09:15:51.676731 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m09:15:51.677231 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m09:15:51.677777 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m09:15:51.678299 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m09:15:51.678758 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m09:15:51.681476 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m09:15:51.682285 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m09:15:51.686562 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:15:51.687065 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m09:15:51.687505 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:15:51.862061 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:51.862622 [debug] [Thread-1 (]: SQL status: OK in 0.175 seconds
[0m09:15:51.865762 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m09:15:51.866544 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:51.866959 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:15:51.867397 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m09:15:52.365074 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:52.365695 [debug] [Thread-1 (]: SQL status: OK in 0.498 seconds
[0m09:15:52.371147 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m09:15:52.371878 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:15:52.372557 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m09:15:52.381087 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba2180db620>]}
[0m09:15:52.381959 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.70s]
[0m09:15:52.382683 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m09:15:52.383188 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m09:15:52.383835 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m09:15:52.384828 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m09:15:52.385531 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m09:15:52.389452 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m09:15:52.390236 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m09:15:52.395424 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:15:52.395952 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m09:15:52.396394 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:15:52.564636 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:52.565193 [debug] [Thread-1 (]: SQL status: OK in 0.169 seconds
[0m09:15:52.568614 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m09:15:52.569343 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:52.569810 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:15:52.570226 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m09:15:52.963882 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:52.964478 [debug] [Thread-1 (]: SQL status: OK in 0.394 seconds
[0m09:15:52.967006 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m09:15:52.967529 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:15:52.967981 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m09:15:52.975103 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba2180d8aa0>]}
[0m09:15:52.975986 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.59s]
[0m09:15:52.976709 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m09:15:52.977220 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m09:15:52.977864 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m09:15:52.978441 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m09:15:52.978911 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m09:15:52.981550 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m09:15:52.982248 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m09:15:52.986409 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:15:52.986902 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m09:15:52.987330 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:15:53.126602 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:53.127227 [debug] [Thread-1 (]: SQL status: OK in 0.140 seconds
[0m09:15:53.130642 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m09:15:53.131480 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:53.131944 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:15:53.132474 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m09:15:53.510844 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:15:53.511462 [debug] [Thread-1 (]: SQL status: OK in 0.378 seconds
[0m09:15:53.514261 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m09:15:53.514736 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:15:53.515151 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m09:15:53.522011 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fc826c5-0497-4c29-8863-6bf2540d01d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba219226840>]}
[0m09:15:53.523323 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.54s]
[0m09:15:53.524440 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m09:15:53.525856 [debug] [MainThread]: On master: ROLLBACK
[0m09:15:53.526261 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:15:53.563356 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:15:53.563853 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:15:53.564233 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:15:53.564618 [debug] [MainThread]: On master: ROLLBACK
[0m09:15:53.565004 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:15:53.565358 [debug] [MainThread]: On master: Close
[0m09:15:53.571709 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:15:53.572333 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m09:15:53.572936 [info ] [MainThread]: 
[0m09:15:53.573449 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 9.46 seconds (9.46s).
[0m09:15:53.575055 [debug] [MainThread]: Command end result
[0m09:15:53.596688 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:15:53.598185 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:15:53.603698 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m09:15:53.604064 [info ] [MainThread]: 
[0m09:15:53.604455 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:15:53.604914 [info ] [MainThread]: 
[0m09:15:53.605309 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m09:15:53.606227 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 10.170874, "process_in_blocks": "0", "process_kernel_time": 0.156186, "process_mem_max_rss": "108920", "process_out_blocks": "2056", "process_user_time": 2.300743}
[0m09:15:53.606748 [debug] [MainThread]: Command `dbt run` succeeded at 09:15:53.606635 after 10.17 seconds
[0m09:15:53.607168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba21a872ff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba2193e5310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ba2193e44d0>]}
[0m09:15:53.607593 [debug] [MainThread]: Flushing usage events
[0m09:15:55.236901 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:47:16.728635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccb09eade0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccb13695e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccb13688c0>]}


============================== 10:47:16.733581 | caa9939e-ac31-498e-ba3a-1d40ab234b10 ==============================
[0m10:47:16.733581 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:47:16.734174 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models warehouse.test6', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:47:16.822895 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:47:16.823515 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:47:16.823915 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:47:16.969191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'caa9939e-ac31-498e-ba3a-1d40ab234b10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccb008d3a0>]}
[0m10:47:17.028103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'caa9939e-ac31-498e-ba3a-1d40ab234b10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccb36a0650>]}
[0m10:47:17.028904 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:47:17.141078 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m10:47:17.207355 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m10:47:17.208047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'caa9939e-ac31-498e-ba3a-1d40ab234b10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccb0c57f20>]}
[0m10:47:18.139508 [error] [MainThread]: Encountered an error:
Compilation Error in model test6 (models/warehouse/test6.sql)
  expected token ',', got 'post_hook'
    line 5
      post_hook=[
[0m10:47:18.140738 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.4657038, "process_in_blocks": "7816", "process_kernel_time": 0.190948, "process_mem_max_rss": "107440", "process_out_blocks": "8", "process_user_time": 2.566865}
[0m10:47:18.141346 [debug] [MainThread]: Command `dbt run` failed at 10:47:18.141234 after 1.47 seconds
[0m10:47:18.141858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccb0819a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccafdfee10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70ccafcafbc0>]}
[0m10:47:18.142341 [debug] [MainThread]: Flushing usage events
[0m10:47:19.335175 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:47:39.070117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc71aaf7ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc71adffd70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc71ab4fad0>]}


============================== 10:47:39.073120 | f6f34ab0-bcc0-4ffb-87dd-a6d8872eb074 ==============================
[0m10:47:39.073120 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:47:39.073770 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --models warehouse.test6', 'send_anonymous_usage_stats': 'True'}
[0m10:47:39.145279 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:47:39.145818 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:47:39.146248 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:47:39.287548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f6f34ab0-bcc0-4ffb-87dd-a6d8872eb074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc719ed35c0>]}
[0m10:47:39.345747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f6f34ab0-bcc0-4ffb-87dd-a6d8872eb074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc71a9f9130>]}
[0m10:47:39.346490 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:47:39.437182 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m10:47:39.501752 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m10:47:39.502376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f6f34ab0-bcc0-4ffb-87dd-a6d8872eb074', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7197363f0>]}
[0m10:47:40.726879 [error] [MainThread]: Encountered an error:
Runtime Error
  Cannot set database in spark!
[0m10:47:40.728271 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.7134492, "process_in_blocks": "0", "process_kernel_time": 0.177788, "process_mem_max_rss": "109548", "process_out_blocks": "16", "process_user_time": 2.855605}
[0m10:47:40.728928 [debug] [MainThread]: Command `dbt run` failed at 10:47:40.728785 after 1.71 seconds
[0m10:47:40.729412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc71d9b3ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc719374b60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc7193751f0>]}
[0m10:47:40.730003 [debug] [MainThread]: Flushing usage events
[0m10:47:41.966220 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:06:13.404233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c434b93a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c43362d20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c43363ce0>]}


============================== 11:06:13.407341 | 70adf64d-3e40-4f01-a664-7f1d8ae2e66b ==============================
[0m11:06:13.407341 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:06:13.408058 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models warehouse.test6', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:06:13.484991 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:06:13.485540 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:06:13.485942 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:06:13.626239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '70adf64d-3e40-4f01-a664-7f1d8ae2e66b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c42ec8a70>]}
[0m11:06:13.689752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '70adf64d-3e40-4f01-a664-7f1d8ae2e66b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c427b94f0>]}
[0m11:06:13.690550 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:06:13.788119 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:06:13.856140 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m11:06:13.856796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '70adf64d-3e40-4f01-a664-7f1d8ae2e66b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c433cc080>]}
[0m11:06:15.104957 [error] [MainThread]: Encountered an error:
Runtime Error
  Cannot set database in spark!
[0m11:06:15.106296 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.7553918, "process_in_blocks": "4252", "process_kernel_time": 0.18677, "process_mem_max_rss": "109368", "process_out_blocks": "24", "process_user_time": 2.877465}
[0m11:06:15.107065 [debug] [MainThread]: Command `dbt run` failed at 11:06:15.106929 after 1.76 seconds
[0m11:06:15.107701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c43615af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c4223b1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x758c41192e40>]}
[0m11:06:15.108353 [debug] [MainThread]: Flushing usage events
[0m11:06:16.827859 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:08:47.171057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59e10312b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59e1a0fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59dfdc18b0>]}


============================== 11:08:47.174077 | 97ac6d89-392a-4e33-857e-e473ed464d05 ==============================
[0m11:08:47.174077 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:08:47.174720 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --models warehouse.test6', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:08:47.251979 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:08:47.252569 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:08:47.253043 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:08:47.393255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59ded66510>]}
[0m11:08:47.452173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59dee16cf0>]}
[0m11:08:47.452937 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:08:47.548394 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:08:47.621560 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m11:08:47.622191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59ded7ef60>]}
[0m11:08:48.943647 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:08:48.960902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59de9cf560>]}
[0m11:08:49.057200 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:08:49.061411 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:08:49.086710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59de9c5fa0>]}
[0m11:08:49.087261 [info ] [MainThread]: Found 7 models, 2 data tests, 1 source, 474 macros
[0m11:08:49.087796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59dd805940>]}
[0m11:08:49.089381 [info ] [MainThread]: 
[0m11:08:49.089932 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:08:49.090388 [info ] [MainThread]: 
[0m11:08:49.091028 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:08:49.091997 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:08:49.099144 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:08:49.099854 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:08:49.100775 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:08:49.288903 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:08:49.289739 [debug] [ThreadPool]: SQL status: OK in 0.189 seconds
[0m11:08:49.293922 [debug] [ThreadPool]: On list_schemas: Close
[0m11:08:49.307572 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:08:49.312734 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:49.313160 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:08:49.313597 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:08:49.313976 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:08:49.930504 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:08:49.931042 [debug] [ThreadPool]: SQL status: OK in 0.617 seconds
[0m11:08:49.935954 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:08:49.936572 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:08:49.937063 [debug] [ThreadPool]: On list_None_default: Close
[0m11:08:49.949840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59de983230>]}
[0m11:08:49.950593 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:49.951116 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:08:49.954851 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test6
[0m11:08:49.955671 [info ] [Thread-1 (]: 1 of 1 START sql table model default.test6 ..................................... [RUN]
[0m11:08:49.956383 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test6)
[0m11:08:49.956958 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test6
[0m11:08:49.967487 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test6"
[0m11:08:49.969264 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test6
[0m11:08:50.001909 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test6"
[0m11:08:50.002562 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test6"} */
drop table if exists default.test6
[0m11:08:50.003108 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:50.117720 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:08:50.118411 [debug] [Thread-1 (]: SQL status: OK in 0.115 seconds
[0m11:08:50.163986 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test6"
[0m11:08:50.164801 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:50.165271 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test6"
[0m11:08:50.165723 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test6"} */

  
    
        create table default.test6
      
      
    using parquet
      
      
      
      
      
      

      as
      -- models/warehouse/test3.sql


SELECT 
    id,
    name,
    doubled_value
FROM default.stg_test
  
[0m11:08:50.289542 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42P01', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;\n'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists\n+- 'Project ['id, 'name, 'doubled_value]\n   +- 'UnresolvedRelation [default, stg_test], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;\n'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists\n+- 'Project ['id, 'name, 'doubled_value]\n   +- 'UnresolvedRelation [default, stg_test], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:08:50.291072 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:08:50.292168 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test6"} */

  
    
        create table default.test6
      
      
    using parquet
      
      
      
      
      
      

      as
      -- models/warehouse/test3.sql


SELECT 
    id,
    name,
    doubled_value
FROM default.stg_test
  
[0m11:08:50.293738 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
  'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
  +- 'Project ['id, 'name, 'doubled_value]
     +- 'UnresolvedRelation [default, stg_test], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
  'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
  +- 'Project ['id, 'name, 'doubled_value]
     +- 'UnresolvedRelation [default, stg_test], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:08:50.295475 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: ROLLBACK
[0m11:08:50.296257 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:08:50.296779 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: Close
[0m11:08:50.314567 [debug] [Thread-1 (]: Runtime Error in model test6 (models/warehouse/test6.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:08:50.316890 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97ac6d89-392a-4e33-857e-e473ed464d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59dd7d9880>]}
[0m11:08:50.317783 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.test6 ............................ [[31mERROR[0m in 0.36s]
[0m11:08:50.318917 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test6
[0m11:08:50.320088 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.test6' to be skipped because of status 'error'.  Reason: Runtime Error in model test6 (models/warehouse/test6.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:08:50.322938 [debug] [MainThread]: On master: ROLLBACK
[0m11:08:50.323620 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:08:50.386247 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:08:50.387327 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:50.388298 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:08:50.389293 [debug] [MainThread]: On master: ROLLBACK
[0m11:08:50.390266 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:08:50.391172 [debug] [MainThread]: On master: Close
[0m11:08:50.400810 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:08:50.401816 [debug] [MainThread]: Connection 'model.dbt_spark_project.test6' was properly closed.
[0m11:08:50.402825 [info ] [MainThread]: 
[0m11:08:50.403853 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 1.31 seconds (1.31s).
[0m11:08:50.406210 [debug] [MainThread]: Command end result
[0m11:08:50.454848 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:08:50.457444 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:08:50.465812 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:08:50.466308 [info ] [MainThread]: 
[0m11:08:50.466841 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:08:50.467313 [info ] [MainThread]: 
[0m11:08:50.468070 [error] [MainThread]:   Runtime Error in model test6 (models/warehouse/test6.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:08:50.468736 [info ] [MainThread]: 
[0m11:08:50.469154 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:08:50.470150 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 3.3523939, "process_in_blocks": "2408", "process_kernel_time": 0.196952, "process_mem_max_rss": "113468", "process_out_blocks": "3056", "process_user_time": 3.349192}
[0m11:08:50.470785 [debug] [MainThread]: Command `dbt run` failed at 11:08:50.470663 after 3.35 seconds
[0m11:08:50.471273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59dfaa96d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59dfaaa570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e59e1494ec0>]}
[0m11:08:50.471723 [debug] [MainThread]: Flushing usage events
[0m11:08:52.069980 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:09:20.175241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded74bd1e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded75355850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded74bd1e50>]}


============================== 11:09:20.178467 | 72259323-db87-4038-b375-b37a0784e98b ==============================
[0m11:09:20.178467 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:09:20.179123 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --models warehouse.test6', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:09:20.257368 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:09:20.257911 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:09:20.258300 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:09:20.396245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '72259323-db87-4038-b375-b37a0784e98b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded73edcbf0>]}
[0m11:09:20.455212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '72259323-db87-4038-b375-b37a0784e98b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded755633e0>]}
[0m11:09:20.455956 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:09:20.549933 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:09:20.658759 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:09:20.659209 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:09:20.665803 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:09:20.715918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '72259323-db87-4038-b375-b37a0784e98b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded73f3e3c0>]}
[0m11:09:20.798795 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:09:20.801652 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:09:20.812526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '72259323-db87-4038-b375-b37a0784e98b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded73b9c260>]}
[0m11:09:20.813075 [info ] [MainThread]: Found 7 models, 2 data tests, 1 source, 474 macros
[0m11:09:20.813524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '72259323-db87-4038-b375-b37a0784e98b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded74038410>]}
[0m11:09:20.815141 [info ] [MainThread]: 
[0m11:09:20.815674 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:09:20.816155 [info ] [MainThread]: 
[0m11:09:20.816834 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:09:20.817903 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:09:20.837936 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:09:20.838593 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:09:20.839069 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:09:20.957581 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:09:20.958501 [debug] [ThreadPool]: SQL status: OK in 0.119 seconds
[0m11:09:20.963506 [debug] [ThreadPool]: On list_schemas: Close
[0m11:09:20.980904 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:09:20.986573 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:20.987090 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:09:20.987568 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:09:20.987981 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:09:21.212775 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:09:21.213474 [debug] [ThreadPool]: SQL status: OK in 0.225 seconds
[0m11:09:21.218722 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:09:21.219610 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:09:21.220103 [debug] [ThreadPool]: On list_None_default: Close
[0m11:09:21.229034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '72259323-db87-4038-b375-b37a0784e98b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded74021280>]}
[0m11:09:21.229640 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:21.230081 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:09:21.232282 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test6
[0m11:09:21.232940 [info ] [Thread-1 (]: 1 of 1 START sql table model default.test6 ..................................... [RUN]
[0m11:09:21.233536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test6)
[0m11:09:21.234003 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test6
[0m11:09:21.242326 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test6"
[0m11:09:21.243230 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test6
[0m11:09:21.274462 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test6"
[0m11:09:21.274991 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test6"} */
drop table if exists default.test6
[0m11:09:21.275446 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:21.412958 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:09:21.414068 [debug] [Thread-1 (]: SQL status: OK in 0.138 seconds
[0m11:09:21.487216 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test6"
[0m11:09:21.488063 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:21.488556 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test6"
[0m11:09:21.489022 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test6"} */

  
    
        create table default.test6
      
      
    using parquet
      
      
      
      
      
      

      as
      -- models/warehouse/test3.sql


SELECT 
    id,
    name,
    doubled_value
FROM default.stg_test
  
[0m11:09:21.522354 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42P01', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;\n'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists\n+- 'Project ['id, 'name, 'doubled_value]\n   +- 'UnresolvedRelation [default, stg_test], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;\n'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists\n+- 'Project ['id, 'name, 'doubled_value]\n   +- 'UnresolvedRelation [default, stg_test], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:09:21.523230 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:09:21.523803 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test6"} */

  
    
        create table default.test6
      
      
    using parquet
      
      
      
      
      
      

      as
      -- models/warehouse/test3.sql


SELECT 
    id,
    name,
    doubled_value
FROM default.stg_test
  
[0m11:09:21.524519 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
  'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
  +- 'Project ['id, 'name, 'doubled_value]
     +- 'UnresolvedRelation [default, stg_test], [], false
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
  'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
  +- 'Project ['id, 'name, 'doubled_value]
     +- 'UnresolvedRelation [default, stg_test], [], false
  
  	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:09:21.525313 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: ROLLBACK
[0m11:09:21.525741 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:09:21.526143 [debug] [Thread-1 (]: On model.dbt_spark_project.test6: Close
[0m11:09:21.535343 [debug] [Thread-1 (]: Runtime Error in model test6 (models/warehouse/test6.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:09:21.537214 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72259323-db87-4038-b375-b37a0784e98b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded73edc230>]}
[0m11:09:21.537919 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.test6 ............................ [[31mERROR[0m in 0.30s]
[0m11:09:21.538831 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test6
[0m11:09:21.539689 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.test6' to be skipped because of status 'error'.  Reason: Runtime Error in model test6 (models/warehouse/test6.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:09:21.541658 [debug] [MainThread]: On master: ROLLBACK
[0m11:09:21.542101 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:09:21.609053 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:09:21.610052 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:21.610779 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:09:21.611460 [debug] [MainThread]: On master: ROLLBACK
[0m11:09:21.612144 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:09:21.612810 [debug] [MainThread]: On master: Close
[0m11:09:21.620648 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:09:21.621419 [debug] [MainThread]: Connection 'model.dbt_spark_project.test6' was properly closed.
[0m11:09:21.622152 [info ] [MainThread]: 
[0m11:09:21.623002 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.81 seconds (0.81s).
[0m11:09:21.624790 [debug] [MainThread]: Command end result
[0m11:09:21.669309 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:09:21.671891 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:09:21.681630 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:09:21.682392 [info ] [MainThread]: 
[0m11:09:21.683286 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:09:21.684092 [info ] [MainThread]: 
[0m11:09:21.685363 [error] [MainThread]:   Runtime Error in model test6 (models/warehouse/test6.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`stg_test` cannot be found. Verify the spelling and correctness of the schema and catalog.
    If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
    To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 24 pos 5;
    'CreateTable `spark_catalog`.`default`.`test6`, ErrorIfExists
    +- 'Project ['id, 'name, 'doubled_value]
       +- 'UnresolvedRelation [default, stg_test], [], false
    
    	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:09:21.686647 [info ] [MainThread]: 
[0m11:09:21.687455 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:09:21.689227 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.5740862, "process_in_blocks": "0", "process_kernel_time": 0.195711, "process_mem_max_rss": "109292", "process_out_blocks": "2096", "process_user_time": 2.122867}
[0m11:09:21.690615 [debug] [MainThread]: Command `dbt run` failed at 11:09:21.690300 after 1.58 seconds
[0m11:09:21.691944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded746dba40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded748ac860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ded73d21100>]}
[0m11:09:21.693260 [debug] [MainThread]: Flushing usage events
[0m11:09:22.895607 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:09:53.473275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e772e94d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e771185760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e7714ae9f0>]}


============================== 11:09:53.476445 | 391d07bd-a758-4987-9401-4f15d64c0e85 ==============================
[0m11:09:53.476445 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:09:53.477450 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --models stg_test', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:09:53.553130 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:09:53.553810 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:09:53.554298 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:09:53.703990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '391d07bd-a758-4987-9401-4f15d64c0e85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e771c2f620>]}
[0m11:09:53.768821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '391d07bd-a758-4987-9401-4f15d64c0e85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e7708fa540>]}
[0m11:09:53.769732 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:09:53.872863 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:09:53.981318 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:09:53.981872 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:09:53.988337 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:09:54.037225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '391d07bd-a758-4987-9401-4f15d64c0e85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e7706a57c0>]}
[0m11:09:54.118884 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:09:54.121486 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:09:54.132834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '391d07bd-a758-4987-9401-4f15d64c0e85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e7703ae810>]}
[0m11:09:54.133467 [info ] [MainThread]: Found 7 models, 2 data tests, 1 source, 474 macros
[0m11:09:54.134047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '391d07bd-a758-4987-9401-4f15d64c0e85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e770681820>]}
[0m11:09:54.135628 [info ] [MainThread]: 
[0m11:09:54.136138 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:09:54.136665 [info ] [MainThread]: 
[0m11:09:54.137352 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:09:54.138268 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:09:54.158453 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:09:54.159506 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:09:54.160375 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:09:54.235157 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:09:54.235781 [debug] [ThreadPool]: SQL status: OK in 0.075 seconds
[0m11:09:54.239684 [debug] [ThreadPool]: On list_schemas: Close
[0m11:09:54.258947 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:09:54.266225 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:54.266802 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:09:54.267306 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:09:54.267775 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:09:54.489295 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:09:54.490253 [debug] [ThreadPool]: SQL status: OK in 0.222 seconds
[0m11:09:54.495927 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:09:54.496758 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:09:54.497446 [debug] [ThreadPool]: On list_None_default: Close
[0m11:09:54.505960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '391d07bd-a758-4987-9401-4f15d64c0e85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e7705cad20>]}
[0m11:09:54.506697 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:54.507263 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:09:54.509734 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.stg_test
[0m11:09:54.510523 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stg_test ................................... [RUN]
[0m11:09:54.511248 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.stg_test)
[0m11:09:54.511876 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.stg_test
[0m11:09:54.523033 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.stg_test"
[0m11:09:54.524528 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.stg_test
[0m11:09:54.569409 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.stg_test"
[0m11:09:54.570445 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:54.571092 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.stg_test"
[0m11:09:54.571807 [debug] [Thread-1 (]: On model.dbt_spark_project.stg_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
create or replace view default.stg_test
  
  
  as
    -- Sử dụng External Table để đọc dữ liệu từ Oracle
CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    id INT,
    name STRING,
    doubled_value INT
)
USING JDBC
OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'METADATA.TEST',
    user 'tafj',
    password 't24tafj',
    driver 'oracle.jdbc.driver.OracleDriver'
)

[0m11:09:54.572457 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:09:54.638842 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 7, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */\ncreate or replace view default.stg_test\n  \n  \n  as\n    -- Sử dụng External Table để đọc dữ liệu từ Oracle\nCREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (\n^^^\n    id INT,\n    name STRING,\n    doubled_value INT\n)\nUSING JDBC\nOPTIONS (\n    url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n    dbtable \'METADATA.TEST\',\n    user \'tafj\',\n    password \'t24tafj\',\n    driver \'oracle.jdbc.driver.OracleDriver\'\n)\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 7, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */\ncreate or replace view default.stg_test\n  \n  \n  as\n    -- Sử dụng External Table để đọc dữ liệu từ Oracle\nCREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (\n^^^\n    id INT,\n    name STRING,\n    doubled_value INT\n)\nUSING JDBC\nOPTIONS (\n    url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n    dbtable \'METADATA.TEST\',\n    user \'tafj\',\n    password \'t24tafj\',\n    driver \'oracle.jdbc.driver.OracleDriver\'\n)\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:09:54.640081 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:09:54.641453 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
create or replace view default.stg_test
  
  
  as
    -- Sử dụng External Table để đọc dữ liệu từ Oracle
CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    id INT,
    name STRING,
    doubled_value INT
)
USING JDBC
OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'METADATA.TEST',
    user 'tafj',
    password 't24tafj',
    driver 'oracle.jdbc.driver.OracleDriver'
)

[0m11:09:54.643208 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
  create or replace view default.stg_test
    
    
    as
      -- Sử dụng External Table để đọc dữ liệu từ Oracle
  CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
  ^^^
      id INT,
      name STRING,
      doubled_value INT
  )
  USING JDBC
  OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'METADATA.TEST',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.driver.OracleDriver'
  )
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
  create or replace view default.stg_test
    
    
    as
      -- Sử dụng External Table để đọc dữ liệu từ Oracle
  CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
  ^^^
      id INT,
      name STRING,
      doubled_value INT
  )
  USING JDBC
  OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'METADATA.TEST',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.driver.OracleDriver'
  )
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:09:54.645260 [debug] [Thread-1 (]: On model.dbt_spark_project.stg_test: ROLLBACK
[0m11:09:54.646373 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:09:54.647269 [debug] [Thread-1 (]: On model.dbt_spark_project.stg_test: Close
[0m11:09:54.660806 [debug] [Thread-1 (]: Runtime Error in model stg_test (models/staging/stg_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
    create or replace view default.stg_test
      
      
      as
        -- Sử dụng External Table để đọc dữ liệu từ Oracle
    CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    ^^^
        id INT,
        name STRING,
        doubled_value INT
    )
    USING JDBC
    OPTIONS (
        url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
        dbtable 'METADATA.TEST',
        user 'tafj',
        password 't24tafj',
        driver 'oracle.jdbc.driver.OracleDriver'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
    create or replace view default.stg_test
      
      
      as
        -- Sử dụng External Table để đọc dữ liệu từ Oracle
    CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    ^^^
        id INT,
        name STRING,
        doubled_value INT
    )
    USING JDBC
    OPTIONS (
        url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
        dbtable 'METADATA.TEST',
        user 'tafj',
        password 't24tafj',
        driver 'oracle.jdbc.driver.OracleDriver'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:09:54.663176 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '391d07bd-a758-4987-9401-4f15d64c0e85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e77046ec00>]}
[0m11:09:54.664088 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stg_test .......................... [[31mERROR[0m in 0.15s]
[0m11:09:54.665297 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.stg_test
[0m11:09:54.666842 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.stg_test' to be skipped because of status 'error'.  Reason: Runtime Error in model stg_test (models/staging/stg_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
    create or replace view default.stg_test
      
      
      as
        -- Sử dụng External Table để đọc dữ liệu từ Oracle
    CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    ^^^
        id INT,
        name STRING,
        doubled_value INT
    )
    USING JDBC
    OPTIONS (
        url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
        dbtable 'METADATA.TEST',
        user 'tafj',
        password 't24tafj',
        driver 'oracle.jdbc.driver.OracleDriver'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
    create or replace view default.stg_test
      
      
      as
        -- Sử dụng External Table để đọc dữ liệu từ Oracle
    CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    ^^^
        id INT,
        name STRING,
        doubled_value INT
    )
    USING JDBC
    OPTIONS (
        url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
        dbtable 'METADATA.TEST',
        user 'tafj',
        password 't24tafj',
        driver 'oracle.jdbc.driver.OracleDriver'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:09:54.669949 [debug] [MainThread]: On master: ROLLBACK
[0m11:09:54.670761 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:09:54.724271 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:09:54.724920 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:09:54.725540 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:09:54.726120 [debug] [MainThread]: On master: ROLLBACK
[0m11:09:54.726709 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:09:54.727244 [debug] [MainThread]: On master: Close
[0m11:09:54.733678 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:09:54.734320 [debug] [MainThread]: Connection 'model.dbt_spark_project.stg_test' was properly closed.
[0m11:09:54.734960 [info ] [MainThread]: 
[0m11:09:54.735623 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.60 seconds (0.60s).
[0m11:09:54.736896 [debug] [MainThread]: Command end result
[0m11:09:54.775564 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:09:54.777785 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:09:54.787074 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:09:54.787827 [info ] [MainThread]: 
[0m11:09:54.788763 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:09:54.789648 [info ] [MainThread]: 
[0m11:09:54.790974 [error] [MainThread]:   Runtime Error in model stg_test (models/staging/stg_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
    create or replace view default.stg_test
      
      
      as
        -- Sử dụng External Table để đọc dữ liệu từ Oracle
    CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    ^^^
        id INT,
        name STRING,
        doubled_value INT
    )
    USING JDBC
    OPTIONS (
        url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
        dbtable 'METADATA.TEST',
        user 'tafj',
        password 't24tafj',
        driver 'oracle.jdbc.driver.OracleDriver'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 7, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.stg_test"} */
    create or replace view default.stg_test
      
      
      as
        -- Sử dụng External Table để đọc dữ liệu từ Oracle
    CREATE EXTERNAL TABLE IF NOT EXISTS metadata.test (
    ^^^
        id INT,
        name STRING,
        doubled_value INT
    )
    USING JDBC
    OPTIONS (
        url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
        dbtable 'METADATA.TEST',
        user 'tafj',
        password 't24tafj',
        driver 'oracle.jdbc.driver.OracleDriver'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:09:54.792332 [info ] [MainThread]: 
[0m11:09:54.793254 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:09:54.795181 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3748418, "process_in_blocks": "0", "process_kernel_time": 0.178757, "process_mem_max_rss": "108572", "process_out_blocks": "2064", "process_user_time": 2.058205}
[0m11:09:54.796324 [debug] [MainThread]: Command `dbt run` failed at 11:09:54.796120 after 1.38 seconds
[0m11:09:54.797293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e772e94d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e771a58080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74e771a58e00>]}
[0m11:09:54.798396 [debug] [MainThread]: Flushing usage events
[0m11:09:56.073086 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:44:20.088265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b326c1520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b323efbf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b32c03980>]}


============================== 13:44:20.092882 | e8e065e7-4d56-45d5-a8df-aa7bdbbb184e ==============================
[0m13:44:20.092882 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:44:20.093728 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --models source_oracle', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:44:20.193636 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:44:20.194270 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:44:20.194765 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:44:20.351413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8e065e7-4d56-45d5-a8df-aa7bdbbb184e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b31d0ae40>]}
[0m13:44:20.409689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8e065e7-4d56-45d5-a8df-aa7bdbbb184e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b31d5d640>]}
[0m13:44:20.410442 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:44:20.513492 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:44:20.675656 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 2 files added, 1 files changed.
[0m13:44:20.676398 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/staging/source_oracle.sql
[0m13:44:20.676933 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/staging/load_to_oracle.sql
[0m13:44:20.677562 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/staging/schema.yml
[0m13:44:20.678029 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/warehouse/test6.sql
[0m13:44:20.678502 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://macros/load_to_oracle.sql
[0m13:44:20.678955 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/staging/stg_test.sql
[0m13:44:20.865270 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'ORACLE_HOST'
[0m13:44:20.866546 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.83325875, "process_in_blocks": "54552", "process_kernel_time": 0.237251, "process_mem_max_rss": "107448", "process_out_blocks": "24", "process_user_time": 2.002562}
[0m13:44:20.867137 [debug] [MainThread]: Command `dbt run` failed at 13:44:20.866991 after 0.83 seconds
[0m13:44:20.867620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b35bb3ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b319700e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x772b31840e90>]}
[0m13:44:20.868111 [debug] [MainThread]: Flushing usage events
[0m13:44:22.225136 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:46:34.970446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2bb3a540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2c71c2c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2bc3c590>]}


============================== 13:46:34.973528 | 38a963dd-30be-499c-8084-16f964b53bf9 ==============================
[0m13:46:34.973528 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:46:34.974248 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --models source_oracle', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:46:35.052965 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:46:35.053635 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:46:35.054106 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:46:35.190824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '38a963dd-30be-499c-8084-16f964b53bf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2c1a40e0>]}
[0m13:46:35.251992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '38a963dd-30be-499c-8084-16f964b53bf9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2afdc1a0>]}
[0m13:46:35.252698 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:46:35.348476 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:46:35.461367 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 2 files added, 1 files changed.
[0m13:46:35.462076 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/staging/load_to_oracle.sql
[0m13:46:35.462614 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/staging/source_oracle.sql
[0m13:46:35.463192 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/staging/schema.yml
[0m13:46:35.463636 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/staging/stg_test.sql
[0m13:46:35.464063 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/warehouse/test6.sql
[0m13:46:35.464520 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://macros/load_to_oracle.sql
[0m13:46:35.650235 [error] [MainThread]: Encountered an error:
Parsing Error
  Env var required but not provided: 'ORACLE_HOST'
[0m13:46:35.651376 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7334293, "process_in_blocks": "8", "process_kernel_time": 0.17692, "process_mem_max_rss": "108008", "process_out_blocks": "16", "process_user_time": 1.860643}
[0m13:46:35.652034 [debug] [MainThread]: Command `dbt run` failed at 13:46:35.651885 after 0.73 seconds
[0m13:46:35.652495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2b6e2e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2f1ab410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fab2b6f87d0>]}
[0m13:46:35.653018 [debug] [MainThread]: Flushing usage events
[0m13:46:36.984680 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:49:43.951796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe735069cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe735961e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe735068410>]}


============================== 13:49:43.954855 | 732fcd0a-24e5-4c38-b325-f4c6a4f02b8d ==============================
[0m13:49:43.954855 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:49:43.955548 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --models source_oracle', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:49:44.028847 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:49:44.029452 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:49:44.029860 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:49:44.165904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '732fcd0a-24e5-4c38-b325-f4c6a4f02b8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73437c530>]}
[0m13:49:44.224699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '732fcd0a-24e5-4c38-b325-f4c6a4f02b8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73447aa80>]}
[0m13:49:44.225509 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:49:44.315569 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:49:44.420863 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 2 files added, 1 files changed.
[0m13:49:44.421540 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/staging/source_oracle.sql
[0m13:49:44.421986 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/staging/load_to_oracle.sql
[0m13:49:44.422534 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/staging/schema.yml
[0m13:49:44.422916 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://macros/load_to_oracle.sql
[0m13:49:44.423347 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/staging/stg_test.sql
[0m13:49:44.423732 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/warehouse/test6.sql
[0m13:49:44.663363 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m13:49:44.668490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '732fcd0a-24e5-4c38-b325-f4c6a4f02b8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73429aa20>]}
[0m13:49:44.735495 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:49:44.738383 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:49:44.757329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '732fcd0a-24e5-4c38-b325-f4c6a4f02b8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe733d35550>]}
[0m13:49:44.758020 [info ] [MainThread]: Found 7 models, 473 macros
[0m13:49:44.758569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '732fcd0a-24e5-4c38-b325-f4c6a4f02b8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73426dc70>]}
[0m13:49:44.760493 [info ] [MainThread]: 
[0m13:49:44.761213 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:49:44.761795 [info ] [MainThread]: 
[0m13:49:44.762494 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:49:44.763665 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:49:44.782305 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:49:44.783369 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:49:44.784237 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:49:45.329179 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:49:45.330124 [debug] [ThreadPool]: SQL status: OK in 0.546 seconds
[0m13:49:45.335283 [debug] [ThreadPool]: On list_schemas: Close
[0m13:49:45.364474 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m13:49:45.369346 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:45.369760 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m13:49:45.370181 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m13:49:45.370548 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:49:45.938399 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:49:45.939325 [debug] [ThreadPool]: SQL status: OK in 0.569 seconds
[0m13:49:45.945939 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m13:49:45.946648 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:49:45.947203 [debug] [ThreadPool]: On list_None_default: Close
[0m13:49:45.969678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '732fcd0a-24e5-4c38-b325-f4c6a4f02b8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe733d666c0>]}
[0m13:49:45.970913 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:45.971918 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:49:45.976235 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.source_oracle
[0m13:49:45.977192 [info ] [Thread-1 (]: 1 of 1 START sql view model default.source_oracle .............................. [RUN]
[0m13:49:45.978071 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.source_oracle)
[0m13:49:45.978734 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.source_oracle
[0m13:49:45.989064 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.source_oracle"
[0m13:49:45.992653 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.source_oracle
[0m13:49:46.036945 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.source_oracle"
[0m13:49:46.038139 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:46.038724 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.source_oracle"
[0m13:49:46.039320 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
create or replace view default.source_oracle
  
  
  as
    

CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
USING org.apache.spark.sql.jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj'
)

[0m13:49:46.039915 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:49:46.148404 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 8, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\ncreate or replace view default.source_oracle\n  \n  \n  as\n    \n\nCREATE OR REPLACE TEMPORARY VIEW source_oracle_test\n^^^\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\'\n)\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 8, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\ncreate or replace view default.source_oracle\n  \n  \n  as\n    \n\nCREATE OR REPLACE TEMPORARY VIEW source_oracle_test\n^^^\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\'\n)\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m13:49:46.149914 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m13:49:46.151021 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
create or replace view default.source_oracle
  
  
  as
    

CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
USING org.apache.spark.sql.jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj'
)

[0m13:49:46.152256 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  create or replace view default.source_oracle
    
    
    as
      
  
  CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
  ^^^
  USING org.apache.spark.sql.jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj'
  )
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  create or replace view default.source_oracle
    
    
    as
      
  
  CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
  ^^^
  USING org.apache.spark.sql.jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj'
  )
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m13:49:46.153699 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: ROLLBACK
[0m13:49:46.154568 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:49:46.155281 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: Close
[0m13:49:46.178876 [debug] [Thread-1 (]: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
    ^^^
    USING org.apache.spark.sql.jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
    ^^^
    USING org.apache.spark.sql.jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:49:46.181434 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '732fcd0a-24e5-4c38-b325-f4c6a4f02b8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe732e57f80>]}
[0m13:49:46.182390 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.source_oracle ..................... [[31mERROR[0m in 0.20s]
[0m13:49:46.183516 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.source_oracle
[0m13:49:46.184741 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.source_oracle' to be skipped because of status 'error'.  Reason: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
    ^^^
    USING org.apache.spark.sql.jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
    ^^^
    USING org.apache.spark.sql.jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m13:49:46.187521 [debug] [MainThread]: On master: ROLLBACK
[0m13:49:46.188339 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:49:46.272807 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:49:46.273552 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:49:46.274165 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:49:46.274668 [debug] [MainThread]: On master: ROLLBACK
[0m13:49:46.275196 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:49:46.275662 [debug] [MainThread]: On master: Close
[0m13:49:46.289492 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:49:46.290337 [debug] [MainThread]: Connection 'model.dbt_spark_project.source_oracle' was properly closed.
[0m13:49:46.291183 [info ] [MainThread]: 
[0m13:49:46.292138 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.53 seconds (1.53s).
[0m13:49:46.293898 [debug] [MainThread]: Command end result
[0m13:49:46.329442 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:49:46.331154 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:49:46.336951 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m13:49:46.337438 [info ] [MainThread]: 
[0m13:49:46.338052 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:49:46.338560 [info ] [MainThread]: 
[0m13:49:46.339346 [error] [MainThread]:   Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
    ^^^
    USING org.apache.spark.sql.jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 8, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    CREATE OR REPLACE TEMPORARY VIEW source_oracle_test
    ^^^
    USING org.apache.spark.sql.jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:49:46.340036 [info ] [MainThread]: 
[0m13:49:46.340578 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:49:46.341752 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.442949, "process_in_blocks": "1496", "process_kernel_time": 0.201921, "process_mem_max_rss": "112504", "process_out_blocks": "2968", "process_user_time": 2.160458}
[0m13:49:46.342418 [debug] [MainThread]: Command `dbt run` failed at 13:49:46.342267 after 2.44 seconds
[0m13:49:46.343157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe734d03c80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7340b1be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7356aa060>]}
[0m13:49:46.343795 [debug] [MainThread]: Flushing usage events
[0m13:49:47.953267 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:54:11.510068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e4099340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e439d760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e3f52c30>]}


============================== 13:54:11.514371 | c1735091-8ca4-4851-9135-b737a1297b1e ==============================
[0m13:54:11.514371 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:54:11.515307 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --models source_oracle', 'send_anonymous_usage_stats': 'True'}
[0m13:54:11.639013 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:54:11.640247 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:54:11.641255 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:54:11.837377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c1735091-8ca4-4851-9135-b737a1297b1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e2e16900>]}
[0m13:54:11.911443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c1735091-8ca4-4851-9135-b737a1297b1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e2e59af0>]}
[0m13:54:11.912146 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:54:12.042328 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:54:12.137130 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:54:12.138424 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/staging/source_oracle.sql
[0m13:54:12.387599 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m13:54:12.393202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c1735091-8ca4-4851-9135-b737a1297b1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e28c9ca0>]}
[0m13:54:12.482115 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:54:12.486309 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:54:12.503301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c1735091-8ca4-4851-9135-b737a1297b1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e2b0ac30>]}
[0m13:54:12.503880 [info ] [MainThread]: Found 7 models, 473 macros
[0m13:54:12.504399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1735091-8ca4-4851-9135-b737a1297b1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e2867830>]}
[0m13:54:12.506013 [info ] [MainThread]: 
[0m13:54:12.506478 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:54:12.506900 [info ] [MainThread]: 
[0m13:54:12.507484 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:54:12.508382 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:54:12.524694 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:54:12.525938 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:54:12.526851 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:54:13.312786 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:54:13.314607 [debug] [ThreadPool]: SQL status: OK in 0.788 seconds
[0m13:54:13.326132 [debug] [ThreadPool]: On list_schemas: Close
[0m13:54:13.349936 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m13:54:13.356433 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:13.357059 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m13:54:13.357569 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m13:54:13.358076 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:54:13.658840 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:54:13.659714 [debug] [ThreadPool]: SQL status: OK in 0.302 seconds
[0m13:54:13.664575 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m13:54:13.665210 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:54:13.665717 [debug] [ThreadPool]: On list_None_default: Close
[0m13:54:13.673843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1735091-8ca4-4851-9135-b737a1297b1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e1a0fbf0>]}
[0m13:54:13.674408 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:13.674799 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:54:13.676834 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.source_oracle
[0m13:54:13.677409 [info ] [Thread-1 (]: 1 of 1 START sql view model default.source_oracle .............................. [RUN]
[0m13:54:13.677913 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.source_oracle)
[0m13:54:13.678349 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.source_oracle
[0m13:54:13.686325 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.source_oracle"
[0m13:54:13.687150 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.source_oracle
[0m13:54:13.739522 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.source_oracle"
[0m13:54:13.740767 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:13.741441 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.source_oracle"
[0m13:54:13.742099 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
create or replace view default.source_oracle
  
  
  as
    

-- Tạo view tạm thời mà không dùng OR REPLACE
CREATE TEMPORARY VIEW source_oracle_test
AS
SELECT *
FROM jdbc(
  'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
  'dbtable'='metadata.test',
  'user'='tafj',
  'password'='t24tafj'
)

[0m13:54:13.742612 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:54:13.800920 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 9, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\ncreate or replace view default.source_oracle\n  \n  \n  as\n    \n\n-- Tạo view tạm thời mà không dùng OR REPLACE\nCREATE TEMPORARY VIEW source_oracle_test\n^^^\nAS\nSELECT *\nFROM jdbc(\n  \'url\'=\'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  \'dbtable\'=\'metadata.test\',\n  \'user\'=\'tafj\',\n  \'password\'=\'t24tafj\'\n)\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 9, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\ncreate or replace view default.source_oracle\n  \n  \n  as\n    \n\n-- Tạo view tạm thời mà không dùng OR REPLACE\nCREATE TEMPORARY VIEW source_oracle_test\n^^^\nAS\nSELECT *\nFROM jdbc(\n  \'url\'=\'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  \'dbtable\'=\'metadata.test\',\n  \'user\'=\'tafj\',\n  \'password\'=\'t24tafj\'\n)\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m13:54:13.802521 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m13:54:13.803775 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
create or replace view default.source_oracle
  
  
  as
    

-- Tạo view tạm thời mà không dùng OR REPLACE
CREATE TEMPORARY VIEW source_oracle_test
AS
SELECT *
FROM jdbc(
  'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
  'dbtable'='metadata.test',
  'user'='tafj',
  'password'='t24tafj'
)

[0m13:54:13.805338 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  create or replace view default.source_oracle
    
    
    as
      
  
  -- Tạo view tạm thời mà không dùng OR REPLACE
  CREATE TEMPORARY VIEW source_oracle_test
  ^^^
  AS
  SELECT *
  FROM jdbc(
    'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
    'dbtable'='metadata.test',
    'user'='tafj',
    'password'='t24tafj'
  )
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  create or replace view default.source_oracle
    
    
    as
      
  
  -- Tạo view tạm thời mà không dùng OR REPLACE
  CREATE TEMPORARY VIEW source_oracle_test
  ^^^
  AS
  SELECT *
  FROM jdbc(
    'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
    'dbtable'='metadata.test',
    'user'='tafj',
    'password'='t24tafj'
  )
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m13:54:13.807177 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: ROLLBACK
[0m13:54:13.808218 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:54:13.809197 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: Close
[0m13:54:13.827661 [debug] [Thread-1 (]: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    -- Tạo view tạm thời mà không dùng OR REPLACE
    CREATE TEMPORARY VIEW source_oracle_test
    ^^^
    AS
    SELECT *
    FROM jdbc(
      'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
      'dbtable'='metadata.test',
      'user'='tafj',
      'password'='t24tafj'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    -- Tạo view tạm thời mà không dùng OR REPLACE
    CREATE TEMPORARY VIEW source_oracle_test
    ^^^
    AS
    SELECT *
    FROM jdbc(
      'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
      'dbtable'='metadata.test',
      'user'='tafj',
      'password'='t24tafj'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:54:13.831621 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1735091-8ca4-4851-9135-b737a1297b1e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e411e600>]}
[0m13:54:13.833214 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.source_oracle ..................... [[31mERROR[0m in 0.15s]
[0m13:54:13.835222 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.source_oracle
[0m13:54:13.837084 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.source_oracle' to be skipped because of status 'error'.  Reason: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    -- Tạo view tạm thời mà không dùng OR REPLACE
    CREATE TEMPORARY VIEW source_oracle_test
    ^^^
    AS
    SELECT *
    FROM jdbc(
      'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
      'dbtable'='metadata.test',
      'user'='tafj',
      'password'='t24tafj'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    -- Tạo view tạm thời mà không dùng OR REPLACE
    CREATE TEMPORARY VIEW source_oracle_test
    ^^^
    AS
    SELECT *
    FROM jdbc(
      'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
      'dbtable'='metadata.test',
      'user'='tafj',
      'password'='t24tafj'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m13:54:13.841105 [debug] [MainThread]: On master: ROLLBACK
[0m13:54:13.841973 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:54:13.915383 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:54:13.916529 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:54:13.917600 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:54:13.918486 [debug] [MainThread]: On master: ROLLBACK
[0m13:54:13.919859 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:54:13.920960 [debug] [MainThread]: On master: Close
[0m13:54:13.931736 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:54:13.932482 [debug] [MainThread]: Connection 'model.dbt_spark_project.source_oracle' was properly closed.
[0m13:54:13.933157 [info ] [MainThread]: 
[0m13:54:13.933827 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 1.43 seconds (1.43s).
[0m13:54:13.935210 [debug] [MainThread]: Command end result
[0m13:54:13.975810 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:54:13.979293 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:54:13.992235 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m13:54:13.993187 [info ] [MainThread]: 
[0m13:54:13.994517 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:54:13.995456 [info ] [MainThread]: 
[0m13:54:13.997060 [error] [MainThread]:   Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    -- Tạo view tạm thời mà không dùng OR REPLACE
    CREATE TEMPORARY VIEW source_oracle_test
    ^^^
    AS
    SELECT *
    FROM jdbc(
      'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
      'dbtable'='metadata.test',
      'user'='tafj',
      'password'='t24tafj'
    )
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 9, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    create or replace view default.source_oracle
      
      
      as
        
    
    -- Tạo view tạm thời mà không dùng OR REPLACE
    CREATE TEMPORARY VIEW source_oracle_test
    ^^^
    AS
    SELECT *
    FROM jdbc(
      'url'='jdbc:oracle:thin:@10.14.223.202:1521/R18',
      'dbtable'='metadata.test',
      'user'='tafj',
      'password'='t24tafj'
    )
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:54:13.998910 [info ] [MainThread]: 
[0m13:54:14.000280 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:54:14.001859 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.559499, "process_in_blocks": "0", "process_kernel_time": 0.261776, "process_mem_max_rss": "111756", "process_out_blocks": "2968", "process_user_time": 2.462898}
[0m13:54:14.002556 [debug] [MainThread]: Command `dbt run` failed at 13:54:14.002393 after 2.56 seconds
[0m13:54:14.003429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e6fab650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e6f88140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7bf3e3f52c30>]}
[0m13:54:14.004403 [debug] [MainThread]: Flushing usage events
[0m13:54:15.427808 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:58:32.586472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc2327860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc3618ce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc2301160>]}


============================== 13:58:32.590672 | 81eac618-4a60-4652-afbd-53b67d0c41f4 ==============================
[0m13:58:32.590672 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:58:32.591331 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run --models source_oracle', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:58:32.696723 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:58:32.697378 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:58:32.697853 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:58:32.836366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '81eac618-4a60-4652-afbd-53b67d0c41f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc1718da0>]}
[0m13:58:32.895964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '81eac618-4a60-4652-afbd-53b67d0c41f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc0fb9fd0>]}
[0m13:58:32.896717 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:58:33.016754 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:58:33.120209 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:58:33.120975 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/staging/source_oracle.sql
[0m13:58:33.353220 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m13:58:33.358671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '81eac618-4a60-4652-afbd-53b67d0c41f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadbb928f50>]}
[0m13:58:33.434492 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:58:33.437582 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:58:33.462336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '81eac618-4a60-4652-afbd-53b67d0c41f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc0cf55e0>]}
[0m13:58:33.462909 [info ] [MainThread]: Found 7 models, 473 macros
[0m13:58:33.463556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81eac618-4a60-4652-afbd-53b67d0c41f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc0a856d0>]}
[0m13:58:33.465530 [info ] [MainThread]: 
[0m13:58:33.466178 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:58:33.466687 [info ] [MainThread]: 
[0m13:58:33.467410 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:58:33.468586 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:58:33.487208 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:58:33.488156 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:58:33.488961 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:33.594222 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:58:33.594946 [debug] [ThreadPool]: SQL status: OK in 0.106 seconds
[0m13:58:33.599073 [debug] [ThreadPool]: On list_schemas: Close
[0m13:58:33.612615 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m13:58:33.618118 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:33.618610 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m13:58:33.619045 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m13:58:33.619450 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:58:33.853027 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:58:33.854228 [debug] [ThreadPool]: SQL status: OK in 0.235 seconds
[0m13:58:33.859715 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m13:58:33.860596 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:58:33.861391 [debug] [ThreadPool]: On list_None_default: Close
[0m13:58:33.870009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81eac618-4a60-4652-afbd-53b67d0c41f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadbb96a5d0>]}
[0m13:58:33.870664 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:33.871158 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:58:33.873790 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.source_oracle
[0m13:58:33.874507 [info ] [Thread-1 (]: 1 of 1 START sql table model default.source_oracle ............................. [RUN]
[0m13:58:33.875187 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.source_oracle)
[0m13:58:33.875746 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.source_oracle
[0m13:58:33.884084 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.source_oracle"
[0m13:58:33.885000 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.source_oracle
[0m13:58:33.920980 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.source_oracle"
[0m13:58:33.921723 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
drop table if exists default.source_oracle
[0m13:58:33.922520 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:58:34.019009 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:58:34.019804 [debug] [Thread-1 (]: SQL status: OK in 0.097 seconds
[0m13:58:34.077495 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.source_oracle"
[0m13:58:34.078428 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:34.078973 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.source_oracle"
[0m13:58:34.079520 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */

  
    
        create table default.source_oracle
      
      
      
      
      
      
      
      

      as
      

-- Tạo bảng tạm thời thay vì view
CREATE TABLE default.source_oracle_test
USING jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj'
)
  
[0m13:58:34.086430 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 19, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\n\n  \n    \n        create table default.source_oracle\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\n-- Tạo bảng tạm thời thay vì view\nCREATE TABLE default.source_oracle_test\n^^^\nUSING jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\'\n)\n  \n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 19, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\n\n  \n    \n        create table default.source_oracle\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\n-- Tạo bảng tạm thời thay vì view\nCREATE TABLE default.source_oracle_test\n^^^\nUSING jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\'\n)\n  \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m13:58:34.087764 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m13:58:34.088695 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */

  
    
        create table default.source_oracle
      
      
      
      
      
      
      
      

      as
      

-- Tạo bảng tạm thời thay vì view
CREATE TABLE default.source_oracle_test
USING jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj'
)
  
[0m13:58:34.089360 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  
    
      
          create table default.source_oracle
        
        
        
        
        
        
        
        
  
        as
        
  
  -- Tạo bảng tạm thời thay vì view
  CREATE TABLE default.source_oracle_test
  ^^^
  USING jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj'
  )
    
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  
    
      
          create table default.source_oracle
        
        
        
        
        
        
        
        
  
        as
        
  
  -- Tạo bảng tạm thời thay vì view
  CREATE TABLE default.source_oracle_test
  ^^^
  USING jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj'
  )
    
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m13:58:34.090117 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: ROLLBACK
[0m13:58:34.090535 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:58:34.090962 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: Close
[0m13:58:34.104135 [debug] [Thread-1 (]: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:58:34.106074 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '81eac618-4a60-4652-afbd-53b67d0c41f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc23261b0>]}
[0m13:58:34.106760 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.source_oracle .................... [[31mERROR[0m in 0.23s]
[0m13:58:34.107803 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.source_oracle
[0m13:58:34.108641 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.source_oracle' to be skipped because of status 'error'.  Reason: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m13:58:34.110465 [debug] [MainThread]: On master: ROLLBACK
[0m13:58:34.110850 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:58:34.155877 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:58:34.156575 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:58:34.157066 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:58:34.157512 [debug] [MainThread]: On master: ROLLBACK
[0m13:58:34.157962 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:58:34.158396 [debug] [MainThread]: On master: Close
[0m13:58:34.163633 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:58:34.164442 [debug] [MainThread]: Connection 'model.dbt_spark_project.source_oracle' was properly closed.
[0m13:58:34.165081 [info ] [MainThread]: 
[0m13:58:34.165730 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.70 seconds (0.70s).
[0m13:58:34.167048 [debug] [MainThread]: Command end result
[0m13:58:34.206978 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:58:34.210306 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:58:34.218468 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m13:58:34.219084 [info ] [MainThread]: 
[0m13:58:34.219752 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:58:34.220450 [info ] [MainThread]: 
[0m13:58:34.221352 [error] [MainThread]:   Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:58:34.222446 [info ] [MainThread]: 
[0m13:58:34.223085 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:58:34.224273 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.6909684, "process_in_blocks": "42904", "process_kernel_time": 0.204838, "process_mem_max_rss": "112036", "process_out_blocks": "2968", "process_user_time": 2.352629}
[0m13:58:34.224884 [debug] [MainThread]: Command `dbt run` failed at 13:58:34.224760 after 1.69 seconds
[0m13:58:34.225681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc2fe3e60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc1b44a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7cadc0b6ddc0>]}
[0m13:58:34.226668 [debug] [MainThread]: Flushing usage events
[0m13:58:35.554529 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:59:16.321854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162f2cfd40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162f9f0c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162fdf8ce0>]}


============================== 13:59:16.324911 | 927f7e24-3fdb-41a2-8e73-a575629a938e ==============================
[0m13:59:16.324911 [info ] [MainThread]: Running with dbt=1.9.3
[0m13:59:16.325642 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models source_oracle', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:59:16.410629 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:59:16.411273 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:59:16.411739 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:59:16.554128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '927f7e24-3fdb-41a2-8e73-a575629a938e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x751630e0c680>]}
[0m13:59:16.612510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '927f7e24-3fdb-41a2-8e73-a575629a938e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162eb525a0>]}
[0m13:59:16.613275 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m13:59:16.708778 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m13:59:16.787068 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:59:16.787781 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/staging/schema.yml
[0m13:59:16.851745 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'source_oracle_test' in the 'models' section of file 'models/staging/schema.yml'
[0m13:59:16.922166 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m13:59:16.927190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '927f7e24-3fdb-41a2-8e73-a575629a938e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162f215940>]}
[0m13:59:17.001393 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:59:17.004306 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:59:17.016211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '927f7e24-3fdb-41a2-8e73-a575629a938e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162e62e960>]}
[0m13:59:17.016835 [info ] [MainThread]: Found 7 models, 473 macros
[0m13:59:17.017340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '927f7e24-3fdb-41a2-8e73-a575629a938e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162e809850>]}
[0m13:59:17.019054 [info ] [MainThread]: 
[0m13:59:17.019597 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:59:17.020112 [info ] [MainThread]: 
[0m13:59:17.020806 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m13:59:17.021933 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m13:59:17.040269 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m13:59:17.041332 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m13:59:17.042306 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:59:17.116610 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:59:17.117248 [debug] [ThreadPool]: SQL status: OK in 0.075 seconds
[0m13:59:17.120549 [debug] [ThreadPool]: On list_schemas: Close
[0m13:59:17.133424 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m13:59:17.139920 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:59:17.140551 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m13:59:17.141132 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m13:59:17.141653 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:59:17.322510 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m13:59:17.323205 [debug] [ThreadPool]: SQL status: OK in 0.182 seconds
[0m13:59:17.327790 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m13:59:17.328425 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m13:59:17.328959 [debug] [ThreadPool]: On list_None_default: Close
[0m13:59:17.337164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '927f7e24-3fdb-41a2-8e73-a575629a938e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162e6fc1a0>]}
[0m13:59:17.338134 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:59:17.338911 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:59:17.342025 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.source_oracle
[0m13:59:17.342869 [info ] [Thread-1 (]: 1 of 1 START sql table model default.source_oracle ............................. [RUN]
[0m13:59:17.343682 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.source_oracle)
[0m13:59:17.344417 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.source_oracle
[0m13:59:17.354903 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.source_oracle"
[0m13:59:17.356053 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.source_oracle
[0m13:59:17.397224 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.source_oracle"
[0m13:59:17.398045 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
drop table if exists default.source_oracle
[0m13:59:17.398648 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:59:17.479851 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m13:59:17.481017 [debug] [Thread-1 (]: SQL status: OK in 0.082 seconds
[0m13:59:17.549180 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.source_oracle"
[0m13:59:17.550225 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m13:59:17.550838 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.source_oracle"
[0m13:59:17.551508 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */

  
    
        create table default.source_oracle
      
      
      
      
      
      
      
      

      as
      

-- Tạo bảng tạm thời thay vì view
CREATE TABLE default.source_oracle_test
USING jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj'
)
  
[0m13:59:17.558204 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 19, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\n\n  \n    \n        create table default.source_oracle\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\n-- Tạo bảng tạm thời thay vì view\nCREATE TABLE default.source_oracle_test\n^^^\nUSING jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\'\n)\n  \n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 19, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */\n\n  \n    \n        create table default.source_oracle\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\n-- Tạo bảng tạm thời thay vì view\nCREATE TABLE default.source_oracle_test\n^^^\nUSING jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\'\n)\n  \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m13:59:17.559349 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m13:59:17.560334 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */

  
    
        create table default.source_oracle
      
      
      
      
      
      
      
      

      as
      

-- Tạo bảng tạm thời thay vì view
CREATE TABLE default.source_oracle_test
USING jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj'
)
  
[0m13:59:17.561368 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  
    
      
          create table default.source_oracle
        
        
        
        
        
        
        
        
  
        as
        
  
  -- Tạo bảng tạm thời thay vì view
  CREATE TABLE default.source_oracle_test
  ^^^
  USING jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj'
  )
    
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
  
    
      
          create table default.source_oracle
        
        
        
        
        
        
        
        
  
        as
        
  
  -- Tạo bảng tạm thời thay vì view
  CREATE TABLE default.source_oracle_test
  ^^^
  USING jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj'
  )
    
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m13:59:17.562511 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: ROLLBACK
[0m13:59:17.563222 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m13:59:17.563864 [debug] [Thread-1 (]: On model.dbt_spark_project.source_oracle: Close
[0m13:59:17.577015 [debug] [Thread-1 (]: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:59:17.579515 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '927f7e24-3fdb-41a2-8e73-a575629a938e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162e8d4950>]}
[0m13:59:17.580554 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.source_oracle .................... [[31mERROR[0m in 0.23s]
[0m13:59:17.581983 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.source_oracle
[0m13:59:17.583868 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.source_oracle' to be skipped because of status 'error'.  Reason: Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m13:59:17.587707 [debug] [MainThread]: On master: ROLLBACK
[0m13:59:17.588535 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:59:17.655734 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:59:17.656883 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:59:17.657856 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:59:17.658539 [debug] [MainThread]: On master: ROLLBACK
[0m13:59:17.659151 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m13:59:17.659650 [debug] [MainThread]: On master: Close
[0m13:59:17.666558 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:59:17.667201 [debug] [MainThread]: Connection 'model.dbt_spark_project.source_oracle' was properly closed.
[0m13:59:17.667767 [info ] [MainThread]: 
[0m13:59:17.668424 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.65 seconds (0.65s).
[0m13:59:17.669842 [debug] [MainThread]: Command end result
[0m13:59:17.719557 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m13:59:17.721381 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m13:59:17.728538 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m13:59:17.729031 [info ] [MainThread]: 
[0m13:59:17.729561 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m13:59:17.730102 [info ] [MainThread]: 
[0m13:59:17.731016 [error] [MainThread]:   Runtime Error in model source_oracle (models/staging/source_oracle.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 19, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.source_oracle"} */
    
      
        
            create table default.source_oracle
          
          
          
          
          
          
          
          
    
          as
          
    
    -- Tạo bảng tạm thời thay vì view
    CREATE TABLE default.source_oracle_test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m13:59:17.732011 [info ] [MainThread]: 
[0m13:59:17.732850 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:59:17.734359 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.465214, "process_in_blocks": "0", "process_kernel_time": 0.188806, "process_mem_max_rss": "110408", "process_out_blocks": "2968", "process_user_time": 2.165775}
[0m13:59:17.735221 [debug] [MainThread]: Command `dbt run` failed at 13:59:17.735054 after 1.47 seconds
[0m13:59:17.735979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x751632f04140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162e8a00e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75162e5cf770>]}
[0m13:59:17.736652 [debug] [MainThread]: Flushing usage events
[0m13:59:19.078114 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:42:15.659342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7183484ca210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718347331fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718346bfa600>]}


============================== 14:42:15.662443 | 267de13d-edce-486b-864a-90389bded3c7 ==============================
[0m14:42:15.662443 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:42:15.663530 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --models test_thrift', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:42:15.757396 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:42:15.757920 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:42:15.758373 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:42:15.906346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '267de13d-edce-486b-864a-90389bded3c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718346061970>]}
[0m14:42:15.965128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '267de13d-edce-486b-864a-90389bded3c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718346a71c10>]}
[0m14:42:15.965905 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:42:16.063652 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:42:16.143469 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 2 files added, 0 files changed.
[0m14:42:16.144152 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/staging/test_thrift.sql
[0m14:42:16.144668 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/schema.yml
[0m14:42:16.145468 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/staging/source_oracle.sql
[0m14:42:16.145852 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/staging/load_to_oracle.sql
[0m14:42:16.430597 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'test' in the 'models' section of file 'models/schema.yml'
[0m14:42:16.484368 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:42:16.489653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '267de13d-edce-486b-864a-90389bded3c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718345b7cc20>]}
[0m14:42:16.560492 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:42:16.563033 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:42:16.574260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '267de13d-edce-486b-864a-90389bded3c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718345b7d5b0>]}
[0m14:42:16.574826 [info ] [MainThread]: Found 6 models, 473 macros
[0m14:42:16.575402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '267de13d-edce-486b-864a-90389bded3c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718344c32cc0>]}
[0m14:42:16.576954 [info ] [MainThread]: 
[0m14:42:16.577556 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:42:16.578029 [info ] [MainThread]: 
[0m14:42:16.578694 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:42:16.579830 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:42:16.597847 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:42:16.598897 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:42:16.599845 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:42:16.802231 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:42:16.802781 [debug] [ThreadPool]: SQL status: OK in 0.203 seconds
[0m14:42:16.807112 [debug] [ThreadPool]: On list_schemas: Close
[0m14:42:16.837113 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:42:16.842537 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:42:16.843021 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:42:16.843448 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:42:16.843859 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:42:17.047697 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:42:17.048845 [debug] [ThreadPool]: SQL status: OK in 0.205 seconds
[0m14:42:17.054393 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:42:17.055292 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:42:17.055989 [debug] [ThreadPool]: On list_None_default: Close
[0m14:42:17.064315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '267de13d-edce-486b-864a-90389bded3c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718344c519d0>]}
[0m14:42:17.064962 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:42:17.065448 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:42:17.067640 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test_thrift
[0m14:42:17.068286 [info ] [Thread-1 (]: 1 of 1 START sql table model default.test_thrift ............................... [RUN]
[0m14:42:17.068860 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test_thrift)
[0m14:42:17.069349 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test_thrift
[0m14:42:17.077956 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test_thrift"
[0m14:42:17.079627 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test_thrift
[0m14:42:17.119681 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test_thrift"
[0m14:42:17.120442 [debug] [Thread-1 (]: On model.dbt_spark_project.test_thrift: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
drop table if exists default.test_thrift
[0m14:42:17.121087 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:42:17.243768 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:42:17.244574 [debug] [Thread-1 (]: SQL status: OK in 0.123 seconds
[0m14:42:17.303067 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test_thrift"
[0m14:42:17.304758 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:42:17.305283 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test_thrift"
[0m14:42:17.305785 [debug] [Thread-1 (]: On model.dbt_spark_project.test_thrift: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */

  
    
        create table default.test_thrift
      
      
      
      
      
      
      
      

      as
      

CREATE TABLE default.test
USING jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj',
  driver 'oracle.jdbc.OracleDriver'
)
  
[0m14:42:17.318200 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 18, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */\n\n  \n    \n        create table default.test_thrift\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\nCREATE TABLE default.test\n^^^\nUSING jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\',\n  driver \'oracle.jdbc.OracleDriver\'\n)\n  \n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'CREATE\'.(line 18, pos 0)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */\n\n  \n    \n        create table default.test_thrift\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\nCREATE TABLE default.test\n^^^\nUSING jdbc\nOPTIONS (\n  url \'jdbc:oracle:thin:@10.14.223.202:1521/R18\',\n  dbtable \'metadata.test\',\n  user \'tafj\',\n  password \'t24tafj\',\n  driver \'oracle.jdbc.OracleDriver\'\n)\n  \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m14:42:17.318961 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m14:42:17.319556 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */

  
    
        create table default.test_thrift
      
      
      
      
      
      
      
      

      as
      

CREATE TABLE default.test
USING jdbc
OPTIONS (
  url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
  dbtable 'metadata.test',
  user 'tafj',
  password 't24tafj',
  driver 'oracle.jdbc.OracleDriver'
)
  
[0m14:42:17.320265 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
  
    
      
          create table default.test_thrift
        
        
        
        
        
        
        
        
  
        as
        
  
  CREATE TABLE default.test
  ^^^
  USING jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj',
    driver 'oracle.jdbc.OracleDriver'
  )
    
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
  
    
      
          create table default.test_thrift
        
        
        
        
        
        
        
        
  
        as
        
  
  CREATE TABLE default.test
  ^^^
  USING jdbc
  OPTIONS (
    url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
    dbtable 'metadata.test',
    user 'tafj',
    password 't24tafj',
    driver 'oracle.jdbc.OracleDriver'
  )
    
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m14:42:17.321144 [debug] [Thread-1 (]: On model.dbt_spark_project.test_thrift: ROLLBACK
[0m14:42:17.321617 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:42:17.322094 [debug] [Thread-1 (]: On model.dbt_spark_project.test_thrift: Close
[0m14:42:17.334255 [debug] [Thread-1 (]: Runtime Error in model test_thrift (models/staging/test_thrift.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
    
      
        
            create table default.test_thrift
          
          
          
          
          
          
          
          
    
          as
          
    
    CREATE TABLE default.test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.OracleDriver'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
    
      
        
            create table default.test_thrift
          
          
          
          
          
          
          
          
    
          as
          
    
    CREATE TABLE default.test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.OracleDriver'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m14:42:17.336640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '267de13d-edce-486b-864a-90389bded3c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718344b4aa20>]}
[0m14:42:17.337677 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.test_thrift ...................... [[31mERROR[0m in 0.27s]
[0m14:42:17.338912 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test_thrift
[0m14:42:17.340073 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.test_thrift' to be skipped because of status 'error'.  Reason: Runtime Error in model test_thrift (models/staging/test_thrift.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
    
      
        
            create table default.test_thrift
          
          
          
          
          
          
          
          
    
          as
          
    
    CREATE TABLE default.test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.OracleDriver'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
    
      
        
            create table default.test_thrift
          
          
          
          
          
          
          
          
    
          as
          
    
    CREATE TABLE default.test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.OracleDriver'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m14:42:17.342328 [debug] [MainThread]: On master: ROLLBACK
[0m14:42:17.342906 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:42:17.435025 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:42:17.435631 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:42:17.436190 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:42:17.436746 [debug] [MainThread]: On master: ROLLBACK
[0m14:42:17.437311 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:42:17.437794 [debug] [MainThread]: On master: Close
[0m14:42:17.447163 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:42:17.447746 [debug] [MainThread]: Connection 'model.dbt_spark_project.test_thrift' was properly closed.
[0m14:42:17.448295 [info ] [MainThread]: 
[0m14:42:17.448817 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.87 seconds (0.87s).
[0m14:42:17.449856 [debug] [MainThread]: Command end result
[0m14:42:17.489000 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:42:17.490953 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:42:17.498297 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:42:17.498863 [info ] [MainThread]: 
[0m14:42:17.499601 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:42:17.500246 [info ] [MainThread]: 
[0m14:42:17.501336 [error] [MainThread]:   Runtime Error in model test_thrift (models/staging/test_thrift.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
    
      
        
            create table default.test_thrift
          
          
          
          
          
          
          
          
    
          as
          
    
    CREATE TABLE default.test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.OracleDriver'
    )
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'CREATE'.(line 18, pos 0)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test_thrift"} */
    
      
        
            create table default.test_thrift
          
          
          
          
          
          
          
          
    
          as
          
    
    CREATE TABLE default.test
    ^^^
    USING jdbc
    OPTIONS (
      url 'jdbc:oracle:thin:@10.14.223.202:1521/R18',
      dbtable 'metadata.test',
      user 'tafj',
      password 't24tafj',
      driver 'oracle.jdbc.OracleDriver'
    )
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m14:42:17.502584 [info ] [MainThread]: 
[0m14:42:17.503365 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:42:17.504730 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.8994542, "process_in_blocks": "2200", "process_kernel_time": 0.195847, "process_mem_max_rss": "113372", "process_out_blocks": "2960", "process_user_time": 2.452089}
[0m14:42:17.505770 [debug] [MainThread]: Command `dbt run` failed at 14:42:17.505541 after 1.90 seconds
[0m14:42:17.506481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x71834a188140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718347184ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x718347184530>]}
[0m14:42:17.507182 [debug] [MainThread]: Flushing usage events
[0m14:42:19.075620 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:45:13.681254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb4655df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb6094fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb470d7f0>]}


============================== 10:45:13.685504 | 80922801-fe3b-4acb-875b-40ce82c5245b ==============================
[0m10:45:13.685504 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:45:13.686160 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run --models thrift_test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:45:13.788061 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:45:13.788610 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:45:13.789087 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:45:13.932418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '80922801-fe3b-4acb-875b-40ce82c5245b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb4029100>]}
[0m10:45:13.990633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '80922801-fe3b-4acb-875b-40ce82c5245b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb3ab2810>]}
[0m10:45:13.991421 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:45:14.100616 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m10:45:14.213225 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m10:45:14.213881 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/thrift_test.sql
[0m10:45:14.214250 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/staging/test_thrift.sql
[0m10:45:14.443629 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m10:45:14.449870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '80922801-fe3b-4acb-875b-40ce82c5245b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb3840890>]}
[0m10:45:14.537455 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:45:14.540869 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:45:14.573875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '80922801-fe3b-4acb-875b-40ce82c5245b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb346bda0>]}
[0m10:45:14.574457 [info ] [MainThread]: Found 6 models, 473 macros
[0m10:45:14.575054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80922801-fe3b-4acb-875b-40ce82c5245b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb38400e0>]}
[0m10:45:14.577104 [info ] [MainThread]: 
[0m10:45:14.577964 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:45:14.578658 [info ] [MainThread]: 
[0m10:45:14.579439 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:45:14.580608 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m10:45:14.591341 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:45:14.591971 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:45:14.592463 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:45:14.777832 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:45:14.778464 [debug] [ThreadPool]: SQL status: OK in 0.186 seconds
[0m10:45:14.783019 [debug] [ThreadPool]: On list_schemas: Close
[0m10:45:14.808383 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__default_default)
[0m10:45:14.809104 [debug] [ThreadPool]: Creating schema "schema: "default_default"
"
[0m10:45:14.813565 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:14.814047 [debug] [ThreadPool]: Using spark connection "create__default_default"
[0m10:45:14.814466 [debug] [ThreadPool]: On create__default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "create__default_default"} */
create schema if not exists default_default
  
[0m10:45:14.814877 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:45:15.024729 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:45:15.025311 [debug] [ThreadPool]: SQL status: OK in 0.210 seconds
[0m10:45:15.026525 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m10:45:15.027009 [debug] [ThreadPool]: On create__default_default: ROLLBACK
[0m10:45:15.027494 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:45:15.027937 [debug] [ThreadPool]: On create__default_default: Close
[0m10:45:15.041802 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default, now list_None_default_default)
[0m10:45:15.046535 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:15.046957 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m10:45:15.047318 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m10:45:15.047678 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:45:15.147394 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:45:15.147998 [debug] [ThreadPool]: SQL status: OK in 0.100 seconds
[0m10:45:15.151467 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m10:45:15.151991 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:45:15.152399 [debug] [ThreadPool]: On list_None_default_default: Close
[0m10:45:15.160406 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m10:45:15.162714 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:15.163170 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:45:15.163596 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:45:15.164023 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:45:15.348033 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:45:15.348568 [debug] [ThreadPool]: SQL status: OK in 0.185 seconds
[0m10:45:15.352993 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:45:15.353519 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:45:15.353922 [debug] [ThreadPool]: On list_None_default: Close
[0m10:45:15.361315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80922801-fe3b-4acb-875b-40ce82c5245b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb38401a0>]}
[0m10:45:15.362287 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:15.362981 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:45:15.365437 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m10:45:15.366050 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m10:45:15.366608 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.thrift_test)
[0m10:45:15.367037 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m10:45:15.374786 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m10:45:15.375557 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m10:45:15.479560 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m10:45:15.480602 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:15.481209 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m10:45:15.481693 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    
        create table default_default.test
      
      
      
      
      
      
      
      

      as
      -- models/thrift_test.sql


SELECT 
    ID,
    NAME,
    DOUBLE_VALUE
FROM 
    raw_data
  
[0m10:45:15.482183 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:45:15.700436 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;\n'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n+- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]\n   +- SubqueryAlias spark_catalog.default.raw_data\n      +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;\n'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n+- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]\n   +- SubqueryAlias spark_catalog.default.raw_data\n      +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m10:45:15.701352 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m10:45:15.702018 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    
        create table default_default.test
      
      
      
      
      
      
      
      

      as
      -- models/thrift_test.sql


SELECT 
    ID,
    NAME,
    DOUBLE_VALUE
FROM 
    raw_data
  
[0m10:45:15.702878 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
  'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
  +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
     +- SubqueryAlias spark_catalog.default.raw_data
        +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
  'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
  +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
     +- SubqueryAlias spark_catalog.default.raw_data
        +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m10:45:15.703797 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m10:45:15.704274 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:45:15.704728 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m10:45:15.719892 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
    'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
    +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
       +- SubqueryAlias spark_catalog.default.raw_data
          +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
    'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
    +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
       +- SubqueryAlias spark_catalog.default.raw_data
          +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:45:15.722526 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80922801-fe3b-4acb-875b-40ce82c5245b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb257c7a0>]}
[0m10:45:15.723338 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.35s]
[0m10:45:15.724479 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m10:45:15.725579 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
    'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
    +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
       +- SubqueryAlias spark_catalog.default.raw_data
          +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
    'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
    +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
       +- SubqueryAlias spark_catalog.default.raw_data
          +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m10:45:15.727685 [debug] [MainThread]: On master: ROLLBACK
[0m10:45:15.728218 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:45:15.803162 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:45:15.803841 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:15.804394 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:45:15.804954 [debug] [MainThread]: On master: ROLLBACK
[0m10:45:15.805493 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:45:15.806045 [debug] [MainThread]: On master: Close
[0m10:45:15.814120 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:45:15.814715 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m10:45:15.815279 [info ] [MainThread]: 
[0m10:45:15.815872 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.24 seconds (1.24s).
[0m10:45:15.817117 [debug] [MainThread]: Command end result
[0m10:45:15.852199 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:45:15.855344 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:45:15.863183 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m10:45:15.863822 [info ] [MainThread]: 
[0m10:45:15.864392 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m10:45:15.864895 [info ] [MainThread]: 
[0m10:45:15.865710 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
    'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
    +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
       +- SubqueryAlias spark_catalog.default.raw_data
          +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `DOUBLE_VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `value`].; line 22 pos 4;
    'CreateTable `spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists
    +- 'Project [ID#65, NAME#66, 'DOUBLE_VALUE]
       +- SubqueryAlias spark_catalog.default.raw_data
          +- Relation spark_catalog.default.raw_data[id#65,name#66,value#67] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m10:45:15.866510 [info ] [MainThread]: 
[0m10:45:15.867029 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m10:45:15.868602 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.2412865, "process_in_blocks": "74664", "process_kernel_time": 0.278919, "process_mem_max_rss": "112016", "process_out_blocks": "2984", "process_user_time": 2.421298}
[0m10:45:15.869240 [debug] [MainThread]: Command `dbt run` failed at 10:45:15.869109 after 2.24 seconds
[0m10:45:15.869809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb4427800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb3bc8890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x709cb4d4f650>]}
[0m10:45:15.870495 [debug] [MainThread]: Flushing usage events
[0m10:45:17.497793 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:45:52.275256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075ec9f080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075ec9de80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075ec9fe00>]}


============================== 10:45:52.278443 | 470f6f3b-221b-4898-81b2-e5b5f0a7b1c5 ==============================
[0m10:45:52.278443 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:45:52.279106 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --models thrift_test', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:45:52.368926 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:45:52.369475 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:45:52.369927 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:45:52.515418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '470f6f3b-221b-4898-81b2-e5b5f0a7b1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075e4bf080>]}
[0m10:45:52.574776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '470f6f3b-221b-4898-81b2-e5b5f0a7b1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075f6a8350>]}
[0m10:45:52.575552 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:45:52.677398 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m10:45:52.756587 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:45:52.757345 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m10:45:52.984821 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m10:45:52.990424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '470f6f3b-221b-4898-81b2-e5b5f0a7b1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075e033860>]}
[0m10:45:53.066608 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:45:53.069521 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:45:53.081433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '470f6f3b-221b-4898-81b2-e5b5f0a7b1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075df098b0>]}
[0m10:45:53.082129 [info ] [MainThread]: Found 6 models, 473 macros
[0m10:45:53.082648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '470f6f3b-221b-4898-81b2-e5b5f0a7b1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075e2c0110>]}
[0m10:45:53.084488 [info ] [MainThread]: 
[0m10:45:53.085102 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:45:53.085613 [info ] [MainThread]: 
[0m10:45:53.086307 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:45:53.087424 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m10:45:53.107852 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:45:53.108857 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:45:53.109873 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:45:53.199786 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:45:53.200369 [debug] [ThreadPool]: SQL status: OK in 0.091 seconds
[0m10:45:53.203994 [debug] [ThreadPool]: On list_schemas: Close
[0m10:45:53.217573 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m10:45:53.223086 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:53.223583 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:45:53.224033 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:45:53.224453 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:45:53.351105 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:45:53.351724 [debug] [ThreadPool]: SQL status: OK in 0.127 seconds
[0m10:45:53.355548 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:45:53.356038 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:45:53.356424 [debug] [ThreadPool]: On list_None_default: Close
[0m10:45:53.362731 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default, now list_None_default_default)
[0m10:45:53.365277 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:53.365713 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m10:45:53.366098 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m10:45:53.366478 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:45:53.435962 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:45:53.436482 [debug] [ThreadPool]: SQL status: OK in 0.070 seconds
[0m10:45:53.439122 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m10:45:53.439624 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:45:53.440040 [debug] [ThreadPool]: On list_None_default_default: Close
[0m10:45:53.445943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '470f6f3b-221b-4898-81b2-e5b5f0a7b1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075dfae630>]}
[0m10:45:53.446606 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:53.447060 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:45:53.449137 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m10:45:53.449811 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m10:45:53.450405 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default, now model.dbt_spark_project.thrift_test)
[0m10:45:53.450911 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m10:45:53.459028 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m10:45:53.459888 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m10:45:53.554074 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m10:45:53.555012 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:53.555686 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m10:45:53.556213 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    
        create table default_default.test
      
      
      
      
      
      
      
      

      as
      -- models/thrift_test.sql


SELECT 
    ID,
    NAME,
    VALUE
FROM 
    raw_data
  
[0m10:45:53.556769 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:45:55.105967 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:45:55.106976 [debug] [Thread-1 (]: SQL status: OK in 1.550 seconds
[0m10:45:55.133408 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m10:45:55.134115 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:45:55.134723 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m10:45:55.144044 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '470f6f3b-221b-4898-81b2-e5b5f0a7b1c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075f855d30>]}
[0m10:45:55.144892 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model default_default.test ................... [[32mOK[0m in 1.69s]
[0m10:45:55.145640 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m10:45:55.146918 [debug] [MainThread]: On master: ROLLBACK
[0m10:45:55.147371 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:45:55.205949 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:45:55.206505 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:45:55.206989 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:45:55.207418 [debug] [MainThread]: On master: ROLLBACK
[0m10:45:55.207860 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:45:55.208274 [debug] [MainThread]: On master: Close
[0m10:45:55.214840 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:45:55.215384 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m10:45:55.215912 [info ] [MainThread]: 
[0m10:45:55.216421 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 2.13 seconds (2.13s).
[0m10:45:55.217453 [debug] [MainThread]: Command end result
[0m10:45:55.241778 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:45:55.244626 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:45:55.256258 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m10:45:55.257297 [info ] [MainThread]: 
[0m10:45:55.258285 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:45:55.259154 [info ] [MainThread]: 
[0m10:45:55.260150 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m10:45:55.261982 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.0414932, "process_in_blocks": "0", "process_kernel_time": 0.199051, "process_mem_max_rss": "112512", "process_out_blocks": "2888", "process_user_time": 2.333602}
[0m10:45:55.263075 [debug] [MainThread]: Command `dbt run` succeeded at 10:45:55.262855 after 3.04 seconds
[0m10:45:55.264171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075f36fc80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075f684530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70075f684d70>]}
[0m10:45:55.265289 [debug] [MainThread]: Flushing usage events
[0m10:45:56.512481 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:49:02.201905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88c95d100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88d0e01d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88d7f9220>]}


============================== 10:49:02.204949 | ccb7eab8-ea58-47e3-be92-0dd88cd374a1 ==============================
[0m10:49:02.204949 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:49:02.205917 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt run --models thrift_test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:49:02.287861 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:49:02.288415 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:49:02.288879 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:49:02.426916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ccb7eab8-ea58-47e3-be92-0dd88cd374a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88c95d100>]}
[0m10:49:02.487842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ccb7eab8-ea58-47e3-be92-0dd88cd374a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88c212e10>]}
[0m10:49:02.488591 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:49:02.586089 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m10:49:02.663656 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:49:02.664173 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:49:02.670584 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m10:49:02.705453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ccb7eab8-ea58-47e3-be92-0dd88cd374a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88beda210>]}
[0m10:49:02.761195 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:49:02.763799 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:49:02.774622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ccb7eab8-ea58-47e3-be92-0dd88cd374a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88ba4d040>]}
[0m10:49:02.775241 [info ] [MainThread]: Found 6 models, 473 macros
[0m10:49:02.775667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ccb7eab8-ea58-47e3-be92-0dd88cd374a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88ba46f90>]}
[0m10:49:02.777657 [info ] [MainThread]: 
[0m10:49:02.778301 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:49:02.778767 [info ] [MainThread]: 
[0m10:49:02.779478 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:49:02.780442 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m10:49:02.791369 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:49:02.791962 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:49:02.792392 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:49:02.889288 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:49:02.890098 [debug] [ThreadPool]: SQL status: OK in 0.098 seconds
[0m10:49:02.894348 [debug] [ThreadPool]: On list_schemas: Close
[0m10:49:02.906257 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m10:49:02.911451 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:49:02.911942 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:49:02.912360 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:49:02.912748 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:49:03.026264 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:49:03.027107 [debug] [ThreadPool]: SQL status: OK in 0.114 seconds
[0m10:49:03.031640 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:49:03.032392 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:49:03.033043 [debug] [ThreadPool]: On list_None_default: Close
[0m10:49:03.038131 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default, now list_None_default_default)
[0m10:49:03.040907 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:49:03.041350 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m10:49:03.041787 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m10:49:03.042187 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:49:03.117510 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:49:03.118374 [debug] [ThreadPool]: SQL status: OK in 0.076 seconds
[0m10:49:03.122755 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m10:49:03.123529 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:49:03.124210 [debug] [ThreadPool]: On list_None_default_default: Close
[0m10:49:03.130509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ccb7eab8-ea58-47e3-be92-0dd88cd374a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88c297020>]}
[0m10:49:03.131412 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:49:03.132027 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:49:03.134163 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m10:49:03.134813 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m10:49:03.135380 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default, now model.dbt_spark_project.thrift_test)
[0m10:49:03.135859 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m10:49:03.145621 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m10:49:03.146924 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m10:49:03.208194 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:49:03.208764 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m10:49:03.209198 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    ID,
    NAME,
    VALUE
FROM 
    raw_data
  
[0m10:49:03.209620 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:49:03.311991 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:49:03.312558 [debug] [Thread-1 (]: SQL status: OK in 0.103 seconds
[0m10:49:03.329994 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m10:49:03.330582 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m10:49:03.415476 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:49:03.416153 [debug] [Thread-1 (]: SQL status: OK in 0.085 seconds
[0m10:49:03.420570 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m10:49:03.421479 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m10:49:03.422061 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m10:49:03.838868 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:49:03.839463 [debug] [Thread-1 (]: SQL status: OK in 0.417 seconds
[0m10:49:03.858059 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m10:49:03.858879 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:49:03.859553 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m10:49:03.902583 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ccb7eab8-ea58-47e3-be92-0dd88cd374a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b889907e00>]}
[0m10:49:03.903411 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model default_default.test ................... [[32mOK[0m in 0.77s]
[0m10:49:03.904133 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m10:49:03.905540 [debug] [MainThread]: On master: ROLLBACK
[0m10:49:03.906048 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:49:03.950438 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:49:03.951026 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:49:03.951477 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:49:03.951931 [debug] [MainThread]: On master: ROLLBACK
[0m10:49:03.952366 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:49:03.952801 [debug] [MainThread]: On master: Close
[0m10:49:03.958129 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:49:03.958677 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m10:49:03.959178 [info ] [MainThread]: 
[0m10:49:03.959675 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.18 seconds (1.18s).
[0m10:49:03.960538 [debug] [MainThread]: Command end result
[0m10:49:03.998705 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:49:04.000592 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:49:04.006706 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m10:49:04.007241 [info ] [MainThread]: 
[0m10:49:04.007879 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:49:04.008348 [info ] [MainThread]: 
[0m10:49:04.008893 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m10:49:04.009977 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.862403, "process_in_blocks": "3704", "process_kernel_time": 0.188714, "process_mem_max_rss": "108952", "process_out_blocks": "1960", "process_user_time": 1.954046}
[0m10:49:04.010506 [debug] [MainThread]: Command `dbt run` succeeded at 10:49:04.010398 after 1.86 seconds
[0m10:49:04.010953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88c896240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88d50d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b88bb99e20>]}
[0m10:49:04.011405 [debug] [MainThread]: Flushing usage events
[0m10:49:05.346454 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:01:49.932161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b7079cf740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b7071be3f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b7079cd2b0>]}


============================== 11:01:49.935440 | 4e3fc847-0064-4925-8d06-59d16526b531 ==============================
[0m11:01:49.935440 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:01:49.936251 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --models thrift_test', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:01:50.019746 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:01:50.020286 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:01:50.020758 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:01:50.157948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e3fc847-0064-4925-8d06-59d16526b531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b706cd6a80>]}
[0m11:01:50.216860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e3fc847-0064-4925-8d06-59d16526b531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b707707c50>]}
[0m11:01:50.217614 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:01:50.318235 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:01:50.402472 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:01:50.403179 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:01:50.645239 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:01:50.650268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e3fc847-0064-4925-8d06-59d16526b531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b7052a6900>]}
[0m11:01:50.726691 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:01:50.729607 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:01:50.741910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e3fc847-0064-4925-8d06-59d16526b531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b70527d700>]}
[0m11:01:50.742595 [info ] [MainThread]: Found 6 models, 473 macros
[0m11:01:50.743150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e3fc847-0064-4925-8d06-59d16526b531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b7065cef60>]}
[0m11:01:50.744835 [info ] [MainThread]: 
[0m11:01:50.745348 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:01:50.745800 [info ] [MainThread]: 
[0m11:01:50.746417 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:01:50.747566 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:01:50.769055 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:01:50.770116 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:01:50.771020 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:01:50.857906 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:01:50.858564 [debug] [ThreadPool]: SQL status: OK in 0.088 seconds
[0m11:01:50.863393 [debug] [ThreadPool]: On list_schemas: Close
[0m11:01:50.880978 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default)
[0m11:01:50.890577 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:01:50.891455 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:01:50.892111 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:01:50.892765 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:01:51.004844 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:01:51.005656 [debug] [ThreadPool]: SQL status: OK in 0.113 seconds
[0m11:01:51.009520 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:01:51.010237 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:01:51.010820 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:01:51.017189 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m11:01:51.020903 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:01:51.021833 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:01:51.022372 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:01:51.022901 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:01:51.147901 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:01:51.148480 [debug] [ThreadPool]: SQL status: OK in 0.126 seconds
[0m11:01:51.152806 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:01:51.153414 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:01:51.153920 [debug] [ThreadPool]: On list_None_default: Close
[0m11:01:51.160086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e3fc847-0064-4925-8d06-59d16526b531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b7052a0a10>]}
[0m11:01:51.160794 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:01:51.161284 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:01:51.163945 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:01:51.164887 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:01:51.165887 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.thrift_test)
[0m11:01:51.166534 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:01:51.176097 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:01:51.176879 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:01:51.241307 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:01:51.241842 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:01:51.242283 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    ID,
    NAME,
    doubled_value
FROM 
    test_staging
  
[0m11:01:51.242746 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:01:51.322600 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:01:51.323318 [debug] [Thread-1 (]: SQL status: OK in 0.081 seconds
[0m11:01:51.343213 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:01:51.343811 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:01:51.388146 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:01:51.388869 [debug] [Thread-1 (]: SQL status: OK in 0.045 seconds
[0m11:01:51.392473 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:01:51.393278 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:01:51.393802 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:01:51.454952 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false\n+- 'Project [ID#535, NAME#536, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])\n         +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]\n            +- Project [ID#517, NAME#518, doubled_value#519]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false\n+- 'Project [ID#535, NAME#536, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])\n         +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]\n            +- Project [ID#517, NAME#518, doubled_value#519]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:01:51.456023 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:01:51.456725 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:01:51.457684 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
  +- 'Project [ID#535, NAME#536, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
           +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
              +- Project [ID#517, NAME#518, doubled_value#519]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
  +- 'Project [ID#535, NAME#536, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
           +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
              +- Project [ID#517, NAME#518, doubled_value#519]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:01:51.458812 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:01:51.459384 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:01:51.459944 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:01:51.504003 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
    +- 'Project [ID#535, NAME#536, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
             +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
                +- Project [ID#517, NAME#518, doubled_value#519]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
    +- 'Project [ID#535, NAME#536, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
             +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
                +- Project [ID#517, NAME#518, doubled_value#519]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:01:51.506771 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4e3fc847-0064-4925-8d06-59d16526b531', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b70496bfb0>]}
[0m11:01:51.507763 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.34s]
[0m11:01:51.509189 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:01:51.510545 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
    +- 'Project [ID#535, NAME#536, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
             +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
                +- Project [ID#517, NAME#518, doubled_value#519]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
    +- 'Project [ID#535, NAME#536, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
             +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
                +- Project [ID#517, NAME#518, doubled_value#519]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:01:51.513146 [debug] [MainThread]: On master: ROLLBACK
[0m11:01:51.513743 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:01:51.558286 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:01:51.559109 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:01:51.559795 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:01:51.560425 [debug] [MainThread]: On master: ROLLBACK
[0m11:01:51.561056 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:01:51.561559 [debug] [MainThread]: On master: Close
[0m11:01:51.566777 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:01:51.567373 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:01:51.567989 [info ] [MainThread]: 
[0m11:01:51.568721 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.82 seconds (0.82s).
[0m11:01:51.570138 [debug] [MainThread]: Command end result
[0m11:01:51.594324 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:01:51.597486 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:01:51.610022 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:01:51.611005 [info ] [MainThread]: 
[0m11:01:51.612115 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:01:51.612886 [info ] [MainThread]: 
[0m11:01:51.614160 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
    +- 'Project [ID#535, NAME#536, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
             +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
                +- Project [ID#517, NAME#518, doubled_value#519]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#538, NAME#539, VALUE#540], Partition Cols: []], false, false, false
    +- 'Project [ID#535, NAME#536, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#535,NAME#536,doubled_value#537])
             +- Project [cast(ID#517 as int) AS ID#535, cast(NAME#518 as string) AS NAME#536, cast(doubled_value#519 as int) AS doubled_value#537]
                +- Project [ID#517, NAME#518, doubled_value#519]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#517,name#518,doubled_value#519] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:01:51.615455 [info ] [MainThread]: 
[0m11:01:51.616140 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:01:51.617432 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.7373874, "process_in_blocks": "8", "process_kernel_time": 0.174479, "process_mem_max_rss": "113012", "process_out_blocks": "3000", "process_user_time": 2.259255}
[0m11:01:51.619329 [debug] [MainThread]: Command `dbt run` failed at 11:01:51.618691 after 1.74 seconds
[0m11:01:51.620785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b706e55190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b706e55130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79b70728b890>]}
[0m11:01:51.622285 [debug] [MainThread]: Flushing usage events
[0m11:01:53.201712 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:02:15.467907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a0a8fe30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a067c650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a1541e80>]}


============================== 11:02:15.471031 | 07ce7a21-79a2-45a1-8669-a49f751d4602 ==============================
[0m11:02:15.471031 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:02:15.471724 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --models thrift_test', 'send_anonymous_usage_stats': 'True'}
[0m11:02:15.554427 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:02:15.554992 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:02:15.555427 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:02:15.698963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07ce7a21-79a2-45a1-8669-a49f751d4602', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a0af9a90>]}
[0m11:02:15.757957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07ce7a21-79a2-45a1-8669-a49f751d4602', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a0d51df0>]}
[0m11:02:15.758739 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:02:15.860590 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:02:15.938982 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:02:15.939444 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:02:15.945922 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:02:15.981928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07ce7a21-79a2-45a1-8669-a49f751d4602', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a0178e00>]}
[0m11:02:16.038173 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:02:16.040795 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:02:16.052139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07ce7a21-79a2-45a1-8669-a49f751d4602', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b09fb5d040>]}
[0m11:02:16.052710 [info ] [MainThread]: Found 6 models, 473 macros
[0m11:02:16.053161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07ce7a21-79a2-45a1-8669-a49f751d4602', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b09fe528a0>]}
[0m11:02:16.054771 [info ] [MainThread]: 
[0m11:02:16.055264 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:02:16.055714 [info ] [MainThread]: 
[0m11:02:16.056400 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:02:16.057373 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:02:16.079366 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:02:16.080478 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:02:16.081510 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:02:16.140923 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:02:16.141452 [debug] [ThreadPool]: SQL status: OK in 0.060 seconds
[0m11:02:16.144286 [debug] [ThreadPool]: On list_schemas: Close
[0m11:02:16.158020 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:02:16.164782 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:16.165398 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:02:16.165923 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:02:16.166449 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:02:16.263029 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:02:16.263556 [debug] [ThreadPool]: SQL status: OK in 0.097 seconds
[0m11:02:16.266854 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:02:16.267344 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:02:16.267740 [debug] [ThreadPool]: On list_None_default: Close
[0m11:02:16.272214 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default, now list_None_default_default)
[0m11:02:16.276000 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:16.276610 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:02:16.277173 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:02:16.277734 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:02:16.351240 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:02:16.351897 [debug] [ThreadPool]: SQL status: OK in 0.074 seconds
[0m11:02:16.355859 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:02:16.356468 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:02:16.356972 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:02:16.363284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07ce7a21-79a2-45a1-8669-a49f751d4602', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a01ab260>]}
[0m11:02:16.364321 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:16.365058 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:02:16.367995 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:02:16.369109 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:02:16.370062 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default, now model.dbt_spark_project.thrift_test)
[0m11:02:16.370890 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:02:16.387256 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:02:16.388699 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:02:16.455205 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:16.455923 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:02:16.456509 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    ID,
    NAME,
    doubled_value
FROM 
    test_staging
  
[0m11:02:16.457044 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:02:16.535177 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:02:16.535832 [debug] [Thread-1 (]: SQL status: OK in 0.079 seconds
[0m11:02:16.551506 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:02:16.552037 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:02:16.601705 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:02:16.602437 [debug] [Thread-1 (]: SQL status: OK in 0.050 seconds
[0m11:02:16.606353 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:02:16.607252 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:02:16.607863 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:02:16.657779 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false\n+- 'Project [ID#601, NAME#602, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])\n         +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]\n            +- Project [ID#583, NAME#584, doubled_value#585]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false\n+- 'Project [ID#601, NAME#602, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])\n         +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]\n            +- Project [ID#583, NAME#584, doubled_value#585]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:02:16.659161 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:02:16.660082 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:02:16.661232 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
  +- 'Project [ID#601, NAME#602, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
           +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
              +- Project [ID#583, NAME#584, doubled_value#585]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
  +- 'Project [ID#601, NAME#602, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
           +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
              +- Project [ID#583, NAME#584, doubled_value#585]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:02:16.662444 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:02:16.663023 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:02:16.663600 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:02:16.739542 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
    +- 'Project [ID#601, NAME#602, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
             +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
                +- Project [ID#583, NAME#584, doubled_value#585]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
    +- 'Project [ID#601, NAME#602, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
             +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
                +- Project [ID#583, NAME#584, doubled_value#585]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:02:16.744583 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07ce7a21-79a2-45a1-8669-a49f751d4602', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b09fdacd10>]}
[0m11:02:16.746200 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.37s]
[0m11:02:16.748592 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:02:16.750516 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
    +- 'Project [ID#601, NAME#602, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
             +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
                +- Project [ID#583, NAME#584, doubled_value#585]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
    +- 'Project [ID#601, NAME#602, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
             +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
                +- Project [ID#583, NAME#584, doubled_value#585]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:02:16.754777 [debug] [MainThread]: On master: ROLLBACK
[0m11:02:16.755414 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:02:16.848749 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:02:16.849704 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:16.850547 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:02:16.851381 [debug] [MainThread]: On master: ROLLBACK
[0m11:02:16.852224 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:02:16.852892 [debug] [MainThread]: On master: Close
[0m11:02:16.857911 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:02:16.858442 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:02:16.858906 [info ] [MainThread]: 
[0m11:02:16.859381 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.80 seconds (0.80s).
[0m11:02:16.860499 [debug] [MainThread]: Command end result
[0m11:02:16.895183 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:02:16.898072 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:02:16.904707 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:02:16.905184 [info ] [MainThread]: 
[0m11:02:16.905722 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:02:16.906203 [info ] [MainThread]: 
[0m11:02:16.907013 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
    +- 'Project [ID#601, NAME#602, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
             +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
                +- Project [ID#583, NAME#584, doubled_value#585]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#604, NAME#605, VALUE#606], Partition Cols: []], false, false, false
    +- 'Project [ID#601, NAME#602, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#601,NAME#602,doubled_value#603])
             +- Project [cast(ID#583 as int) AS ID#601, cast(NAME#584 as string) AS NAME#602, cast(doubled_value#585 as int) AS doubled_value#603]
                +- Project [ID#583, NAME#584, doubled_value#585]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#583,name#584,doubled_value#585] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:02:16.907782 [info ] [MainThread]: 
[0m11:02:16.908202 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:02:16.909066 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.495133, "process_in_blocks": "0", "process_kernel_time": 0.193988, "process_mem_max_rss": "109100", "process_out_blocks": "2064", "process_user_time": 2.109007}
[0m11:02:16.909553 [debug] [MainThread]: Command `dbt run` failed at 11:02:16.909453 after 1.50 seconds
[0m11:02:16.910003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a067c650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a104cef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75b0a104c560>]}
[0m11:02:16.910436 [debug] [MainThread]: Flushing usage events
[0m11:02:18.136926 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:04:51.723318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d57edec30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d57e2ff20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d57d980e0>]}


============================== 11:04:51.726401 | 17988717-a5c7-4188-92ed-c057661095e2 ==============================
[0m11:04:51.726401 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:04:51.727154 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models thrift_test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:04:51.812840 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:04:51.813381 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:04:51.813838 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:04:51.947979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '17988717-a5c7-4188-92ed-c057661095e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d5706cd10>]}
[0m11:04:52.006131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '17988717-a5c7-4188-92ed-c057661095e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d57408530>]}
[0m11:04:52.006878 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:04:52.106594 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:04:52.184916 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:04:52.185616 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:04:52.422907 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:04:52.430109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '17988717-a5c7-4188-92ed-c057661095e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d56c66d20>]}
[0m11:04:52.529252 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:04:52.532257 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:04:52.544510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '17988717-a5c7-4188-92ed-c057661095e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d55d1a2d0>]}
[0m11:04:52.545131 [info ] [MainThread]: Found 6 models, 473 macros
[0m11:04:52.545613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '17988717-a5c7-4188-92ed-c057661095e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d570967e0>]}
[0m11:04:52.547178 [info ] [MainThread]: 
[0m11:04:52.547631 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:04:52.548056 [info ] [MainThread]: 
[0m11:04:52.548741 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:04:52.549731 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:04:52.561007 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:04:52.562072 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:04:52.563021 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:04:52.635494 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:04:52.636031 [debug] [ThreadPool]: SQL status: OK in 0.073 seconds
[0m11:04:52.639181 [debug] [ThreadPool]: On list_schemas: Close
[0m11:04:52.650414 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default)
[0m11:04:52.656150 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:52.656696 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:04:52.657153 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:04:52.657600 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:04:52.739209 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:04:52.739828 [debug] [ThreadPool]: SQL status: OK in 0.082 seconds
[0m11:04:52.743612 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:04:52.744208 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:04:52.744707 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:04:52.750140 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m11:04:52.752941 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:52.753687 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:04:52.754173 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:04:52.754630 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:04:52.844137 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:04:52.844825 [debug] [ThreadPool]: SQL status: OK in 0.090 seconds
[0m11:04:52.848869 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:04:52.849539 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:04:52.850054 [debug] [ThreadPool]: On list_None_default: Close
[0m11:04:52.856549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '17988717-a5c7-4188-92ed-c057661095e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d5716aed0>]}
[0m11:04:52.857574 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:52.858356 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:04:52.861260 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:04:52.862029 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:04:52.862747 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.thrift_test)
[0m11:04:52.863376 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:04:52.872751 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:04:52.873759 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:04:52.940403 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:52.940967 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:04:52.941446 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    id,
    name,
    doubled_value
FROM 
    test_staging
  
[0m11:04:52.941972 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:04:53.027031 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:04:53.028064 [debug] [Thread-1 (]: SQL status: OK in 0.086 seconds
[0m11:04:53.050140 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:04:53.050898 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:04:53.103750 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:04:53.104444 [debug] [Thread-1 (]: SQL status: OK in 0.053 seconds
[0m11:04:53.108627 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:04:53.109624 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:04:53.110215 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:04:53.159919 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false\n+- 'Project [ID#667, NAME#668, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])\n         +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]\n            +- Project [id#649, name#650, doubled_value#651]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false\n+- 'Project [ID#667, NAME#668, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])\n         +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]\n            +- Project [id#649, name#650, doubled_value#651]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:04:53.161091 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:04:53.161927 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:04:53.162978 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
  +- 'Project [ID#667, NAME#668, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
           +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
              +- Project [id#649, name#650, doubled_value#651]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
  +- 'Project [ID#667, NAME#668, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
           +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
              +- Project [id#649, name#650, doubled_value#651]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:04:53.164099 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:04:53.164767 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:04:53.165277 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:04:53.210375 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
    +- 'Project [ID#667, NAME#668, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
             +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
                +- Project [id#649, name#650, doubled_value#651]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
    +- 'Project [ID#667, NAME#668, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
             +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
                +- Project [id#649, name#650, doubled_value#651]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:04:53.212946 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '17988717-a5c7-4188-92ed-c057661095e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d55420140>]}
[0m11:04:53.213930 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.35s]
[0m11:04:53.215310 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:04:53.216595 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
    +- 'Project [ID#667, NAME#668, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
             +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
                +- Project [id#649, name#650, doubled_value#651]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
    +- 'Project [ID#667, NAME#668, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
             +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
                +- Project [id#649, name#650, doubled_value#651]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:04:53.219075 [debug] [MainThread]: On master: ROLLBACK
[0m11:04:53.219622 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:04:53.263282 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:04:53.264299 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:04:53.265124 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:04:53.265880 [debug] [MainThread]: On master: ROLLBACK
[0m11:04:53.266625 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:04:53.267365 [debug] [MainThread]: On master: Close
[0m11:04:53.272290 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:04:53.273247 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:04:53.274129 [info ] [MainThread]: 
[0m11:04:53.275063 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.73 seconds (0.73s).
[0m11:04:53.277470 [debug] [MainThread]: Command end result
[0m11:04:53.321715 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:04:53.324856 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:04:53.334068 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:04:53.334552 [info ] [MainThread]: 
[0m11:04:53.335068 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:04:53.335545 [info ] [MainThread]: 
[0m11:04:53.336393 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
    +- 'Project [ID#667, NAME#668, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
             +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
                +- Project [id#649, name#650, doubled_value#651]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#670, NAME#671, VALUE#672], Partition Cols: []], false, false, false
    +- 'Project [ID#667, NAME#668, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#667,name#668,doubled_value#669])
             +- Project [cast(id#649 as int) AS id#667, cast(name#650 as string) AS name#668, cast(doubled_value#651 as int) AS doubled_value#669]
                +- Project [id#649, name#650, doubled_value#651]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#649,name#650,doubled_value#651] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:04:53.337198 [info ] [MainThread]: 
[0m11:04:53.337679 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:04:53.338703 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.6671385, "process_in_blocks": "0", "process_kernel_time": 0.207474, "process_mem_max_rss": "112804", "process_out_blocks": "2992", "process_user_time": 2.355039}
[0m11:04:53.339378 [debug] [MainThread]: Command `dbt run` failed at 11:04:53.339228 after 1.67 seconds
[0m11:04:53.339907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d59784ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d5b517ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x763d57a00c80>]}
[0m11:04:53.340434 [debug] [MainThread]: Flushing usage events
[0m11:04:54.963430 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:06:58.141633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d6c91a450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d6963ede0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d69d8d5e0>]}


============================== 11:06:58.144625 | 9773d70e-ba35-43d8-9b1b-f681d078a500 ==============================
[0m11:06:58.144625 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:06:58.145596 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'fail_fast': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run --models thrift_test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:06:58.232458 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:06:58.233144 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:06:58.233546 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:06:58.377408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9773d70e-ba35-43d8-9b1b-f681d078a500', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d6ae3c3e0>]}
[0m11:06:58.435604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9773d70e-ba35-43d8-9b1b-f681d078a500', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d69104e90>]}
[0m11:06:58.436354 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:06:58.531730 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:06:58.612384 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m11:06:58.613072 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/sources.yml
[0m11:06:58.613568 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:06:58.809318 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_project.thrift_test' (models/thrift_test.sql) depends on a source named 'spark_catalog.test_staging' which was not found
[0m11:06:58.810442 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7251837, "process_in_blocks": "0", "process_kernel_time": 0.195687, "process_mem_max_rss": "107912", "process_out_blocks": "16", "process_user_time": 1.881999}
[0m11:06:58.811017 [debug] [MainThread]: Command `dbt run` failed at 11:06:58.810901 after 0.73 seconds
[0m11:06:58.811462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d692fbe00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d68904a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c5d68b89bb0>]}
[0m11:06:58.811975 [debug] [MainThread]: Flushing usage events
[0m11:07:00.046361 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:07:14.992389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a19a32360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a19f09490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a1b7748f0>]}


============================== 11:07:14.996056 | b6288e1c-3614-4955-84e6-85b49f9608c4 ==============================
[0m11:07:14.996056 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:07:14.996782 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --models thrift_test', 'send_anonymous_usage_stats': 'True'}
[0m11:07:15.108441 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:07:15.109600 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:07:15.110683 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:07:15.254146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b6288e1c-3614-4955-84e6-85b49f9608c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a1a2f0920>]}
[0m11:07:15.313130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b6288e1c-3614-4955-84e6-85b49f9608c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a196c70b0>]}
[0m11:07:15.313913 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:07:15.409927 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:07:15.488154 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m11:07:15.488814 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/sources.yml
[0m11:07:15.489338 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:07:15.774279 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_project.thrift_test' (models/thrift_test.sql) depends on a source named 'spark_catalog.test_staging' which was not found
[0m11:07:15.775565 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.83626145, "process_in_blocks": "0", "process_kernel_time": 0.204011, "process_mem_max_rss": "109596", "process_out_blocks": "16", "process_user_time": 2.002294}
[0m11:07:15.776227 [debug] [MainThread]: Command `dbt run` failed at 11:07:15.776089 after 0.84 seconds
[0m11:07:15.776726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a1a2ce8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a18da7b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d7a18cbf9e0>]}
[0m11:07:15.777230 [debug] [MainThread]: Flushing usage events
[0m11:07:17.059408 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:07:35.605485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x781821a5da90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7818218805c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7818225ce450>]}


============================== 11:07:35.608504 | 5cdcf852-8313-479b-a719-b59694417aab ==============================
[0m11:07:35.608504 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:07:35.609141 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models thrift_test', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:07:35.698958 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:07:35.699672 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:07:35.700207 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:07:35.837919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5cdcf852-8313-479b-a719-b59694417aab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x781820f93920>]}
[0m11:07:35.897075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5cdcf852-8313-479b-a719-b59694417aab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7818217002c0>]}
[0m11:07:35.897812 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:07:35.998989 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:07:36.078053 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m11:07:36.078653 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/sources.yml
[0m11:07:36.079206 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:07:36.413717 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:07:36.432053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5cdcf852-8313-479b-a719-b59694417aab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78181b939be0>]}
[0m11:07:36.513465 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:07:36.516119 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:07:36.526707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5cdcf852-8313-479b-a719-b59694417aab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78181b83f9e0>]}
[0m11:07:36.527299 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m11:07:36.527821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cdcf852-8313-479b-a719-b59694417aab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78182100ae70>]}
[0m11:07:36.529411 [info ] [MainThread]: 
[0m11:07:36.529921 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:07:36.530503 [info ] [MainThread]: 
[0m11:07:36.531181 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:07:36.532189 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:07:36.550133 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:07:36.551174 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:07:36.552111 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:07:36.624879 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:07:36.625361 [debug] [ThreadPool]: SQL status: OK in 0.073 seconds
[0m11:07:36.628407 [debug] [ThreadPool]: On list_schemas: Close
[0m11:07:36.639301 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default)
[0m11:07:36.644847 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:36.645474 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:07:36.645900 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:07:36.646318 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:07:36.712488 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:07:36.713008 [debug] [ThreadPool]: SQL status: OK in 0.067 seconds
[0m11:07:36.715864 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:07:36.716310 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:07:36.716704 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:07:36.720896 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m11:07:36.724590 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:36.725351 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:07:36.726069 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:07:36.726773 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:07:36.815509 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:07:36.816434 [debug] [ThreadPool]: SQL status: OK in 0.090 seconds
[0m11:07:36.820411 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:07:36.821057 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:07:36.821555 [debug] [ThreadPool]: On list_None_default: Close
[0m11:07:36.827901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cdcf852-8313-479b-a719-b59694417aab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x781820f92600>]}
[0m11:07:36.828686 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:36.829210 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:07:36.831627 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:07:36.832355 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:07:36.832960 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.thrift_test)
[0m11:07:36.833415 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:07:36.842674 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:07:36.843748 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:07:36.909122 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:36.909671 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:07:36.910145 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT
    id AS ID,
    name AS NAME,
    doubled_value AS DOUBLED_VALUE
FROM
    default.test_staging
  
[0m11:07:36.910608 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:36.984046 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:07:36.985158 [debug] [Thread-1 (]: SQL status: OK in 0.074 seconds
[0m11:07:37.016269 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:07:37.017011 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:07:37.063726 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:07:37.064510 [debug] [Thread-1 (]: SQL status: OK in 0.047 seconds
[0m11:07:37.068187 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:07:37.069022 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:07:37.069556 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:07:37.127498 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false\n+- 'Project [ID#739, NAME#740, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])\n         +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]\n            +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false\n+- 'Project [ID#739, NAME#740, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])\n         +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]\n            +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:07:37.128644 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:07:37.129364 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:07:37.130367 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
  +- 'Project [ID#739, NAME#740, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
           +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
              +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
  +- 'Project [ID#739, NAME#740, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
           +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
              +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:07:37.131485 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:07:37.132032 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:07:37.132534 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:07:37.174657 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
    +- 'Project [ID#739, NAME#740, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
             +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
                +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
    +- 'Project [ID#739, NAME#740, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
             +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
                +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:07:37.177466 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cdcf852-8313-479b-a719-b59694417aab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78181af342f0>]}
[0m11:07:37.178588 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.34s]
[0m11:07:37.180082 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:07:37.181468 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
    +- 'Project [ID#739, NAME#740, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
             +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
                +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
    +- 'Project [ID#739, NAME#740, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
             +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
                +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:07:37.184173 [debug] [MainThread]: On master: ROLLBACK
[0m11:07:37.184891 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:07:37.229988 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:07:37.231046 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:37.231971 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:07:37.232845 [debug] [MainThread]: On master: ROLLBACK
[0m11:07:37.233720 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:07:37.234561 [debug] [MainThread]: On master: Close
[0m11:07:37.241739 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:07:37.242700 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:07:37.243610 [info ] [MainThread]: 
[0m11:07:37.244698 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.71 seconds (0.71s).
[0m11:07:37.246427 [debug] [MainThread]: Command end result
[0m11:07:37.270421 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:07:37.273915 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:07:37.285857 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:07:37.286918 [info ] [MainThread]: 
[0m11:07:37.287890 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:07:37.288679 [info ] [MainThread]: 
[0m11:07:37.289829 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
    +- 'Project [ID#739, NAME#740, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
             +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
                +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `NAME`, `DOUBLED_VALUE`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#742, NAME#743, VALUE#744], Partition Cols: []], false, false, false
    +- 'Project [ID#739, NAME#740, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#739,NAME#740,DOUBLED_VALUE#741])
             +- Project [cast(ID#736 as int) AS ID#739, cast(NAME#737 as string) AS NAME#740, cast(DOUBLED_VALUE#738 as int) AS DOUBLED_VALUE#741]
                +- Project [id#718 AS ID#736, name#719 AS NAME#737, doubled_value#720 AS DOUBLED_VALUE#738]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#718,name#719,doubled_value#720] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:07:37.291131 [info ] [MainThread]: 
[0m11:07:37.291756 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:07:37.293088 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.7401341, "process_in_blocks": "0", "process_kernel_time": 0.209617, "process_mem_max_rss": "114532", "process_out_blocks": "3008", "process_user_time": 2.491452}
[0m11:07:37.293742 [debug] [MainThread]: Command `dbt run` failed at 11:07:37.293595 after 1.74 seconds
[0m11:07:37.294485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7818216a1730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x781820f77fe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7818217007d0>]}
[0m11:07:37.295203 [debug] [MainThread]: Flushing usage events
[0m11:07:39.367149 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:08:25.976908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173fd4d640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173f923e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173fb21c10>]}


============================== 11:08:25.980256 | a73fb2fd-2309-4155-bc34-10c57720c70e ==============================
[0m11:08:25.980256 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:08:25.980999 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --models thrift_test', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:08:26.068601 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:08:26.069239 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:08:26.069705 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:08:26.208445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a73fb2fd-2309-4155-bc34-10c57720c70e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173fe629f0>]}
[0m11:08:26.268812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a73fb2fd-2309-4155-bc34-10c57720c70e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173ef6e5d0>]}
[0m11:08:26.269543 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:08:26.375567 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:08:26.474646 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:08:26.475422 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:08:26.742110 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:08:26.763145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a73fb2fd-2309-4155-bc34-10c57720c70e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173e99a1b0>]}
[0m11:08:26.842619 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:08:26.845513 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:08:26.856739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a73fb2fd-2309-4155-bc34-10c57720c70e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173eb14740>]}
[0m11:08:26.857285 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m11:08:26.857713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a73fb2fd-2309-4155-bc34-10c57720c70e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173f630410>]}
[0m11:08:26.859276 [info ] [MainThread]: 
[0m11:08:26.859743 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:08:26.860272 [info ] [MainThread]: 
[0m11:08:26.860928 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:08:26.861915 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:08:26.879298 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:08:26.880292 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:08:26.881102 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:08:26.946281 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:08:26.947013 [debug] [ThreadPool]: SQL status: OK in 0.066 seconds
[0m11:08:26.951059 [debug] [ThreadPool]: On list_schemas: Close
[0m11:08:26.967691 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:08:26.974044 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:26.974716 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:08:26.975159 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:08:26.975576 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:08:27.067001 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:08:27.067608 [debug] [ThreadPool]: SQL status: OK in 0.092 seconds
[0m11:08:27.071060 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:08:27.071641 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:08:27.072133 [debug] [ThreadPool]: On list_None_default: Close
[0m11:08:27.077235 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default, now list_None_default_default)
[0m11:08:27.080743 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:27.081326 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:08:27.081830 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:08:27.082305 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:08:27.165470 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:08:27.166039 [debug] [ThreadPool]: SQL status: OK in 0.084 seconds
[0m11:08:27.169113 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:08:27.169688 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:08:27.170136 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:08:27.175548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a73fb2fd-2309-4155-bc34-10c57720c70e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e174108d2b0>]}
[0m11:08:27.176222 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:27.176713 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:08:27.179024 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:08:27.179803 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:08:27.180450 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default, now model.dbt_spark_project.thrift_test)
[0m11:08:27.180978 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:08:27.188600 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:08:27.189440 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:08:27.252258 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:27.252837 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:08:27.253334 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    ID,
    name,
    doubled_value
FROM 
    test_staging
  
[0m11:08:27.253794 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:08:27.329409 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:08:27.330197 [debug] [Thread-1 (]: SQL status: OK in 0.076 seconds
[0m11:08:27.353291 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:08:27.353992 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:08:27.405131 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:08:27.405829 [debug] [Thread-1 (]: SQL status: OK in 0.051 seconds
[0m11:08:27.410221 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:08:27.411150 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:08:27.411748 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:08:27.468686 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false\n+- 'Project [ID#808, NAME#809, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])\n         +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]\n            +- Project [ID#790, name#791, doubled_value#792]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false\n+- 'Project [ID#808, NAME#809, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])\n         +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]\n            +- Project [ID#790, name#791, doubled_value#792]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:08:27.469822 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:08:27.470671 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:08:27.471727 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
  +- 'Project [ID#808, NAME#809, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
           +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
              +- Project [ID#790, name#791, doubled_value#792]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
  +- 'Project [ID#808, NAME#809, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
           +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
              +- Project [ID#790, name#791, doubled_value#792]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:08:27.473063 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:08:27.473740 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:08:27.474344 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:08:27.529622 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
    +- 'Project [ID#808, NAME#809, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
             +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
                +- Project [ID#790, name#791, doubled_value#792]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
    +- 'Project [ID#808, NAME#809, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
             +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
                +- Project [ID#790, name#791, doubled_value#792]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:08:27.531830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a73fb2fd-2309-4155-bc34-10c57720c70e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173da51bb0>]}
[0m11:08:27.532626 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.35s]
[0m11:08:27.533730 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:08:27.534727 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
    +- 'Project [ID#808, NAME#809, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
             +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
                +- Project [ID#790, name#791, doubled_value#792]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
    +- 'Project [ID#808, NAME#809, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
             +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
                +- Project [ID#790, name#791, doubled_value#792]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:08:27.536695 [debug] [MainThread]: On master: ROLLBACK
[0m11:08:27.537116 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:08:27.574733 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:08:27.575252 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:08:27.575677 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:08:27.576091 [debug] [MainThread]: On master: ROLLBACK
[0m11:08:27.576498 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:08:27.576916 [debug] [MainThread]: On master: Close
[0m11:08:27.580670 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:08:27.581106 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:08:27.581483 [info ] [MainThread]: 
[0m11:08:27.581948 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.72 seconds (0.72s).
[0m11:08:27.582939 [debug] [MainThread]: Command end result
[0m11:08:27.618616 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:08:27.620787 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:08:27.628504 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:08:27.629138 [info ] [MainThread]: 
[0m11:08:27.629769 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:08:27.630327 [info ] [MainThread]: 
[0m11:08:27.631366 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
    +- 'Project [ID#808, NAME#809, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
             +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
                +- Project [ID#790, name#791, doubled_value#792]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#811, NAME#812, VALUE#813], Partition Cols: []], false, false, false
    +- 'Project [ID#808, NAME#809, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#808,name#809,doubled_value#810])
             +- Project [cast(ID#790 as int) AS ID#808, cast(name#791 as string) AS name#809, cast(doubled_value#792 as int) AS doubled_value#810]
                +- Project [ID#790, name#791, doubled_value#792]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#790,name#791,doubled_value#792] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:08:27.632433 [info ] [MainThread]: 
[0m11:08:27.633027 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:08:27.634220 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.7119554, "process_in_blocks": "0", "process_kernel_time": 0.186607, "process_mem_max_rss": "112732", "process_out_blocks": "3016", "process_user_time": 2.390966}
[0m11:08:27.634953 [debug] [MainThread]: Command `dbt run` failed at 11:08:27.634778 after 1.71 seconds
[0m11:08:27.635923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e173f60d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e17400d0740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e17400d0bc0>]}
[0m11:08:27.636928 [debug] [MainThread]: Flushing usage events
[0m11:08:28.930517 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:15:50.979756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bffa238f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bff274740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bff2a13d0>]}


============================== 11:15:50.982786 | 18fcea38-d1e9-4116-86c4-941d77e5fe88 ==============================
[0m11:15:50.982786 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:15:50.983556 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --models thrift_test', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:15:51.061615 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:15:51.062176 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:15:51.062590 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:15:51.205935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '18fcea38-d1e9-4116-86c4-941d77e5fe88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bff875b50>]}
[0m11:15:51.269791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '18fcea38-d1e9-4116-86c4-941d77e5fe88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bff875b50>]}
[0m11:15:51.270556 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:15:51.364890 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:15:51.457408 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:15:51.457904 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:15:51.464431 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:15:51.507272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18fcea38-d1e9-4116-86c4-941d77e5fe88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bfe474710>]}
[0m11:15:51.577145 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:15:51.579690 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:15:51.591311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18fcea38-d1e9-4116-86c4-941d77e5fe88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bfe3ee780>]}
[0m11:15:51.591913 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m11:15:51.592346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18fcea38-d1e9-4116-86c4-941d77e5fe88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bfe4a10d0>]}
[0m11:15:51.593870 [info ] [MainThread]: 
[0m11:15:51.594334 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:15:51.594815 [info ] [MainThread]: 
[0m11:15:51.595454 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:15:51.596427 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:15:51.616859 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:15:51.617934 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:15:51.618823 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:15:51.679859 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:15:51.680395 [debug] [ThreadPool]: SQL status: OK in 0.062 seconds
[0m11:15:51.683275 [debug] [ThreadPool]: On list_schemas: Close
[0m11:15:51.695682 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:15:51.703349 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:15:51.703917 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:15:51.704413 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:15:51.704895 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:15:51.791417 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:15:51.792004 [debug] [ThreadPool]: SQL status: OK in 0.087 seconds
[0m11:15:51.795335 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:15:51.795867 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:15:51.796266 [debug] [ThreadPool]: On list_None_default: Close
[0m11:15:51.800579 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default, now list_None_default_default)
[0m11:15:51.803553 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:15:51.804031 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:15:51.804453 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:15:51.804856 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:15:51.864728 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:15:51.865259 [debug] [ThreadPool]: SQL status: OK in 0.060 seconds
[0m11:15:51.868250 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:15:51.868783 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:15:51.869208 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:15:51.873459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18fcea38-d1e9-4116-86c4-941d77e5fe88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759c0088d010>]}
[0m11:15:51.874050 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:15:51.874458 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:15:51.876686 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:15:51.877337 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:15:51.877942 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default, now model.dbt_spark_project.thrift_test)
[0m11:15:51.878415 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:15:51.885790 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:15:51.886577 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:15:51.942893 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:15:51.943440 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:15:51.943988 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    ID,
    name,
    doubled_value
FROM 
    test_staging
  
[0m11:15:51.944440 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:15:52.005980 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:15:52.006694 [debug] [Thread-1 (]: SQL status: OK in 0.062 seconds
[0m11:15:52.032902 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:15:52.033836 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:15:52.077867 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:15:52.078476 [debug] [Thread-1 (]: SQL status: OK in 0.044 seconds
[0m11:15:52.081947 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:15:52.082724 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:15:52.083254 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:15:52.124639 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false\n+- 'Project [ID#894, NAME#895, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])\n         +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]\n            +- Project [ID#876, name#877, doubled_value#878]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false\n+- 'Project [ID#894, NAME#895, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])\n         +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]\n            +- Project [ID#876, name#877, doubled_value#878]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:15:52.125600 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:15:52.126255 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:15:52.127101 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
  +- 'Project [ID#894, NAME#895, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
           +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
              +- Project [ID#876, name#877, doubled_value#878]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
  +- 'Project [ID#894, NAME#895, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
           +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
              +- Project [ID#876, name#877, doubled_value#878]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:15:52.128102 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:15:52.128593 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:15:52.129089 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:15:52.165804 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
    +- 'Project [ID#894, NAME#895, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
             +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
                +- Project [ID#876, name#877, doubled_value#878]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
    +- 'Project [ID#894, NAME#895, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
             +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
                +- Project [ID#876, name#877, doubled_value#878]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:15:52.168237 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '18fcea38-d1e9-4116-86c4-941d77e5fe88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bfe22bfb0>]}
[0m11:15:52.169122 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.29s]
[0m11:15:52.170327 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:15:52.171479 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
    +- 'Project [ID#894, NAME#895, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
             +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
                +- Project [ID#876, name#877, doubled_value#878]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
    +- 'Project [ID#894, NAME#895, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
             +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
                +- Project [ID#876, name#877, doubled_value#878]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:15:52.174511 [debug] [MainThread]: On master: ROLLBACK
[0m11:15:52.175169 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:15:52.218387 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:15:52.219144 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:15:52.219739 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:15:52.220369 [debug] [MainThread]: On master: ROLLBACK
[0m11:15:52.221009 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:15:52.221555 [debug] [MainThread]: On master: Close
[0m11:15:52.227325 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:15:52.227853 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:15:52.228316 [info ] [MainThread]: 
[0m11:15:52.228822 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.63 seconds (0.63s).
[0m11:15:52.229969 [debug] [MainThread]: Command end result
[0m11:15:52.269039 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:15:52.270778 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:15:52.276791 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:15:52.277272 [info ] [MainThread]: 
[0m11:15:52.277776 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:15:52.278258 [info ] [MainThread]: 
[0m11:15:52.279067 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
    +- 'Project [ID#894, NAME#895, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
             +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
                +- Project [ID#876, name#877, doubled_value#878]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#897, NAME#898, VALUE#899], Partition Cols: []], false, false, false
    +- 'Project [ID#894, NAME#895, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#894,name#895,doubled_value#896])
             +- Project [cast(ID#876 as int) AS ID#894, cast(name#877 as string) AS name#895, cast(doubled_value#878 as int) AS doubled_value#896]
                +- Project [ID#876, name#877, doubled_value#878]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#876,name#877,doubled_value#878] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:15:52.279950 [info ] [MainThread]: 
[0m11:15:52.280449 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:15:52.281473 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.3546494, "process_in_blocks": "0", "process_kernel_time": 0.193403, "process_mem_max_rss": "109388", "process_out_blocks": "2072", "process_user_time": 2.080585}
[0m11:15:52.282099 [debug] [MainThread]: Command `dbt run` failed at 11:15:52.281977 after 1.36 seconds
[0m11:15:52.282578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bff8503b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bff850bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x759bfc072e70>]}
[0m11:15:52.283199 [debug] [MainThread]: Flushing usage events
[0m11:15:53.550853 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:19:23.994887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c38a0a40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c506cbc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c4b31160>]}


============================== 11:19:23.997904 | b2377f80-abd9-469c-b82d-bf1fb599a7e0 ==============================
[0m11:19:23.997904 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:19:23.998671 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models thrift_test', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:19:24.087360 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:19:24.087997 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:19:24.088407 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:19:24.230026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2377f80-abd9-469c-b82d-bf1fb599a7e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c311cad0>]}
[0m11:19:24.288937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2377f80-abd9-469c-b82d-bf1fb599a7e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c3b85d90>]}
[0m11:19:24.289732 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:19:24.391833 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:19:24.485283 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:19:24.486000 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/schema.yml
[0m11:19:24.559818 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:19:24.578185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b2377f80-abd9-469c-b82d-bf1fb599a7e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c334a3f0>]}
[0m11:19:24.654243 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:19:24.656910 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:19:24.668370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b2377f80-abd9-469c-b82d-bf1fb599a7e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c26e1d60>]}
[0m11:19:24.669106 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m11:19:24.669683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2377f80-abd9-469c-b82d-bf1fb599a7e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c24a8b90>]}
[0m11:19:24.671391 [info ] [MainThread]: 
[0m11:19:24.671988 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:19:24.672447 [info ] [MainThread]: 
[0m11:19:24.673244 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:19:24.674795 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:19:24.694976 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:19:24.696053 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:19:24.697004 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:19:24.756987 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:19:24.757694 [debug] [ThreadPool]: SQL status: OK in 0.061 seconds
[0m11:19:24.760909 [debug] [ThreadPool]: On list_schemas: Close
[0m11:19:24.772804 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default)
[0m11:19:24.779315 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:19:24.780066 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:19:24.780612 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:19:24.781141 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:19:24.857066 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:19:24.857598 [debug] [ThreadPool]: SQL status: OK in 0.076 seconds
[0m11:19:24.861387 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:19:24.861969 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:19:24.862387 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:19:24.866526 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m11:19:24.868991 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:19:24.869455 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:19:24.869878 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:19:24.870278 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:19:24.948425 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:19:24.948960 [debug] [ThreadPool]: SQL status: OK in 0.079 seconds
[0m11:19:24.952262 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:19:24.952809 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:19:24.953258 [debug] [ThreadPool]: On list_None_default: Close
[0m11:19:24.958434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2377f80-abd9-469c-b82d-bf1fb599a7e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c24bfb30>]}
[0m11:19:24.959058 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:19:24.959477 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:19:24.961754 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:19:24.962397 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:19:24.963009 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.thrift_test)
[0m11:19:24.963473 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:19:24.971944 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:19:24.972797 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:19:25.030583 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:19:25.031141 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:19:25.031608 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    ID,
    name,
    doubled_value
FROM 
    test_staging
  
[0m11:19:25.032184 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:19:25.090823 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:19:25.091835 [debug] [Thread-1 (]: SQL status: OK in 0.060 seconds
[0m11:19:25.115103 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:19:25.116053 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:19:25.164912 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:19:25.165579 [debug] [Thread-1 (]: SQL status: OK in 0.049 seconds
[0m11:19:25.169315 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:19:25.170189 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:19:25.170741 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:19:25.217454 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false\n+- 'Project [ID#960, NAME#961, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])\n         +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]\n            +- Project [ID#942, name#943, doubled_value#944]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false\n+- 'Project [ID#960, NAME#961, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])\n         +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]\n            +- Project [ID#942, name#943, doubled_value#944]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:19:25.218481 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:19:25.219191 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:19:25.220172 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
  +- 'Project [ID#960, NAME#961, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
           +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
              +- Project [ID#942, name#943, doubled_value#944]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
  +- 'Project [ID#960, NAME#961, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
           +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
              +- Project [ID#942, name#943, doubled_value#944]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:19:25.221324 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:19:25.221919 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:19:25.222449 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:19:25.262845 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
    +- 'Project [ID#960, NAME#961, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
             +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
                +- Project [ID#942, name#943, doubled_value#944]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
    +- 'Project [ID#960, NAME#961, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
             +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
                +- Project [ID#942, name#943, doubled_value#944]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:19:25.265616 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2377f80-abd9-469c-b82d-bf1fb599a7e0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c1b9a450>]}
[0m11:19:25.266760 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.30s]
[0m11:19:25.268134 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:19:25.269456 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
    +- 'Project [ID#960, NAME#961, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
             +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
                +- Project [ID#942, name#943, doubled_value#944]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
    +- 'Project [ID#960, NAME#961, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
             +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
                +- Project [ID#942, name#943, doubled_value#944]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:19:25.272870 [debug] [MainThread]: On master: ROLLBACK
[0m11:19:25.273863 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:19:25.313452 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:19:25.313978 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:19:25.314391 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:19:25.314828 [debug] [MainThread]: On master: ROLLBACK
[0m11:19:25.315193 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:19:25.315567 [debug] [MainThread]: On master: Close
[0m11:19:25.318888 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:19:25.319370 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:19:25.319869 [info ] [MainThread]: 
[0m11:19:25.320380 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.65 seconds (0.65s).
[0m11:19:25.321443 [debug] [MainThread]: Command end result
[0m11:19:25.345833 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:19:25.349084 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:19:25.361700 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:19:25.362386 [info ] [MainThread]: 
[0m11:19:25.363147 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:19:25.363813 [info ] [MainThread]: 
[0m11:19:25.365004 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
    +- 'Project [ID#960, NAME#961, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
             +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
                +- Project [ID#942, name#943, doubled_value#944]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#963, NAME#964, VALUE#965], Partition Cols: []], false, false, false
    +- 'Project [ID#960, NAME#961, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#960,name#961,doubled_value#962])
             +- Project [cast(ID#942 as int) AS ID#960, cast(name#943 as string) AS name#961, cast(doubled_value#944 as int) AS doubled_value#962]
                +- Project [ID#942, name#943, doubled_value#944]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#942,name#943,doubled_value#944] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:19:25.366291 [info ] [MainThread]: 
[0m11:19:25.367073 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:19:25.368587 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.431055, "process_in_blocks": "0", "process_kernel_time": 0.182458, "process_mem_max_rss": "109776", "process_out_blocks": "3016", "process_user_time": 2.214432}
[0m11:19:25.369612 [debug] [MainThread]: Command `dbt run` failed at 11:19:25.369418 after 1.43 seconds
[0m11:19:25.370360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c3381970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c6bb3ec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7642c2796ed0>]}
[0m11:19:25.371089 [debug] [MainThread]: Flushing usage events
[0m11:19:35.372542 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:21:44.128497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523df97da0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523e6e51f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523e335af0>]}


============================== 11:21:44.131940 | 14962f2a-bce2-4d5a-9943-7477f9f3b3fc ==============================
[0m11:21:44.131940 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:21:44.132579 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt run --models thrift_test', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:21:44.213891 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:21:44.214615 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:21:44.215293 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:21:44.360445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '14962f2a-bce2-4d5a-9943-7477f9f3b3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523da46420>]}
[0m11:21:44.418791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '14962f2a-bce2-4d5a-9943-7477f9f3b3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523e52caa0>]}
[0m11:21:44.419545 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:21:44.519777 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:21:44.616791 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:21:44.617263 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:21:44.623779 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:21:44.669958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '14962f2a-bce2-4d5a-9943-7477f9f3b3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523d144980>]}
[0m11:21:44.743642 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:21:44.746423 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:21:44.757448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '14962f2a-bce2-4d5a-9943-7477f9f3b3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523cf546e0>]}
[0m11:21:44.758111 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m11:21:44.758630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14962f2a-bce2-4d5a-9943-7477f9f3b3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523d07aab0>]}
[0m11:21:44.760177 [info ] [MainThread]: 
[0m11:21:44.760689 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:21:44.761256 [info ] [MainThread]: 
[0m11:21:44.761964 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:21:44.763017 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:21:44.781988 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:21:44.783000 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:21:44.783895 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:21:44.851303 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:21:44.851872 [debug] [ThreadPool]: SQL status: OK in 0.068 seconds
[0m11:21:44.854769 [debug] [ThreadPool]: On list_schemas: Close
[0m11:21:44.869220 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m11:21:44.882932 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:21:44.883992 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:21:44.884937 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:21:44.885840 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:21:44.970983 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:21:44.971556 [debug] [ThreadPool]: SQL status: OK in 0.086 seconds
[0m11:21:44.974737 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:21:44.975241 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:21:44.975631 [debug] [ThreadPool]: On list_None_default: Close
[0m11:21:44.979843 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default, now list_None_default_default)
[0m11:21:44.982860 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:21:44.983297 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:21:44.983689 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:21:44.984067 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:21:45.048695 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:21:45.049311 [debug] [ThreadPool]: SQL status: OK in 0.065 seconds
[0m11:21:45.052822 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:21:45.053416 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:21:45.053897 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:21:45.058924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14962f2a-bce2-4d5a-9943-7477f9f3b3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523f4c50a0>]}
[0m11:21:45.059915 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:21:45.060634 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:21:45.063679 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:21:45.064382 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:21:45.065010 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default_default, now model.dbt_spark_project.thrift_test)
[0m11:21:45.065509 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:21:45.074819 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:21:45.075818 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:21:45.150437 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:21:45.151036 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:21:45.151529 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    ID,
    name,
    doubled_value
FROM 
    test_staging
  
[0m11:21:45.152013 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:21:45.224968 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:21:45.226078 [debug] [Thread-1 (]: SQL status: OK in 0.074 seconds
[0m11:21:45.257182 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:21:45.257971 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:21:45.304593 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:21:45.305294 [debug] [Thread-1 (]: SQL status: OK in 0.047 seconds
[0m11:21:45.309078 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:21:45.310053 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:21:45.310689 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:21:45.357473 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false\n+- 'Project [ID#1026, NAME#1027, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])\n         +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]\n            +- Project [ID#1008, name#1009, doubled_value#1010]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false\n+- 'Project [ID#1026, NAME#1027, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])\n         +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]\n            +- Project [ID#1008, name#1009, doubled_value#1010]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:21:45.358518 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:21:45.359488 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:21:45.360560 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
  +- 'Project [ID#1026, NAME#1027, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
           +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
              +- Project [ID#1008, name#1009, doubled_value#1010]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
  +- 'Project [ID#1026, NAME#1027, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
           +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
              +- Project [ID#1008, name#1009, doubled_value#1010]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:21:45.361717 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:21:45.362299 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:21:45.362872 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:21:45.405701 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
    +- 'Project [ID#1026, NAME#1027, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
             +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
                +- Project [ID#1008, name#1009, doubled_value#1010]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
    +- 'Project [ID#1026, NAME#1027, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
             +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
                +- Project [ID#1008, name#1009, doubled_value#1010]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:21:45.407888 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14962f2a-bce2-4d5a-9943-7477f9f3b3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523cf768a0>]}
[0m11:21:45.408655 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.34s]
[0m11:21:45.409657 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:21:45.410630 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
    +- 'Project [ID#1026, NAME#1027, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
             +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
                +- Project [ID#1008, name#1009, doubled_value#1010]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
    +- 'Project [ID#1026, NAME#1027, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
             +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
                +- Project [ID#1008, name#1009, doubled_value#1010]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:21:45.412971 [debug] [MainThread]: On master: ROLLBACK
[0m11:21:45.413395 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:21:45.458103 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:21:45.458953 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:21:45.459596 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:21:45.460249 [debug] [MainThread]: On master: ROLLBACK
[0m11:21:45.461043 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:21:45.461944 [debug] [MainThread]: On master: Close
[0m11:21:45.468235 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:21:45.468828 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:21:45.469366 [info ] [MainThread]: 
[0m11:21:45.469974 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.71 seconds (0.71s).
[0m11:21:45.471315 [debug] [MainThread]: Command end result
[0m11:21:45.511918 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:21:45.513788 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:21:45.520369 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:21:45.520820 [info ] [MainThread]: 
[0m11:21:45.521302 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:21:45.521726 [info ] [MainThread]: 
[0m11:21:45.522469 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
    +- 'Project [ID#1026, NAME#1027, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
             +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
                +- Project [ID#1008, name#1009, doubled_value#1010]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`ID`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1029, NAME#1030, VALUE#1031], Partition Cols: []], false, false, false
    +- 'Project [ID#1026, NAME#1027, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [ID#1026,name#1027,doubled_value#1028])
             +- Project [cast(ID#1008 as int) AS ID#1026, cast(name#1009 as string) AS name#1027, cast(doubled_value#1010 as int) AS doubled_value#1028]
                +- Project [ID#1008, name#1009, doubled_value#1010]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1008,name#1009,doubled_value#1010] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:21:45.523188 [info ] [MainThread]: 
[0m11:21:45.523643 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:21:45.524561 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.4553988, "process_in_blocks": "0", "process_kernel_time": 0.197459, "process_mem_max_rss": "109728", "process_out_blocks": "2072", "process_user_time": 2.241864}
[0m11:21:45.525210 [debug] [MainThread]: Command `dbt run` failed at 11:21:45.525043 after 1.46 seconds
[0m11:21:45.525697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523e2de600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523d3a7ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b523d07aea0>]}
[0m11:21:45.526168 [debug] [MainThread]: Flushing usage events
[0m11:21:46.966487 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:22:49.086110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac8391ede0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac836d55b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac8328b1d0>]}


============================== 11:22:49.089107 | dcd93385-6977-4691-86b4-d737ae5cf575 ==============================
[0m11:22:49.089107 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:22:49.089820 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --models thrift_test', 'send_anonymous_usage_stats': 'True'}
[0m11:22:49.182871 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:22:49.183413 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:22:49.183880 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:22:49.332695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dcd93385-6977-4691-86b4-d737ae5cf575', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac832adbe0>]}
[0m11:22:49.392992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dcd93385-6977-4691-86b4-d737ae5cf575', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac82240800>]}
[0m11:22:49.393804 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:22:49.491240 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:22:49.587723 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:22:49.588400 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:22:49.839214 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:22:49.854160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dcd93385-6977-4691-86b4-d737ae5cf575', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac81ea5490>]}
[0m11:22:49.949907 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:22:49.952571 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:22:49.963447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dcd93385-6977-4691-86b4-d737ae5cf575', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac80d17b90>]}
[0m11:22:49.964069 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m11:22:49.964547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dcd93385-6977-4691-86b4-d737ae5cf575', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac81cd1af0>]}
[0m11:22:49.966014 [info ] [MainThread]: 
[0m11:22:49.966525 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:22:49.966974 [info ] [MainThread]: 
[0m11:22:49.967708 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:22:49.968694 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:22:49.979524 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:22:49.980017 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:22:49.980407 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:22:50.051030 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:22:50.051598 [debug] [ThreadPool]: SQL status: OK in 0.071 seconds
[0m11:22:50.054566 [debug] [ThreadPool]: On list_schemas: Close
[0m11:22:50.065479 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default)
[0m11:22:50.071157 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:50.071794 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:22:50.072229 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:22:50.072657 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:22:50.143486 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:22:50.144117 [debug] [ThreadPool]: SQL status: OK in 0.071 seconds
[0m11:22:50.147923 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:22:50.148601 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:22:50.149217 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:22:50.154341 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m11:22:50.157151 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:50.157767 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:22:50.158324 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:22:50.158875 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:22:50.234462 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:22:50.235111 [debug] [ThreadPool]: SQL status: OK in 0.076 seconds
[0m11:22:50.239222 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:22:50.239916 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:22:50.240444 [debug] [ThreadPool]: On list_None_default: Close
[0m11:22:50.246162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dcd93385-6977-4691-86b4-d737ae5cf575', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac80d204d0>]}
[0m11:22:50.246930 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:50.247485 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:22:50.250003 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:22:50.250876 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:22:50.251607 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.thrift_test)
[0m11:22:50.252245 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:22:50.262234 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:22:50.263547 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:22:50.331441 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:50.331997 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:22:50.332515 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    id,
    name,
    value
FROM 
    test_staging
  
[0m11:22:50.333022 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:22:50.428086 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;\n'CreateViewCommand `test__dbt_tmp`, SELECT \n    id,\n    name,\n    value\nFROM \n    test_staging, false, true, LocalTempView, false\n+- 'Project [id#1074, name#1075, 'value]\n   +- SubqueryAlias spark_catalog.default.test_staging\n      +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;\n'CreateViewCommand `test__dbt_tmp`, SELECT \n    id,\n    name,\n    value\nFROM \n    test_staging, false, true, LocalTempView, false\n+- 'Project [id#1074, name#1075, 'value]\n   +- SubqueryAlias spark_catalog.default.test_staging\n      +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:22:50.429633 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:22:50.431059 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    id,
    name,
    value
FROM 
    test_staging
  
[0m11:22:50.432872 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
  'CreateViewCommand `test__dbt_tmp`, SELECT 
      id,
      name,
      value
  FROM 
      test_staging, false, true, LocalTempView, false
  +- 'Project [id#1074, name#1075, 'value]
     +- SubqueryAlias spark_catalog.default.test_staging
        +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
  'CreateViewCommand `test__dbt_tmp`, SELECT 
      id,
      name,
      value
  FROM 
      test_staging, false, true, LocalTempView, false
  +- 'Project [id#1074, name#1075, 'value]
     +- SubqueryAlias spark_catalog.default.test_staging
        +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.immutable.List.foreach(List.scala:431)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:22:50.434906 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:22:50.435575 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:22:50.436257 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:22:50.453234 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
    'CreateViewCommand `test__dbt_tmp`, SELECT 
        id,
        name,
        value
    FROM 
        test_staging, false, true, LocalTempView, false
    +- 'Project [id#1074, name#1075, 'value]
       +- SubqueryAlias spark_catalog.default.test_staging
          +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
    'CreateViewCommand `test__dbt_tmp`, SELECT 
        id,
        name,
        value
    FROM 
        test_staging, false, true, LocalTempView, false
    +- 'Project [id#1074, name#1075, 'value]
       +- SubqueryAlias spark_catalog.default.test_staging
          +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:22:50.456940 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dcd93385-6977-4691-86b4-d737ae5cf575', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac83459c40>]}
[0m11:22:50.458366 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.20s]
[0m11:22:50.460337 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:22:50.461690 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
    'CreateViewCommand `test__dbt_tmp`, SELECT 
        id,
        name,
        value
    FROM 
        test_staging, false, true, LocalTempView, false
    +- 'Project [id#1074, name#1075, 'value]
       +- SubqueryAlias spark_catalog.default.test_staging
          +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
    'CreateViewCommand `test__dbt_tmp`, SELECT 
        id,
        name,
        value
    FROM 
        test_staging, false, true, LocalTempView, false
    +- 'Project [id#1074, name#1075, 'value]
       +- SubqueryAlias spark_catalog.default.test_staging
          +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:22:50.464011 [debug] [MainThread]: On master: ROLLBACK
[0m11:22:50.464733 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:22:50.514190 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:22:50.515009 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:22:50.515722 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:22:50.516493 [debug] [MainThread]: On master: ROLLBACK
[0m11:22:50.517476 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:22:50.518491 [debug] [MainThread]: On master: Close
[0m11:22:50.523635 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:22:50.524429 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:22:50.525182 [info ] [MainThread]: 
[0m11:22:50.525960 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.56 seconds (0.56s).
[0m11:22:50.527904 [debug] [MainThread]: Command end result
[0m11:22:50.576540 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:22:50.579596 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:22:50.590991 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:22:50.591648 [info ] [MainThread]: 
[0m11:22:50.592363 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:22:50.593018 [info ] [MainThread]: 
[0m11:22:50.594160 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
    'CreateViewCommand `test__dbt_tmp`, SELECT 
        id,
        name,
        value
    FROM 
        test_staging, false, true, LocalTempView, false
    +- 'Project [id#1074, name#1075, 'value]
       +- SubqueryAlias spark_catalog.default.test_staging
          +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 11 pos 4;
    'CreateViewCommand `test__dbt_tmp`, SELECT 
        id,
        name,
        value
    FROM 
        test_staging, false, true, LocalTempView, false
    +- 'Project [id#1074, name#1075, 'value]
       +- SubqueryAlias spark_catalog.default.test_staging
          +- Relation spark_catalog.default.test_staging[id#1074,name#1075,doubled_value#1076] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.immutable.List.foreach(List.scala:431)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:22:50.595344 [info ] [MainThread]: 
[0m11:22:50.596020 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:22:50.597370 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.5667971, "process_in_blocks": "0", "process_kernel_time": 0.209417, "process_mem_max_rss": "113088", "process_out_blocks": "3000", "process_user_time": 2.525968}
[0m11:22:50.598287 [debug] [MainThread]: Command `dbt run` failed at 11:22:50.598080 after 1.57 seconds
[0m11:22:50.598957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac84621100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac82d137d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x74ac82c40fe0>]}
[0m11:22:50.599569 [debug] [MainThread]: Flushing usage events
[0m11:22:51.892270 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:23:17.134863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715e09f9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7160309d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715e09dc10>]}


============================== 11:23:17.138169 | 12361ce6-9303-4d3f-9222-6668f5c628ef ==============================
[0m11:23:17.138169 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:23:17.139108 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run --models thrift_test', 'send_anonymous_usage_stats': 'True'}
[0m11:23:17.224652 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:23:17.225862 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:23:17.226816 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:23:17.383479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '12361ce6-9303-4d3f-9222-6668f5c628ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715e09dc10>]}
[0m11:23:17.447323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '12361ce6-9303-4d3f-9222-6668f5c628ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715cc007a0>]}
[0m11:23:17.448196 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:23:17.550520 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:23:17.650587 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:23:17.651369 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:23:17.909787 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m11:23:17.925184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '12361ce6-9303-4d3f-9222-6668f5c628ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715b75a3f0>]}
[0m11:23:18.017924 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:23:18.020473 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:23:18.031578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '12361ce6-9303-4d3f-9222-6668f5c628ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715cac4b30>]}
[0m11:23:18.032229 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m11:23:18.032700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '12361ce6-9303-4d3f-9222-6668f5c628ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715d450230>]}
[0m11:23:18.034272 [info ] [MainThread]: 
[0m11:23:18.034724 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:23:18.035317 [info ] [MainThread]: 
[0m11:23:18.035915 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m11:23:18.036906 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m11:23:18.051675 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m11:23:18.052738 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m11:23:18.053639 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:23:18.112131 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:23:18.112850 [debug] [ThreadPool]: SQL status: OK in 0.059 seconds
[0m11:23:18.116258 [debug] [ThreadPool]: On list_schemas: Close
[0m11:23:18.126853 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default_default)
[0m11:23:18.132891 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:23:18.133472 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m11:23:18.133960 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m11:23:18.134351 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:23:18.195991 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:23:18.196530 [debug] [ThreadPool]: SQL status: OK in 0.062 seconds
[0m11:23:18.199976 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m11:23:18.200610 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:23:18.201097 [debug] [ThreadPool]: On list_None_default_default: Close
[0m11:23:18.205351 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m11:23:18.208965 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:23:18.209701 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m11:23:18.210374 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m11:23:18.211030 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:23:18.285748 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m11:23:18.286328 [debug] [ThreadPool]: SQL status: OK in 0.075 seconds
[0m11:23:18.290080 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m11:23:18.290656 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m11:23:18.291148 [debug] [ThreadPool]: On list_None_default: Close
[0m11:23:18.297034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '12361ce6-9303-4d3f-9222-6668f5c628ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715c8fd4f0>]}
[0m11:23:18.297802 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:23:18.298299 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:23:18.300819 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.thrift_test
[0m11:23:18.301593 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test ........................ [RUN]
[0m11:23:18.302263 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.thrift_test)
[0m11:23:18.302834 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.thrift_test
[0m11:23:18.310905 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.thrift_test"
[0m11:23:18.311771 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.thrift_test
[0m11:23:18.378135 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:23:18.378758 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:23:18.379335 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

  
    create or replace temporary view test__dbt_tmp as
      -- models/thrift_test.sql


SELECT 
    id,
    name,
    doubled_value
FROM 
    test_staging
  
[0m11:23:18.380011 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:23:18.470075 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:23:18.471102 [debug] [Thread-1 (]: SQL status: OK in 0.091 seconds
[0m11:23:18.496283 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:23:18.497606 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

      describe extended default_default.test
  
[0m11:23:18.553720 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m11:23:18.554707 [debug] [Thread-1 (]: SQL status: OK in 0.056 seconds
[0m11:23:18.558813 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.thrift_test"
[0m11:23:18.559819 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.thrift_test"
[0m11:23:18.560392 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:23:18.611238 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42703', errorCode=0, errorMessage="org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false\n+- 'Project [ID#1135, NAME#1136, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])\n         +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]\n            +- Project [id#1117, name#1118, doubled_value#1119]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;\n'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false\n+- 'Project [ID#1135, NAME#1136, 'VALUE]\n   +- SubqueryAlias test__dbt_tmp\n      +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])\n         +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]\n            +- Project [id#1117, name#1118, doubled_value#1119]\n               +- SubqueryAlias spark_catalog.default.test_staging\n                  +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n", taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m11:23:18.612842 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m11:23:18.613971 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.thrift_test"} */

    insert into table default_default.test
    select `ID`, `NAME`, `VALUE` from test__dbt_tmp


[0m11:23:18.615361 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
  +- 'Project [ID#1135, NAME#1136, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
           +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
              +- Project [id#1117, name#1118, doubled_value#1119]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
  'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
  +- 'Project [ID#1135, NAME#1136, 'VALUE]
     +- SubqueryAlias test__dbt_tmp
        +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
           +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
              +- Project [id#1117, name#1118, doubled_value#1119]
                 +- SubqueryAlias spark_catalog.default.test_staging
                    +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
  
  	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
  	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
  	at scala.collection.Iterator.foreach(Iterator.scala:943)
  	at scala.collection.Iterator.foreach$(Iterator.scala:943)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
  	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
  	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m11:23:18.616868 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: ROLLBACK
[0m11:23:18.617576 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m11:23:18.618295 [debug] [Thread-1 (]: On model.dbt_spark_project.thrift_test: Close
[0m11:23:18.675998 [debug] [Thread-1 (]: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
    +- 'Project [ID#1135, NAME#1136, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
             +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
                +- Project [id#1117, name#1118, doubled_value#1119]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
    +- 'Project [ID#1135, NAME#1136, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
             +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
                +- Project [id#1117, name#1118, doubled_value#1119]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:23:18.679970 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '12361ce6-9303-4d3f-9222-6668f5c628ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715b63cdd0>]}
[0m11:23:18.680978 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test ............... [[31mERROR[0m in 0.38s]
[0m11:23:18.682198 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.thrift_test
[0m11:23:18.683421 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.thrift_test' to be skipped because of status 'error'.  Reason: Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
    +- 'Project [ID#1135, NAME#1136, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
             +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
                +- Project [id#1117, name#1118, doubled_value#1119]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
    +- 'Project [ID#1135, NAME#1136, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
             +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
                +- Project [id#1117, name#1118, doubled_value#1119]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m11:23:18.685838 [debug] [MainThread]: On master: ROLLBACK
[0m11:23:18.686380 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:23:18.728270 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:23:18.729260 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:23:18.730213 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:23:18.731100 [debug] [MainThread]: On master: ROLLBACK
[0m11:23:18.732032 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m11:23:18.732880 [debug] [MainThread]: On master: Close
[0m11:23:18.738094 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:23:18.739034 [debug] [MainThread]: Connection 'model.dbt_spark_project.thrift_test' was properly closed.
[0m11:23:18.739942 [info ] [MainThread]: 
[0m11:23:18.740898 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.70 seconds (0.70s).
[0m11:23:18.742791 [debug] [MainThread]: Command end result
[0m11:23:18.774922 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m11:23:18.778673 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m11:23:18.791036 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m11:23:18.791606 [info ] [MainThread]: 
[0m11:23:18.792266 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m11:23:18.792874 [info ] [MainThread]: 
[0m11:23:18.793824 [error] [MainThread]:   Runtime Error in model thrift_test (models/thrift_test.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
    +- 'Project [ID#1135, NAME#1136, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
             +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
                +- Project [id#1117, name#1118, doubled_value#1119]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `VALUE` cannot be resolved. Did you mean one of the following? [`id`, `name`, `doubled_value`].; line 4 pos 25;
    'InsertIntoStatement HiveTableRelation [`spark_catalog`.`default_default`.`test`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [ID#1138, NAME#1139, VALUE#1140], Partition Cols: []], false, false, false
    +- 'Project [ID#1135, NAME#1136, 'VALUE]
       +- SubqueryAlias test__dbt_tmp
          +- View (`test__dbt_tmp`, [id#1135,name#1136,doubled_value#1137])
             +- Project [cast(id#1117 as int) AS id#1135, cast(name#1118 as string) AS name#1136, cast(doubled_value#1119 as int) AS doubled_value#1137]
                +- Project [id#1117, name#1118, doubled_value#1119]
                   +- SubqueryAlias spark_catalog.default.test_staging
                      +- Relation spark_catalog.default.test_staging[id#1117,name#1118,doubled_value#1119] parquet
    
    	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
    	at scala.collection.immutable.Stream.foreach(Stream.scala:533)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
    	at scala.collection.Iterator.foreach(Iterator.scala:943)
    	at scala.collection.Iterator.foreach$(Iterator.scala:943)
    	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
    	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
    	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
    	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
    	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
    	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m11:23:18.794777 [info ] [MainThread]: 
[0m11:23:18.795318 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:23:18.796514 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.7178814, "process_in_blocks": "0", "process_kernel_time": 0.2159, "process_mem_max_rss": "113160", "process_out_blocks": "3016", "process_user_time": 2.494852}
[0m11:23:18.797212 [debug] [MainThread]: Command `dbt run` failed at 11:23:18.797065 after 1.72 seconds
[0m11:23:18.797988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71610fbec0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715ca1d4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f715d96e420>]}
[0m11:23:18.798618 [debug] [MainThread]: Flushing usage events
[0m11:23:20.157096 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:25:10.166198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301cae02de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301cb1c4ce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301cb1c4dd0>]}


============================== 11:25:10.169115 | 9f103624-d144-4b70-beec-5ea2a3a4c573 ==============================
[0m11:25:10.169115 [info ] [MainThread]: Running with dbt=1.9.3
[0m11:25:10.169763 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models thrift_test', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:25:10.258923 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:25:10.259490 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:25:10.260049 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:25:10.408689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9f103624-d144-4b70-beec-5ea2a3a4c573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301cc310890>]}
[0m11:25:10.470426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9f103624-d144-4b70-beec-5ea2a3a4c573', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301cb16c3b0>]}
[0m11:25:10.471225 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m11:25:10.568614 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m11:25:10.663658 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:25:10.664419 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/thrift_test.sql
[0m11:25:10.855292 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.dbt_spark_project.thrift_test' (models/thrift_test.sql) depends on a node named 'test_staging' which was not found
[0m11:25:10.856542 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.7460175, "process_in_blocks": "0", "process_kernel_time": 0.183807, "process_mem_max_rss": "107600", "process_out_blocks": "16", "process_user_time": 2.025882}
[0m11:25:10.857132 [debug] [MainThread]: Command `dbt run` failed at 11:25:10.857019 after 0.75 seconds
[0m11:25:10.857597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301cae02de0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301c9ca0350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7301c9ca0290>]}
[0m11:25:10.858084 [debug] [MainThread]: Flushing usage events
[0m11:25:12.187897 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:49:21.500201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e85548db710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8554aa7800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8554aa7fe0>]}


============================== 14:49:21.504382 | f1f838c5-2c49-4f2b-a34c-30df63df326e ==============================
[0m14:49:21.504382 [info ] [MainThread]: Running with dbt=1.9.3
[0m14:49:21.505077 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m14:49:21.608338 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:49:21.609038 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:49:21.609538 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:49:21.751441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8553e5fc50>]}
[0m14:49:21.810227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8553a5ed80>]}
[0m14:49:21.811001 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m14:49:21.919790 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m14:49:22.050798 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
[0m14:49:22.051368 [debug] [MainThread]: Partial parsing: deleted file: dbt_spark_project://models/thrift_test.sql
[0m14:49:22.112583 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m14:49:22.129221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e85532061e0>]}
[0m14:49:22.218082 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:49:22.221481 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:49:22.243448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e85532590d0>]}
[0m14:49:22.244098 [info ] [MainThread]: Found 5 models, 1 source, 473 macros
[0m14:49:22.244589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8553210890>]}
[0m14:49:22.247069 [info ] [MainThread]: 
[0m14:49:22.247600 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:49:22.248151 [info ] [MainThread]: 
[0m14:49:22.248800 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m14:49:22.257382 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m14:49:22.282761 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m14:49:22.283338 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m14:49:22.283846 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:49:22.425611 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:49:22.426251 [debug] [ThreadPool]: SQL status: OK in 0.142 seconds
[0m14:49:22.429934 [debug] [ThreadPool]: On list_schemas: Close
[0m14:49:22.437232 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m14:49:22.443144 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:22.443656 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m14:49:22.444108 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m14:49:22.444564 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:49:22.592711 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m14:49:22.593301 [debug] [ThreadPool]: SQL status: OK in 0.149 seconds
[0m14:49:22.596846 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m14:49:22.597390 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m14:49:22.597846 [debug] [ThreadPool]: On list_None_default: Close
[0m14:49:22.604985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8553a5f080>]}
[0m14:49:22.605806 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:22.606265 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:22.609308 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m14:49:22.609983 [info ] [Thread-1 (]: 1 of 5 START sql table model default.transformed_data .......................... [RUN]
[0m14:49:22.610577 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.transformed_data)
[0m14:49:22.611088 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m14:49:22.619317 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m14:49:22.620909 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m14:49:22.660569 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:49:22.661159 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m14:49:22.661681 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:22.735447 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:22.736138 [debug] [Thread-1 (]: SQL status: OK in 0.074 seconds
[0m14:49:22.781448 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m14:49:22.782462 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:22.783014 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m14:49:22.783530 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m14:49:23.874220 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:23.874827 [debug] [Thread-1 (]: SQL status: OK in 1.091 seconds
[0m14:49:23.895800 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m14:49:23.896367 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:23.896860 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m14:49:23.904360 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e855295acc0>]}
[0m14:49:23.905396 [info ] [Thread-1 (]: 1 of 5 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.29s]
[0m14:49:23.906290 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m14:49:23.906851 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m14:49:23.907373 [info ] [Thread-1 (]: 2 of 5 START sql table model default.transformed_data2 ......................... [RUN]
[0m14:49:23.908060 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m14:49:23.908492 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m14:49:23.910834 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m14:49:23.911872 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m14:49:23.918152 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:49:23.918975 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m14:49:23.919604 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:24.007086 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:24.007854 [debug] [Thread-1 (]: SQL status: OK in 0.088 seconds
[0m14:49:24.012458 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m14:49:24.013504 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:24.014166 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m14:49:24.014804 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m14:49:24.382525 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:24.383271 [debug] [Thread-1 (]: SQL status: OK in 0.368 seconds
[0m14:49:24.386363 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m14:49:24.387196 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:24.387944 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m14:49:24.395482 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e855298d760>]}
[0m14:49:24.396376 [info ] [Thread-1 (]: 2 of 5 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.49s]
[0m14:49:24.397319 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m14:49:24.397950 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m14:49:24.398603 [info ] [Thread-1 (]: 3 of 5 START sql table model default.transformed_data3 ......................... [RUN]
[0m14:49:24.399430 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m14:49:24.400067 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m14:49:24.403581 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m14:49:24.404898 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m14:49:24.412224 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:49:24.413380 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m14:49:24.414436 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:24.494551 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:24.495191 [debug] [Thread-1 (]: SQL status: OK in 0.081 seconds
[0m14:49:24.498827 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m14:49:24.499674 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:24.500205 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m14:49:24.500707 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m14:49:24.890517 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:24.891158 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m14:49:24.893639 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m14:49:24.894171 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:24.894690 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m14:49:24.900759 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e855291c980>]}
[0m14:49:24.901579 [info ] [Thread-1 (]: 3 of 5 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.50s]
[0m14:49:24.902316 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m14:49:24.902852 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m14:49:24.903414 [info ] [Thread-1 (]: 4 of 5 START sql table model default.transformed_data4 ......................... [RUN]
[0m14:49:24.904005 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m14:49:24.904573 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m14:49:24.907246 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m14:49:24.908164 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m14:49:24.912739 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:49:24.913298 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m14:49:24.913790 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:24.991603 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:24.992304 [debug] [Thread-1 (]: SQL status: OK in 0.078 seconds
[0m14:49:24.997915 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m14:49:24.999336 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:25.000279 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m14:49:25.001207 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m14:49:25.362649 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:25.363369 [debug] [Thread-1 (]: SQL status: OK in 0.361 seconds
[0m14:49:25.366111 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m14:49:25.366778 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:25.367375 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m14:49:25.373639 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e855291af60>]}
[0m14:49:25.374507 [info ] [Thread-1 (]: 4 of 5 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.47s]
[0m14:49:25.375305 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m14:49:25.375874 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m14:49:25.376494 [info ] [Thread-1 (]: 5 of 5 START sql table model default.transformed_data5 ......................... [RUN]
[0m14:49:25.377176 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m14:49:25.377682 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m14:49:25.380214 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m14:49:25.381936 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m14:49:25.389982 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:49:25.391027 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m14:49:25.391918 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:49:25.470369 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:25.471150 [debug] [Thread-1 (]: SQL status: OK in 0.079 seconds
[0m14:49:25.474698 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m14:49:25.475440 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:25.475956 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m14:49:25.476472 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m14:49:25.854283 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m14:49:25.854866 [debug] [Thread-1 (]: SQL status: OK in 0.378 seconds
[0m14:49:25.857102 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m14:49:25.857575 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m14:49:25.858044 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m14:49:25.863169 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f838c5-2c49-4f2b-a34c-30df63df326e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e85529cbef0>]}
[0m14:49:25.864043 [info ] [Thread-1 (]: 5 of 5 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.49s]
[0m14:49:25.864775 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m14:49:25.866121 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:25.866610 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:49:25.916701 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:49:25.917478 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:49:25.918099 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:49:25.918952 [debug] [MainThread]: On master: ROLLBACK
[0m14:49:25.919868 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m14:49:25.921084 [debug] [MainThread]: On master: Close
[0m14:49:25.930747 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:49:25.931345 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m14:49:25.932219 [info ] [MainThread]: 
[0m14:49:25.932981 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 3.68 seconds (3.68s).
[0m14:49:25.934724 [debug] [MainThread]: Command end result
[0m14:49:25.980230 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m14:49:25.982125 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m14:49:25.988928 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m14:49:25.989433 [info ] [MainThread]: 
[0m14:49:25.990035 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:49:25.990541 [info ] [MainThread]: 
[0m14:49:25.991126 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m14:49:25.992804 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.5478506, "process_in_blocks": "48080", "process_kernel_time": 0.27357, "process_mem_max_rss": "109348", "process_out_blocks": "2992", "process_user_time": 2.342323}
[0m14:49:25.993470 [debug] [MainThread]: Command `dbt run` succeeded at 14:49:25.993335 after 4.55 seconds
[0m14:49:25.994138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8553ef2210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8553e17350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7e8553e17500>]}
[0m14:49:25.994722 [debug] [MainThread]: Flushing usage events
[0m14:49:27.555418 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:15:07.012877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75699637c0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7569973b84d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756997865070>]}


============================== 16:15:07.023764 | 25750433-d7d9-4c21-ae37-a33c052e0351 ==============================
[0m16:15:07.023764 [info ] [MainThread]: Running with dbt=1.9.3
[0m16:15:07.025187 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models test2', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:15:07.145543 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:15:07.146077 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:15:07.146504 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:15:07.317614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '25750433-d7d9-4c21-ae37-a33c052e0351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7569959b1fa0>]}
[0m16:15:07.376023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '25750433-d7d9-4c21-ae37-a33c052e0351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7569966446e0>]}
[0m16:15:07.376774 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m16:15:07.509831 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m16:15:07.645309 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m16:15:07.645934 [debug] [MainThread]: Partial parsing: added file: dbt_spark_project://models/test2.sql
[0m16:15:07.888641 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m16:15:07.912084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '25750433-d7d9-4c21-ae37-a33c052e0351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756994eb24b0>]}
[0m16:15:07.996295 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:15:07.999774 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:15:08.032935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '25750433-d7d9-4c21-ae37-a33c052e0351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7569950a4e00>]}
[0m16:15:08.033554 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m16:15:08.034177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25750433-d7d9-4c21-ae37-a33c052e0351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756994f834d0>]}
[0m16:15:08.036185 [info ] [MainThread]: 
[0m16:15:08.036854 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:15:08.037432 [info ] [MainThread]: 
[0m16:15:08.038435 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m16:15:08.040402 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m16:15:08.061562 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m16:15:08.062540 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:15:08.063411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:15:08.247525 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:15:08.248347 [debug] [ThreadPool]: SQL status: OK in 0.185 seconds
[0m16:15:08.252492 [debug] [ThreadPool]: On list_schemas: Close
[0m16:15:08.290500 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__default_default)
[0m16:15:08.291586 [debug] [ThreadPool]: Creating schema "schema: "default_default"
"
[0m16:15:08.298166 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:08.298822 [debug] [ThreadPool]: Using spark connection "create__default_default"
[0m16:15:08.299473 [debug] [ThreadPool]: On create__default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "create__default_default"} */
create schema if not exists default_default
  
[0m16:15:08.300111 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:15:08.516825 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:15:08.517676 [debug] [ThreadPool]: SQL status: OK in 0.218 seconds
[0m16:15:08.518879 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m16:15:08.519343 [debug] [ThreadPool]: On create__default_default: ROLLBACK
[0m16:15:08.519799 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:15:08.520208 [debug] [ThreadPool]: On create__default_default: Close
[0m16:15:08.534890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create__default_default, now list_None_default_default)
[0m16:15:08.540146 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:08.540601 [debug] [ThreadPool]: Using spark connection "list_None_default_default"
[0m16:15:08.541016 [debug] [ThreadPool]: On list_None_default_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default_default"} */
show table extended in default_default like '*'
  
[0m16:15:08.541367 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:15:08.625441 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:15:08.625994 [debug] [ThreadPool]: SQL status: OK in 0.085 seconds
[0m16:15:08.629161 [debug] [ThreadPool]: On list_None_default_default: ROLLBACK
[0m16:15:08.629686 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:15:08.630097 [debug] [ThreadPool]: On list_None_default_default: Close
[0m16:15:08.636956 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_default_default, now list_None_default)
[0m16:15:08.639147 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:08.639614 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m16:15:08.640036 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m16:15:08.640447 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:15:08.807767 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:15:08.808332 [debug] [ThreadPool]: SQL status: OK in 0.168 seconds
[0m16:15:08.812580 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m16:15:08.813116 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:15:08.813534 [debug] [ThreadPool]: On list_None_default: Close
[0m16:15:08.821049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25750433-d7d9-4c21-ae37-a33c052e0351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x756995024ec0>]}
[0m16:15:08.821747 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:08.822236 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:15:08.824923 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m16:15:08.825617 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default_default.test2 ....................... [RUN]
[0m16:15:08.826225 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m16:15:08.826823 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m16:15:08.837510 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m16:15:08.838902 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m16:15:08.931833 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test2"
[0m16:15:08.936252 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:08.936807 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:15:08.937258 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    
        create table default_default.test2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id,
    name,
    value * 3 AS tripled_value
FROM 

  
[0m16:15:08.937700 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:15:08.989855 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */\n\n  \n    \n        create table default_default.test2\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\nSELECT \n    id,\n    name,\n    value * 3 AS tripled_value\nFROM \n\n  \n--^^^\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */\n\n  \n    \n        create table default_default.test2\n      \n      \n      \n      \n      \n      \n      \n      \n\n      as\n      \n\nSELECT \n    id,\n    name,\n    value * 3 AS tripled_value\nFROM \n\n  \n--^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m16:15:08.991127 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m16:15:08.991770 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    
        create table default_default.test2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id,
    name,
    value * 3 AS tripled_value
FROM 

  
[0m16:15:08.992505 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
  
    
      
          create table default_default.test2
        
        
        
        
        
        
        
        
  
        as
        
  
  SELECT 
      id,
      name,
      value * 3 AS tripled_value
  FROM 
  
    
  --^^^
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
  
    
      
          create table default_default.test2
        
        
        
        
        
        
        
        
  
        as
        
  
  SELECT 
      id,
      name,
      value * 3 AS tripled_value
  FROM 
  
    
  --^^^
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m16:15:08.993386 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m16:15:08.993895 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m16:15:08.994350 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m16:15:09.007645 [debug] [Thread-1 (]: Runtime Error in model test2 (models/test2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        
            create table default_default.test2
          
          
          
          
          
          
          
          
    
          as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
      
    --^^^
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        
            create table default_default.test2
          
          
          
          
          
          
          
          
    
          as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
      
    --^^^
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m16:15:09.009574 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25750433-d7d9-4c21-ae37-a33c052e0351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75699666e3f0>]}
[0m16:15:09.010356 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default_default.test2 .............. [[31mERROR[0m in 0.18s]
[0m16:15:09.011265 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m16:15:09.012199 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.test2' to be skipped because of status 'error'.  Reason: Runtime Error in model test2 (models/test2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        
            create table default_default.test2
          
          
          
          
          
          
          
          
    
          as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
      
    --^^^
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        
            create table default_default.test2
          
          
          
          
          
          
          
          
    
          as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
      
    --^^^
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m16:15:09.014336 [debug] [MainThread]: On master: ROLLBACK
[0m16:15:09.014835 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:15:09.065183 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:15:09.065892 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:09.066373 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:15:09.066836 [debug] [MainThread]: On master: ROLLBACK
[0m16:15:09.067335 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:15:09.067900 [debug] [MainThread]: On master: Close
[0m16:15:09.073500 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:15:09.073999 [debug] [MainThread]: Connection 'model.dbt_spark_project.test2' was properly closed.
[0m16:15:09.074457 [info ] [MainThread]: 
[0m16:15:09.074998 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.04 seconds (1.04s).
[0m16:15:09.076114 [debug] [MainThread]: Command end result
[0m16:15:09.116840 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:15:09.120420 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:15:09.133714 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m16:15:09.134506 [info ] [MainThread]: 
[0m16:15:09.135433 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:15:09.136334 [info ] [MainThread]: 
[0m16:15:09.137750 [error] [MainThread]:   Runtime Error in model test2 (models/test2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        
            create table default_default.test2
          
          
          
          
          
          
          
          
    
          as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
      
    --^^^
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 24, pos 2)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        
            create table default_default.test2
          
          
          
          
          
          
          
          
    
          as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
      
    --^^^
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m16:15:09.139137 [info ] [MainThread]: 
[0m16:15:09.140144 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:15:09.142629 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 2.1859808, "process_in_blocks": "68032", "process_kernel_time": 0.243024, "process_mem_max_rss": "112032", "process_out_blocks": "2968", "process_user_time": 2.436275}
[0m16:15:09.143835 [debug] [MainThread]: Command `dbt run` failed at 16:15:09.143603 after 2.19 seconds
[0m16:15:09.144872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x75699529c230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7569951d4500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7569954b7e00>]}
[0m16:15:09.145865 [debug] [MainThread]: Flushing usage events
[0m16:15:10.460166 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:15:52.787850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b43d1ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b5725ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b43d36b0>]}


============================== 16:15:52.791058 | 53709887-f65c-46be-a2d1-9ad197c980aa ==============================
[0m16:15:52.791058 [info ] [MainThread]: Running with dbt=1.9.3
[0m16:15:52.791829 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'profiles_dir': '/home/minh/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --models test2', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:15:52.884714 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:15:52.885291 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:15:52.885754 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:15:53.026574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '53709887-f65c-46be-a2d1-9ad197c980aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b367dee0>]}
[0m16:15:53.086638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '53709887-f65c-46be-a2d1-9ad197c980aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b37d6120>]}
[0m16:15:53.087358 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m16:15:53.184001 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m16:15:53.275859 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:15:53.276556 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/test2.sql
[0m16:15:53.520066 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m16:15:53.540568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53709887-f65c-46be-a2d1-9ad197c980aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b32d0e30>]}
[0m16:15:53.618063 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:15:53.620578 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:15:53.632532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53709887-f65c-46be-a2d1-9ad197c980aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b33502f0>]}
[0m16:15:53.633223 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m16:15:53.633709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53709887-f65c-46be-a2d1-9ad197c980aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b3185bb0>]}
[0m16:15:53.635319 [info ] [MainThread]: 
[0m16:15:53.635804 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:15:53.636264 [info ] [MainThread]: 
[0m16:15:53.636899 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m16:15:53.637817 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m16:15:53.654878 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m16:15:53.655905 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:15:53.656915 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:15:53.795470 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:15:53.796099 [debug] [ThreadPool]: SQL status: OK in 0.139 seconds
[0m16:15:53.800274 [debug] [ThreadPool]: On list_schemas: Close
[0m16:15:53.813776 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m16:15:53.819368 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:53.819858 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m16:15:53.820286 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m16:15:53.820700 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:15:53.962501 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:15:53.963214 [debug] [ThreadPool]: SQL status: OK in 0.142 seconds
[0m16:15:53.967794 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m16:15:53.968386 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:15:53.968873 [debug] [ThreadPool]: On list_None_default: Close
[0m16:15:53.977233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53709887-f65c-46be-a2d1-9ad197c980aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b36deb70>]}
[0m16:15:53.977976 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:53.978498 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:15:53.981711 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m16:15:53.982504 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default.test2 ............................... [RUN]
[0m16:15:53.983218 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m16:15:53.983812 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m16:15:53.998139 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m16:15:53.999664 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m16:15:54.059963 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:54.060530 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:15:54.061071 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 3 AS tripled_value
FROM 

WHERE id > (SELECT MAX(id) FROM default.test2)

  
[0m16:15:54.061517 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:15:54.108208 [debug] [Thread-1 (]: Spark adapter: Poll response: TGetOperationStatusResp(status=TStatus(statusCode=0, infoMessages=None, sqlState=None, errorCode=None, errorMessage=None), operationState=5, sqlState='42601', errorCode=0, errorMessage='org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'>\'.(line 13, pos 9)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */\n\n  \n    create or replace temporary view test2__dbt_tmp as\n      \n\nSELECT \n    id,\n    name,\n    value * 3 AS tripled_value\nFROM \n\nWHERE id > (SELECT MAX(id) FROM default.test2)\n---------^^^\n\n  \n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:714)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:525)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near \'>\'.(line 13, pos 9)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */\n\n  \n    create or replace temporary view test2__dbt_tmp as\n      \n\nSELECT \n    id,\n    name,\n    value * 3 AS tripled_value\nFROM \n\nWHERE id > (SELECT MAX(id) FROM default.test2)\n---------^^^\n\n  \n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)\n\t... 16 more\n', taskStatus=None, operationStarted=None, operationCompleted=None, hasResultSet=None, progressUpdateResponse=None)
[0m16:15:54.109771 [debug] [Thread-1 (]: Spark adapter: Poll status: 5
[0m16:15:54.111043 [debug] [Thread-1 (]: Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 3 AS tripled_value
FROM 

WHERE id > (SELECT MAX(id) FROM default.test2)

  
[0m16:15:54.112510 [debug] [Thread-1 (]: Spark adapter: Database Error
  org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
  
    
      create or replace temporary view test2__dbt_tmp as
        
  
  SELECT 
      id,
      name,
      value * 3 AS tripled_value
  FROM 
  
  WHERE id > (SELECT MAX(id) FROM default.test2)
  ---------^^^
  
    
  
  	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
  	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
  	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
  	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
  	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
  	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
  	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
  	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
  	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
  	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
  	at java.base/java.lang.Thread.run(Thread.java:1583)
  Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
  [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
  
    
      create or replace temporary view test2__dbt_tmp as
        
  
  SELECT 
      id,
      name,
      value * 3 AS tripled_value
  FROM 
  
  WHERE id > (SELECT MAX(id) FROM default.test2)
  ---------^^^
  
    
  
  	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
  	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
  	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
  	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
  	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
  	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
  	... 16 more
  
[0m16:15:54.114241 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m16:15:54.115265 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m16:15:54.116232 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m16:15:54.133617 [debug] [Thread-1 (]: Runtime Error in model test2 (models/test2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        create or replace temporary view test2__dbt_tmp as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
    WHERE id > (SELECT MAX(id) FROM default.test2)
    ---------^^^
    
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        create or replace temporary view test2__dbt_tmp as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
    WHERE id > (SELECT MAX(id) FROM default.test2)
    ---------^^^
    
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m16:15:54.137514 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '53709887-f65c-46be-a2d1-9ad197c980aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b4ade510>]}
[0m16:15:54.139019 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model default.test2 ...................... [[31mERROR[0m in 0.15s]
[0m16:15:54.140326 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m16:15:54.141508 [debug] [Thread-4 (]: Marking all children of 'model.dbt_spark_project.test2' to be skipped because of status 'error'.  Reason: Runtime Error in model test2 (models/test2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        create or replace temporary view test2__dbt_tmp as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
    WHERE id > (SELECT MAX(id) FROM default.test2)
    ---------^^^
    
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        create or replace temporary view test2__dbt_tmp as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
    WHERE id > (SELECT MAX(id) FROM default.test2)
    ---------^^^
    
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    .
[0m16:15:54.143900 [debug] [MainThread]: On master: ROLLBACK
[0m16:15:54.144714 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:15:54.207842 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:15:54.208848 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:15:54.209675 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:15:54.210517 [debug] [MainThread]: On master: ROLLBACK
[0m16:15:54.211361 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:15:54.212179 [debug] [MainThread]: On master: Close
[0m16:15:54.219081 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:15:54.220058 [debug] [MainThread]: Connection 'model.dbt_spark_project.test2' was properly closed.
[0m16:15:54.220848 [info ] [MainThread]: 
[0m16:15:54.221634 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 0.58 seconds (0.58s).
[0m16:15:54.223301 [debug] [MainThread]: Command end result
[0m16:15:54.261893 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:15:54.263685 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:15:54.269517 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m16:15:54.270018 [info ] [MainThread]: 
[0m16:15:54.270585 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:15:54.271112 [info ] [MainThread]: 
[0m16:15:54.271925 [error] [MainThread]:   Runtime Error in model test2 (models/test2.sql)
  Database Error
    org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        create or replace temporary view test2__dbt_tmp as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
    WHERE id > (SELECT MAX(id) FROM default.test2)
    ---------^^^
    
      
    
    	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:43)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:263)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:167)
    	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
    	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:167)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:162)
    	at java.base/java.security.AccessController.doPrivileged(AccessController.java:714)
    	at java.base/javax.security.auth.Subject.doAs(Subject.java:525)
    	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:176)
    	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
    	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
    	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    	at java.base/java.lang.Thread.run(Thread.java:1583)
    Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
    [PARSE_SYNTAX_ERROR] Syntax error at or near '>'.(line 13, pos 9)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */
    
      
        create or replace temporary view test2__dbt_tmp as
          
    
    SELECT 
        id,
        name,
        value * 3 AS tripled_value
    FROM 
    
    WHERE id > (SELECT MAX(id) FROM default.test2)
    ---------^^^
    
      
    
    	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)
    	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)
    	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:54)
    	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:684)
    	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
    	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:683)
    	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
    	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
    	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
    	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:228)
    	... 16 more
    
[0m16:15:54.272627 [info ] [MainThread]: 
[0m16:15:54.273082 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:15:54.274008 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.5407579, "process_in_blocks": "408", "process_kernel_time": 0.189603, "process_mem_max_rss": "112844", "process_out_blocks": "2960", "process_user_time": 2.291213}
[0m16:15:54.274552 [debug] [MainThread]: Command `dbt run` failed at 16:15:54.274447 after 1.54 seconds
[0m16:15:54.275007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b31d29f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b3187410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7d26b34c9820>]}
[0m16:15:54.275445 [debug] [MainThread]: Flushing usage events
[0m16:15:55.518485 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:17:30.493773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b1a57f20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b2eecce0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b3488d70>]}


============================== 16:17:30.496893 | fef32613-1020-40dd-a6dd-793f856ff6ff ==============================
[0m16:17:30.496893 [info ] [MainThread]: Running with dbt=1.9.3
[0m16:17:30.497945 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --models test2', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:17:30.588267 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:17:30.588957 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:17:30.589547 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:17:30.730430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fef32613-1020-40dd-a6dd-793f856ff6ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b1541f40>]}
[0m16:17:30.789179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fef32613-1020-40dd-a6dd-793f856ff6ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b11897c0>]}
[0m16:17:30.789943 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m16:17:30.884407 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m16:17:30.976193 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:17:30.976880 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/test2.sql
[0m16:17:31.218695 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m16:17:31.241131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fef32613-1020-40dd-a6dd-793f856ff6ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b084a900>]}
[0m16:17:31.319722 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:17:31.323128 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:17:31.334642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fef32613-1020-40dd-a6dd-793f856ff6ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b08c09b0>]}
[0m16:17:31.335194 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m16:17:31.335641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fef32613-1020-40dd-a6dd-793f856ff6ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b0875a90>]}
[0m16:17:31.337109 [info ] [MainThread]: 
[0m16:17:31.337535 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:17:31.337998 [info ] [MainThread]: 
[0m16:17:31.338634 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m16:17:31.339680 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m16:17:31.359491 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m16:17:31.360411 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:17:31.361171 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:17:31.439734 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:17:31.440283 [debug] [ThreadPool]: SQL status: OK in 0.079 seconds
[0m16:17:31.443874 [debug] [ThreadPool]: On list_schemas: Close
[0m16:17:31.457885 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m16:17:31.462906 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:17:31.463434 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m16:17:31.463856 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m16:17:31.464245 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:17:31.632667 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:17:31.633398 [debug] [ThreadPool]: SQL status: OK in 0.169 seconds
[0m16:17:31.648224 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m16:17:31.648884 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:17:31.649445 [debug] [ThreadPool]: On list_None_default: Close
[0m16:17:31.660588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fef32613-1020-40dd-a6dd-793f856ff6ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b0875a90>]}
[0m16:17:31.661502 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:17:31.662098 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:17:31.664766 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m16:17:31.665688 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default.test2 ............................... [RUN]
[0m16:17:31.666624 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m16:17:31.667327 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m16:17:31.682337 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m16:17:31.684329 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m16:17:31.738462 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m16:17:31.739094 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:17:31.739613 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 3 AS tripled_value
FROM raw_data 

WHERE id > (SELECT MAX(id) FROM default.test2)

  
[0m16:17:31.740119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:17:32.154781 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:17:32.155616 [debug] [Thread-1 (]: SQL status: OK in 0.415 seconds
[0m16:17:32.176302 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:17:32.177037 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

      describe extended default.test2
  
[0m16:17:32.299888 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:17:32.300718 [debug] [Thread-1 (]: SQL status: OK in 0.123 seconds
[0m16:17:32.306974 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test2"
[0m16:17:32.308068 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:17:32.308786 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

    insert into table default.test2
    select `ID`, `NAME`, `TRIPLED_VALUE` from test2__dbt_tmp


[0m16:17:34.228003 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:17:34.228803 [debug] [Thread-1 (]: SQL status: OK in 1.919 seconds
[0m16:17:34.248801 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m16:17:34.249341 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m16:17:34.249818 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m16:17:34.351781 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fef32613-1020-40dd-a6dd-793f856ff6ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b0877d10>]}
[0m16:17:34.353157 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model default.test2 .......................... [[32mOK[0m in 2.68s]
[0m16:17:34.354602 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m16:17:34.356475 [debug] [MainThread]: On master: ROLLBACK
[0m16:17:34.357056 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:17:34.406739 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:17:34.407758 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:17:34.408670 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:17:34.409572 [debug] [MainThread]: On master: ROLLBACK
[0m16:17:34.411452 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:17:34.412429 [debug] [MainThread]: On master: Close
[0m16:17:34.419536 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:17:34.420068 [debug] [MainThread]: Connection 'model.dbt_spark_project.test2' was properly closed.
[0m16:17:34.420522 [info ] [MainThread]: 
[0m16:17:34.421041 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.08 seconds (3.08s).
[0m16:17:34.421977 [debug] [MainThread]: Command end result
[0m16:17:34.455164 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:17:34.457262 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:17:34.463463 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m16:17:34.463945 [info ] [MainThread]: 
[0m16:17:34.464451 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:17:34.464916 [info ] [MainThread]: 
[0m16:17:34.465436 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:17:34.466370 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.0247355, "process_in_blocks": "0", "process_kernel_time": 0.185481, "process_mem_max_rss": "112624", "process_out_blocks": "2912", "process_user_time": 2.304554}
[0m16:17:34.466911 [debug] [MainThread]: Command `dbt run` succeeded at 16:17:34.466802 after 4.03 seconds
[0m16:17:34.467366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b4fb3ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b09a0830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x78a5b09a3bf0>]}
[0m16:17:34.467857 [debug] [MainThread]: Flushing usage events
[0m16:17:36.075157 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:21:18.831679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0e2c3320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0dfe8b30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0fd187d0>]}


============================== 16:21:18.834755 | fafbaccf-52ce-4efd-a942-62fa9f28de0b ==============================
[0m16:21:18.834755 [info ] [MainThread]: Running with dbt=1.9.3
[0m16:21:18.835475 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --models test2', 'send_anonymous_usage_stats': 'True'}
[0m16:21:18.920488 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:21:18.921033 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:21:18.921529 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:21:19.119568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fafbaccf-52ce-4efd-a942-62fa9f28de0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0f9cc350>]}
[0m16:21:19.179955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fafbaccf-52ce-4efd-a942-62fa9f28de0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0d654680>]}
[0m16:21:19.180819 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m16:21:19.277702 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m16:21:19.371059 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:21:19.371777 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/test2.sql
[0m16:21:19.611917 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m16:21:19.634830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fafbaccf-52ce-4efd-a942-62fa9f28de0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0c0a23c0>]}
[0m16:21:19.710396 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:21:19.713066 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:21:19.725066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fafbaccf-52ce-4efd-a942-62fa9f28de0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0d39c770>]}
[0m16:21:19.725789 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m16:21:19.726400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fafbaccf-52ce-4efd-a942-62fa9f28de0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0c08da00>]}
[0m16:21:19.728046 [info ] [MainThread]: 
[0m16:21:19.728568 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:21:19.729017 [info ] [MainThread]: 
[0m16:21:19.729608 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m16:21:19.730586 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m16:21:19.747782 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m16:21:19.748780 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:21:19.749586 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:21:19.840276 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:21:19.840886 [debug] [ThreadPool]: SQL status: OK in 0.091 seconds
[0m16:21:19.845540 [debug] [ThreadPool]: On list_schemas: Close
[0m16:21:19.858578 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m16:21:19.864081 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:21:19.864660 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m16:21:19.865109 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m16:21:19.865541 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:21:19.962912 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m16:21:19.963548 [debug] [ThreadPool]: SQL status: OK in 0.098 seconds
[0m16:21:19.967507 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m16:21:19.968073 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m16:21:19.968497 [debug] [ThreadPool]: On list_None_default: Close
[0m16:21:19.974121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fafbaccf-52ce-4efd-a942-62fa9f28de0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0c08da00>]}
[0m16:21:19.974747 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:21:19.975174 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:21:19.977352 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m16:21:19.978019 [info ] [Thread-1 (]: 1 of 1 START sql incremental model default.test2 ............................... [RUN]
[0m16:21:19.978706 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m16:21:19.979291 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m16:21:19.987765 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m16:21:19.988769 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m16:21:20.053983 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m16:21:20.054549 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:21:20.055041 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 3 AS tripled_value
FROM raw_data
  
[0m16:21:20.055504 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:21:20.146278 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:21:20.146928 [debug] [Thread-1 (]: SQL status: OK in 0.091 seconds
[0m16:21:20.166571 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:21:20.167127 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

      describe extended default.test2
  
[0m16:21:20.225409 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:21:20.226182 [debug] [Thread-1 (]: SQL status: OK in 0.059 seconds
[0m16:21:20.230503 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test2"
[0m16:21:20.231420 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m16:21:20.232014 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

    insert into table default.test2
    select `ID`, `NAME`, `TRIPLED_VALUE` from test2__dbt_tmp


[0m16:21:20.756513 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m16:21:20.757402 [debug] [Thread-1 (]: SQL status: OK in 0.525 seconds
[0m16:21:20.782904 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m16:21:20.783724 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m16:21:20.784438 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m16:21:20.832412 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fafbaccf-52ce-4efd-a942-62fa9f28de0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0c057e60>]}
[0m16:21:20.833688 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model default.test2 .......................... [[32mOK[0m in 0.85s]
[0m16:21:20.834886 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m16:21:20.836522 [debug] [MainThread]: On master: ROLLBACK
[0m16:21:20.836990 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:21:20.877621 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:21:20.878706 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:21:20.879726 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:21:20.880695 [debug] [MainThread]: On master: ROLLBACK
[0m16:21:20.881648 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m16:21:20.882461 [debug] [MainThread]: On master: Close
[0m16:21:20.887655 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:21:20.888445 [debug] [MainThread]: Connection 'model.dbt_spark_project.test2' was properly closed.
[0m16:21:20.889224 [info ] [MainThread]: 
[0m16:21:20.890090 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.16 seconds (1.16s).
[0m16:21:20.891720 [debug] [MainThread]: Command end result
[0m16:21:20.935413 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m16:21:20.938362 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m16:21:20.948227 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m16:21:20.949017 [info ] [MainThread]: 
[0m16:21:20.949901 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:21:20.950677 [info ] [MainThread]: 
[0m16:21:20.951525 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:21:20.953067 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.1757333, "process_in_blocks": "160", "process_kernel_time": 0.205409, "process_mem_max_rss": "112504", "process_out_blocks": "2912", "process_user_time": 2.525735}
[0m16:21:20.954012 [debug] [MainThread]: Command `dbt run` succeeded at 16:21:20.953834 after 2.18 seconds
[0m16:21:20.954771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0fd187d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0d5bf4a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x722b0de1af60>]}
[0m16:21:20.955563 [debug] [MainThread]: Flushing usage events
[0m16:21:22.276210 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:46:28.482684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b8397b00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b8199430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b83de7e0>]}


============================== 10:46:28.538362 | 66062a16-5a83-4e83-bb73-0454dfb5d69c ==============================
[0m10:46:28.538362 [info ] [MainThread]: Running with dbt=1.9.3
[0m10:46:28.539198 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:46:28.849192 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:46:28.850059 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:46:28.850692 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:46:29.104639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b7700920>]}
[0m10:46:29.179946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b7700500>]}
[0m10:46:29.180728 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m10:46:29.274051 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m10:46:29.417014 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m10:46:29.417686 [debug] [MainThread]: Partial parsing: updated file: dbt_spark_project://models/test2.sql
[0m10:46:29.699232 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m10:46:29.714365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b5fe7530>]}
[0m10:46:29.797401 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:46:29.801785 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:46:29.825975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b72e8380>]}
[0m10:46:29.827090 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m10:46:29.827645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b5fd1730>]}
[0m10:46:29.829903 [info ] [MainThread]: 
[0m10:46:29.830597 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:46:29.831183 [info ] [MainThread]: 
[0m10:46:29.832619 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m10:46:29.840746 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m10:46:29.852671 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m10:46:29.853397 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m10:46:29.853855 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:46:32.864644 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:46:32.865169 [debug] [ThreadPool]: SQL status: OK in 3.011 seconds
[0m10:46:32.923366 [debug] [ThreadPool]: On list_schemas: Close
[0m10:46:32.946914 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m10:46:32.952525 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:32.952973 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m10:46:32.953356 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m10:46:32.953719 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:46:33.424001 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m10:46:33.424630 [debug] [ThreadPool]: SQL status: OK in 0.471 seconds
[0m10:46:33.430471 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m10:46:33.430998 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m10:46:33.431415 [debug] [ThreadPool]: On list_None_default: Close
[0m10:46:33.441481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b70aeba0>]}
[0m10:46:33.442152 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:33.442652 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:46:33.445549 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m10:46:33.446211 [info ] [Thread-1 (]: 1 of 6 START sql incremental model default.test2 ............................... [RUN]
[0m10:46:33.446792 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m10:46:33.447273 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m10:46:33.455138 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m10:46:33.456956 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m10:46:33.505369 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:33.505940 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m10:46:33.506440 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 10 AS tripled_value
FROM raw_data
  
[0m10:46:33.506927 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:46:33.853983 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:33.854702 [debug] [Thread-1 (]: SQL status: OK in 0.348 seconds
[0m10:46:33.873930 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m10:46:33.874536 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

      describe extended default.test2
  
[0m10:46:34.009115 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:34.009917 [debug] [Thread-1 (]: SQL status: OK in 0.135 seconds
[0m10:46:34.015093 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test2"
[0m10:46:34.016699 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m10:46:34.017209 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

    insert into table default.test2
    select `ID`, `NAME`, `TRIPLED_VALUE` from test2__dbt_tmp


[0m10:46:37.034218 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:37.035153 [debug] [Thread-1 (]: SQL status: OK in 3.017 seconds
[0m10:46:37.061500 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m10:46:37.062147 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:46:37.062708 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m10:46:37.134752 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b7367680>]}
[0m10:46:37.135642 [info ] [Thread-1 (]: 1 of 6 OK created sql incremental model default.test2 .......................... [[32mOK[0m in 3.69s]
[0m10:46:37.136623 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m10:46:37.137412 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m10:46:37.138215 [info ] [Thread-1 (]: 2 of 6 START sql table model default.transformed_data .......................... [RUN]
[0m10:46:37.138838 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.test2, now model.dbt_spark_project.transformed_data)
[0m10:46:37.139363 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m10:46:37.142010 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m10:46:37.142725 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m10:46:37.159474 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m10:46:37.160044 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m10:46:37.160525 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:46:37.250167 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:37.250736 [debug] [Thread-1 (]: SQL status: OK in 0.090 seconds
[0m10:46:37.295994 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m10:46:37.296802 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:37.297270 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m10:46:37.297725 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m10:46:38.502648 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:38.503359 [debug] [Thread-1 (]: SQL status: OK in 1.205 seconds
[0m10:46:38.510703 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m10:46:38.511307 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:46:38.511853 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m10:46:38.519586 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b5f72d20>]}
[0m10:46:38.520473 [info ] [Thread-1 (]: 2 of 6 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.38s]
[0m10:46:38.521221 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m10:46:38.521778 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m10:46:38.522387 [info ] [Thread-1 (]: 3 of 6 START sql table model default.transformed_data2 ......................... [RUN]
[0m10:46:38.522982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m10:46:38.523494 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m10:46:38.526133 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m10:46:38.527067 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m10:46:38.532095 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m10:46:38.532823 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m10:46:38.533425 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:46:38.621218 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:38.621904 [debug] [Thread-1 (]: SQL status: OK in 0.088 seconds
[0m10:46:38.625964 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m10:46:38.626856 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:38.627490 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m10:46:38.628154 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m10:46:39.070444 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:39.071069 [debug] [Thread-1 (]: SQL status: OK in 0.442 seconds
[0m10:46:39.073552 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m10:46:39.074067 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:46:39.074552 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m10:46:39.081431 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b5eba9f0>]}
[0m10:46:39.082262 [info ] [Thread-1 (]: 3 of 6 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.56s]
[0m10:46:39.083142 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m10:46:39.084071 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m10:46:39.085151 [info ] [Thread-1 (]: 4 of 6 START sql table model default.transformed_data3 ......................... [RUN]
[0m10:46:39.085970 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m10:46:39.086671 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m10:46:39.089745 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m10:46:39.090578 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m10:46:39.096994 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m10:46:39.097610 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m10:46:39.098131 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:46:39.174637 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:39.175395 [debug] [Thread-1 (]: SQL status: OK in 0.077 seconds
[0m10:46:39.179348 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m10:46:39.180127 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:39.180645 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m10:46:39.181128 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m10:46:39.657442 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:39.658030 [debug] [Thread-1 (]: SQL status: OK in 0.476 seconds
[0m10:46:39.660842 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m10:46:39.661436 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:46:39.661914 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m10:46:39.669405 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b457cc50>]}
[0m10:46:39.670290 [info ] [Thread-1 (]: 4 of 6 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.58s]
[0m10:46:39.671068 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m10:46:39.671659 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m10:46:39.672315 [info ] [Thread-1 (]: 5 of 6 START sql table model default.transformed_data4 ......................... [RUN]
[0m10:46:39.673076 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m10:46:39.673687 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m10:46:39.676318 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m10:46:39.677039 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m10:46:39.680986 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m10:46:39.681580 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m10:46:39.682083 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:46:39.748752 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:39.749362 [debug] [Thread-1 (]: SQL status: OK in 0.067 seconds
[0m10:46:39.752868 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m10:46:39.753649 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:39.754148 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m10:46:39.754657 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m10:46:40.158324 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:40.158993 [debug] [Thread-1 (]: SQL status: OK in 0.404 seconds
[0m10:46:40.161431 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m10:46:40.161922 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:46:40.162393 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m10:46:40.169529 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b5f96360>]}
[0m10:46:40.170419 [info ] [Thread-1 (]: 5 of 6 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.50s]
[0m10:46:40.171378 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m10:46:40.172050 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m10:46:40.172751 [info ] [Thread-1 (]: 6 of 6 START sql table model default.transformed_data5 ......................... [RUN]
[0m10:46:40.173577 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m10:46:40.174275 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m10:46:40.177414 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m10:46:40.178331 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m10:46:40.183105 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m10:46:40.183687 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m10:46:40.184182 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:46:40.257879 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:40.258530 [debug] [Thread-1 (]: SQL status: OK in 0.074 seconds
[0m10:46:40.262429 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m10:46:40.263402 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:40.263932 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m10:46:40.264617 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m10:46:40.695357 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m10:46:40.696039 [debug] [Thread-1 (]: SQL status: OK in 0.431 seconds
[0m10:46:40.698677 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m10:46:40.699264 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m10:46:40.699778 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m10:46:40.708545 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '66062a16-5a83-4e83-bb73-0454dfb5d69c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b5f44fb0>]}
[0m10:46:40.709492 [info ] [Thread-1 (]: 6 of 6 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.53s]
[0m10:46:40.710365 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m10:46:40.712012 [debug] [MainThread]: On master: ROLLBACK
[0m10:46:40.712894 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:46:40.749406 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:46:40.750307 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:46:40.750978 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:46:40.751654 [debug] [MainThread]: On master: ROLLBACK
[0m10:46:40.752303 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m10:46:40.752937 [debug] [MainThread]: On master: Close
[0m10:46:40.758744 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:46:40.759570 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m10:46:40.760316 [info ] [MainThread]: 
[0m10:46:40.760995 [info ] [MainThread]: Finished running 1 incremental model, 5 table models in 0 hours 0 minutes and 10.93 seconds (10.93s).
[0m10:46:40.763340 [debug] [MainThread]: Command end result
[0m10:46:40.787105 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m10:46:40.788960 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m10:46:40.795018 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m10:46:40.795526 [info ] [MainThread]: 
[0m10:46:40.796115 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:46:40.796703 [info ] [MainThread]: 
[0m10:46:40.797335 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m10:46:40.798644 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.52232, "process_in_blocks": "71624", "process_kernel_time": 0.380038, "process_mem_max_rss": "112616", "process_out_blocks": "3024", "process_user_time": 3.454169}
[0m10:46:40.799400 [debug] [MainThread]: Command `dbt run` succeeded at 10:46:40.799199 after 12.52 seconds
[0m10:46:40.800204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b87441d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b8744f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7734b8206450>]}
[0m10:46:40.800936 [debug] [MainThread]: Flushing usage events
[0m10:46:42.797501 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:41:32.879787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61458bdf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6145dab9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61458bd7c0>]}


============================== 09:41:32.885387 | fd3204f0-3a57-42c8-8814-6dfd76c70e25 ==============================
[0m09:41:32.885387 [info ] [MainThread]: Running with dbt=1.9.3
[0m09:41:32.886454 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/minh/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m09:41:33.010587 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m09:41:33.011351 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m09:41:33.011961 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m09:41:33.248966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b614740c2c0>]}
[0m09:41:33.344336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61452882f0>]}
[0m09:41:33.345143 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m09:41:33.445485 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m09:41:33.588763 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:41:33.589193 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:41:33.595803 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m09:41:33.624680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6145d1e8a0>]}
[0m09:41:33.703343 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:41:33.707437 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:41:33.730381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6144f2f650>]}
[0m09:41:33.731002 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m09:41:33.731392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61450a9f40>]}
[0m09:41:33.733189 [info ] [MainThread]: 
[0m09:41:33.733585 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m09:41:33.733942 [info ] [MainThread]: 
[0m09:41:33.734472 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m09:41:33.740360 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m09:41:33.750876 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m09:41:33.751292 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m09:41:33.751607 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:41:37.022079 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:41:37.022572 [debug] [ThreadPool]: SQL status: OK in 3.271 seconds
[0m09:41:37.078999 [debug] [ThreadPool]: On list_schemas: Close
[0m09:41:37.102584 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m09:41:37.108888 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:37.109374 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m09:41:37.109730 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m09:41:37.110079 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:41:37.629745 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m09:41:37.630420 [debug] [ThreadPool]: SQL status: OK in 0.520 seconds
[0m09:41:37.637081 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m09:41:37.637622 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m09:41:37.638088 [debug] [ThreadPool]: On list_None_default: Close
[0m09:41:37.650674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b614506b0b0>]}
[0m09:41:37.651457 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:37.651928 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:41:37.656002 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m09:41:37.656758 [info ] [Thread-1 (]: 1 of 6 START sql incremental model default.test2 ............................... [RUN]
[0m09:41:37.657439 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m09:41:37.657999 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m09:41:37.666026 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m09:41:37.669174 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m09:41:37.723870 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:37.724470 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m09:41:37.725063 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 10 AS tripled_value
FROM raw_data
  
[0m09:41:37.725664 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:41:38.144374 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:38.145013 [debug] [Thread-1 (]: SQL status: OK in 0.419 seconds
[0m09:41:38.167726 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m09:41:38.169290 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

      describe extended default.test2
  
[0m09:41:38.481457 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:38.482501 [debug] [Thread-1 (]: SQL status: OK in 0.312 seconds
[0m09:41:38.491545 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test2"
[0m09:41:38.492987 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m09:41:38.493476 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

    insert into table default.test2
    select `ID`, `NAME`, `TRIPLED_VALUE` from test2__dbt_tmp


[0m09:41:42.090917 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:42.091789 [debug] [Thread-1 (]: SQL status: OK in 3.598 seconds
[0m09:41:42.121376 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m09:41:42.122385 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:41:42.123243 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m09:41:42.269020 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61473db200>]}
[0m09:41:42.271532 [info ] [Thread-1 (]: 1 of 6 OK created sql incremental model default.test2 .......................... [[32mOK[0m in 4.61s]
[0m09:41:42.273230 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m09:41:42.274414 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m09:41:42.275868 [info ] [Thread-1 (]: 2 of 6 START sql table model default.transformed_data .......................... [RUN]
[0m09:41:42.277061 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.test2, now model.dbt_spark_project.transformed_data)
[0m09:41:42.278164 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m09:41:42.283876 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m09:41:42.285130 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m09:41:42.328168 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:41:42.329197 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m09:41:42.330462 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:41:42.508603 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:42.509381 [debug] [Thread-1 (]: SQL status: OK in 0.179 seconds
[0m09:41:42.585540 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m09:41:42.587251 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:42.588125 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m09:41:42.588970 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m09:41:44.225466 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:44.226502 [debug] [Thread-1 (]: SQL status: OK in 1.637 seconds
[0m09:41:44.239607 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m09:41:44.240509 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:41:44.241358 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m09:41:44.254194 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b614437ce90>]}
[0m09:41:44.255811 [info ] [Thread-1 (]: 2 of 6 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.98s]
[0m09:41:44.257272 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m09:41:44.258308 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m09:41:44.259496 [info ] [Thread-1 (]: 3 of 6 START sql table model default.transformed_data2 ......................... [RUN]
[0m09:41:44.260469 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m09:41:44.261376 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m09:41:44.270862 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m09:41:44.272093 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m09:41:44.280347 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:41:44.281415 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m09:41:44.282241 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:41:44.439471 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:44.441775 [debug] [Thread-1 (]: SQL status: OK in 0.159 seconds
[0m09:41:44.449981 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m09:41:44.457582 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:44.459957 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m09:41:44.461112 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m09:41:45.176320 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:45.177763 [debug] [Thread-1 (]: SQL status: OK in 0.715 seconds
[0m09:41:45.184472 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m09:41:45.185629 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:41:45.186566 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m09:41:45.204069 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b614433f440>]}
[0m09:41:45.205388 [info ] [Thread-1 (]: 3 of 6 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.94s]
[0m09:41:45.206514 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m09:41:45.207325 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m09:41:45.208432 [info ] [Thread-1 (]: 4 of 6 START sql table model default.transformed_data3 ......................... [RUN]
[0m09:41:45.209528 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m09:41:45.210431 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m09:41:45.215087 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m09:41:45.216533 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m09:41:45.226375 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:41:45.227401 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m09:41:45.228298 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:41:45.347197 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:45.348199 [debug] [Thread-1 (]: SQL status: OK in 0.120 seconds
[0m09:41:45.354399 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m09:41:45.355616 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:45.356268 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m09:41:45.356862 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m09:41:45.977113 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:45.978136 [debug] [Thread-1 (]: SQL status: OK in 0.621 seconds
[0m09:41:45.981668 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m09:41:45.982333 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:41:45.982956 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m09:41:45.996472 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61443005c0>]}
[0m09:41:45.997511 [info ] [Thread-1 (]: 4 of 6 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.79s]
[0m09:41:45.998426 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m09:41:45.999102 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m09:41:45.999998 [info ] [Thread-1 (]: 5 of 6 START sql table model default.transformed_data4 ......................... [RUN]
[0m09:41:46.000762 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m09:41:46.001747 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m09:41:46.005379 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m09:41:46.006313 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m09:41:46.011539 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:41:46.012378 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m09:41:46.012942 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:41:46.127714 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:46.128906 [debug] [Thread-1 (]: SQL status: OK in 0.116 seconds
[0m09:41:46.135344 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m09:41:46.136700 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:46.137606 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m09:41:46.138492 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m09:41:46.677164 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:46.678035 [debug] [Thread-1 (]: SQL status: OK in 0.539 seconds
[0m09:41:46.681805 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m09:41:46.682647 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:41:46.683401 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m09:41:46.693912 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6144356630>]}
[0m09:41:46.694976 [info ] [Thread-1 (]: 5 of 6 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.69s]
[0m09:41:46.696499 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m09:41:46.697380 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m09:41:46.698457 [info ] [Thread-1 (]: 6 of 6 START sql table model default.transformed_data5 ......................... [RUN]
[0m09:41:46.699331 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m09:41:46.699923 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m09:41:46.703052 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m09:41:46.703846 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m09:41:46.710314 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:41:46.710871 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m09:41:46.711306 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:41:46.777931 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:46.778458 [debug] [Thread-1 (]: SQL status: OK in 0.067 seconds
[0m09:41:46.781636 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m09:41:46.782530 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:46.783231 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m09:41:46.783789 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m09:41:47.290482 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m09:41:47.291081 [debug] [Thread-1 (]: SQL status: OK in 0.507 seconds
[0m09:41:47.293316 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m09:41:47.293786 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m09:41:47.294241 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m09:41:47.305575 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fd3204f0-3a57-42c8-8814-6dfd76c70e25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61443aa900>]}
[0m09:41:47.306446 [info ] [Thread-1 (]: 6 of 6 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.61s]
[0m09:41:47.307384 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m09:41:47.308814 [debug] [MainThread]: On master: ROLLBACK
[0m09:41:47.309303 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:41:47.362689 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:41:47.363584 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m09:41:47.364119 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m09:41:47.364590 [debug] [MainThread]: On master: ROLLBACK
[0m09:41:47.365056 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m09:41:47.365617 [debug] [MainThread]: On master: Close
[0m09:41:47.373442 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:41:47.373943 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m09:41:47.374409 [info ] [MainThread]: 
[0m09:41:47.374834 [info ] [MainThread]: Finished running 1 incremental model, 5 table models in 0 hours 0 minutes and 13.64 seconds (13.64s).
[0m09:41:47.376348 [debug] [MainThread]: Command end result
[0m09:41:47.401120 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m09:41:47.404123 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m09:41:47.410659 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m09:41:47.411262 [info ] [MainThread]: 
[0m09:41:47.411919 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:41:47.412597 [info ] [MainThread]: 
[0m09:41:47.413450 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m09:41:47.414949 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.629599, "process_in_blocks": "69880", "process_kernel_time": 0.345421, "process_mem_max_rss": "109416", "process_out_blocks": "2088", "process_user_time": 3.407022}
[0m09:41:47.415615 [debug] [MainThread]: Command `dbt run` succeeded at 09:41:47.415467 after 14.63 seconds
[0m09:41:47.416979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61462c8230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b61462c8860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7b6145c021e0>]}
[0m09:41:47.418489 [debug] [MainThread]: Flushing usage events
[0m09:41:48.951091 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:12:13.294023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde181d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde1c649e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde1a1edb0>]}


============================== 15:12:13.299149 | 5b0eeb8f-7917-4d01-98d4-b47b67fd8591 ==============================
[0m15:12:13.299149 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:12:13.299763 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/home/minh/.dbt', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m15:12:13.413506 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:12:13.414104 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:12:13.414585 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:12:13.573340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde1ff00e0>]}
[0m15:12:13.634428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde1a1dd30>]}
[0m15:12:13.635161 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m15:12:13.713102 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:12:14.005295 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:12:14.005721 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:12:14.012735 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m15:12:14.044802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde1eb3860>]}
[0m15:12:14.123762 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:12:14.128228 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:12:14.148094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde28763f0>]}
[0m15:12:14.148636 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m15:12:14.149036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde0bf2bd0>]}
[0m15:12:14.151319 [info ] [MainThread]: 
[0m15:12:14.151963 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:12:14.152414 [info ] [MainThread]: 
[0m15:12:14.153057 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:12:14.159872 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:12:14.174176 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:12:14.174652 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:12:14.175023 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:12:17.067575 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:12:17.068338 [debug] [ThreadPool]: SQL status: OK in 2.893 seconds
[0m15:12:17.132031 [debug] [ThreadPool]: On list_schemas: Close
[0m15:12:17.167472 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m15:12:17.173968 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:17.174401 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m15:12:17.174780 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m15:12:17.175120 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:12:17.776338 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:12:17.777261 [debug] [ThreadPool]: SQL status: OK in 0.602 seconds
[0m15:12:17.784794 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m15:12:17.785289 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:12:17.785658 [debug] [ThreadPool]: On list_None_default: Close
[0m15:12:17.798316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde0a14110>]}
[0m15:12:17.798979 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:17.799414 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:12:17.803445 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m15:12:17.804171 [info ] [Thread-1 (]: 1 of 6 START sql incremental model default.test2 ............................... [RUN]
[0m15:12:17.804813 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m15:12:17.805361 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m15:12:17.814454 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m15:12:17.815939 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m15:12:17.871101 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:17.871621 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m15:12:17.872043 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 10 AS tripled_value
FROM raw_data
  
[0m15:12:17.872455 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:12:18.219901 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:18.220473 [debug] [Thread-1 (]: SQL status: OK in 0.348 seconds
[0m15:12:18.237724 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m15:12:18.238225 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

      describe extended default.test2
  
[0m15:12:18.360043 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:18.360700 [debug] [Thread-1 (]: SQL status: OK in 0.122 seconds
[0m15:12:18.365770 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test2"
[0m15:12:18.366719 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m15:12:18.367145 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

    insert into table default.test2
    select `ID`, `NAME`, `TRIPLED_VALUE` from test2__dbt_tmp


[0m15:12:20.846987 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:20.847714 [debug] [Thread-1 (]: SQL status: OK in 2.480 seconds
[0m15:12:20.863766 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m15:12:20.864269 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:12:20.864671 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m15:12:20.939401 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde09b8b00>]}
[0m15:12:20.940291 [info ] [Thread-1 (]: 1 of 6 OK created sql incremental model default.test2 .......................... [[32mOK[0m in 3.13s]
[0m15:12:20.941109 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m15:12:20.941733 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m15:12:20.942624 [info ] [Thread-1 (]: 2 of 6 START sql table model default.transformed_data .......................... [RUN]
[0m15:12:20.943574 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.test2, now model.dbt_spark_project.transformed_data)
[0m15:12:20.944193 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m15:12:20.948400 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m15:12:20.950793 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m15:12:20.972589 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:12:20.973135 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m15:12:20.973559 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:12:21.540006 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:21.540608 [debug] [Thread-1 (]: SQL status: OK in 0.567 seconds
[0m15:12:21.576211 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m15:12:21.576926 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:21.577388 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:12:21.577803 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m15:12:22.706448 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:22.707227 [debug] [Thread-1 (]: SQL status: OK in 1.129 seconds
[0m15:12:22.721876 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m15:12:22.722585 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:12:22.723179 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m15:12:22.730777 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79ddde85bd70>]}
[0m15:12:22.731846 [info ] [Thread-1 (]: 2 of 6 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.79s]
[0m15:12:22.732801 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m15:12:22.733515 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m15:12:22.734764 [info ] [Thread-1 (]: 3 of 6 START sql table model default.transformed_data2 ......................... [RUN]
[0m15:12:22.736006 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m15:12:22.736838 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m15:12:22.741495 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m15:12:22.742206 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m15:12:22.746104 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:12:22.746612 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m15:12:22.747033 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:12:22.916881 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:22.917459 [debug] [Thread-1 (]: SQL status: OK in 0.170 seconds
[0m15:12:22.920559 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m15:12:22.921313 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:22.921755 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:12:22.922203 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m15:12:23.423298 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:23.423878 [debug] [Thread-1 (]: SQL status: OK in 0.501 seconds
[0m15:12:23.426265 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m15:12:23.426722 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:12:23.427136 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m15:12:23.434209 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79ddde835d30>]}
[0m15:12:23.434981 [info ] [Thread-1 (]: 3 of 6 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.70s]
[0m15:12:23.435918 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m15:12:23.436584 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m15:12:23.437553 [info ] [Thread-1 (]: 4 of 6 START sql table model default.transformed_data3 ......................... [RUN]
[0m15:12:23.438417 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m15:12:23.438984 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m15:12:23.442492 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m15:12:23.443235 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m15:12:23.447553 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:12:23.448067 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m15:12:23.448511 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:12:23.606690 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:23.607274 [debug] [Thread-1 (]: SQL status: OK in 0.159 seconds
[0m15:12:23.610233 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m15:12:23.610953 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:23.611386 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:12:23.611817 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m15:12:23.982940 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:23.983746 [debug] [Thread-1 (]: SQL status: OK in 0.371 seconds
[0m15:12:23.986118 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m15:12:23.986602 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:12:23.987018 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m15:12:23.994454 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79ddde7fa060>]}
[0m15:12:23.995650 [info ] [Thread-1 (]: 4 of 6 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.56s]
[0m15:12:23.996774 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m15:12:23.997819 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m15:12:23.999070 [info ] [Thread-1 (]: 5 of 6 START sql table model default.transformed_data4 ......................... [RUN]
[0m15:12:24.000191 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m15:12:24.001099 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m15:12:24.005683 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m15:12:24.006768 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m15:12:24.013300 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:12:24.014040 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m15:12:24.014676 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:12:24.156133 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:24.156658 [debug] [Thread-1 (]: SQL status: OK in 0.142 seconds
[0m15:12:24.159684 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m15:12:24.160385 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:24.160806 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:12:24.161240 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m15:12:24.586170 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:24.586818 [debug] [Thread-1 (]: SQL status: OK in 0.425 seconds
[0m15:12:24.589675 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m15:12:24.590314 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:12:24.590822 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m15:12:24.599263 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde097eb10>]}
[0m15:12:24.600220 [info ] [Thread-1 (]: 5 of 6 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.60s]
[0m15:12:24.601062 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m15:12:24.601756 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m15:12:24.602683 [info ] [Thread-1 (]: 6 of 6 START sql table model default.transformed_data5 ......................... [RUN]
[0m15:12:24.603616 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m15:12:24.604584 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m15:12:24.608392 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m15:12:24.609273 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m15:12:24.616252 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:12:24.616849 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m15:12:24.617351 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:12:24.773992 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:24.774545 [debug] [Thread-1 (]: SQL status: OK in 0.157 seconds
[0m15:12:24.777352 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m15:12:24.778037 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:24.778466 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:12:24.778886 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m15:12:25.102025 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:12:25.102610 [debug] [Thread-1 (]: SQL status: OK in 0.323 seconds
[0m15:12:25.104878 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m15:12:25.105324 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:12:25.105719 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m15:12:25.111827 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b0eeb8f-7917-4d01-98d4-b47b67fd8591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79ddde891d00>]}
[0m15:12:25.112630 [info ] [Thread-1 (]: 6 of 6 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.51s]
[0m15:12:25.113323 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m15:12:25.114864 [debug] [MainThread]: On master: ROLLBACK
[0m15:12:25.115475 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:12:25.152623 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:12:25.153176 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:12:25.153563 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:12:25.153935 [debug] [MainThread]: On master: ROLLBACK
[0m15:12:25.154328 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:12:25.154704 [debug] [MainThread]: On master: Close
[0m15:12:25.160292 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:12:25.160850 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m15:12:25.161306 [info ] [MainThread]: 
[0m15:12:25.161727 [info ] [MainThread]: Finished running 1 incremental model, 5 table models in 0 hours 0 minutes and 11.01 seconds (11.01s).
[0m15:12:25.163306 [debug] [MainThread]: Command end result
[0m15:12:25.184250 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:12:25.185950 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:12:25.191952 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m15:12:25.192409 [info ] [MainThread]: 
[0m15:12:25.192939 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:12:25.193574 [info ] [MainThread]: 
[0m15:12:25.194086 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m15:12:25.195132 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.956549, "process_in_blocks": "67864", "process_kernel_time": 0.265192, "process_mem_max_rss": "109404", "process_out_blocks": "2096", "process_user_time": 2.343034}
[0m15:12:25.195775 [debug] [MainThread]: Command `dbt run` succeeded at 15:12:25.195640 after 11.96 seconds
[0m15:12:25.196217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde181d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde11250a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x79dde097c260>]}
[0m15:12:25.196672 [debug] [MainThread]: Flushing usage events
[0m15:12:27.143622 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:13:10.891100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7058114ba900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7058110ace30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70581193a690>]}


============================== 15:13:10.894223 | 543d49d6-7fb6-4a29-b6cf-d3486c01e29d ==============================
[0m15:13:10.894223 [info ] [MainThread]: Running with dbt=1.9.3
[0m15:13:10.894958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/home/minh/Desktop/dbt-spark/dbt_spark_project/logs', 'profiles_dir': '/home/minh/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:13:10.947819 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m15:13:10.948320 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m15:13:10.948777 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m15:13:11.093292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7058116706e0>]}
[0m15:13:11.156853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7058107cd520>]}
[0m15:13:11.157557 [info ] [MainThread]: Registered adapter: spark=1.9.2
[0m15:13:11.226839 [debug] [MainThread]: checksum: 5671c00892cdbcef29c3b00cc5ba7510c3d18a35c883caeb9ab9ee266dc29c8f, vars: {}, profile: , target: , version: 1.9.3
[0m15:13:11.323153 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:13:11.323647 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:13:11.330797 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_spark_project.example
[0m15:13:11.359710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70581029ec90>]}
[0m15:13:11.431828 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:13:11.434287 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:13:11.445299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70580ff7bfb0>]}
[0m15:13:11.445934 [info ] [MainThread]: Found 6 models, 1 source, 473 macros
[0m15:13:11.446336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70581041b560>]}
[0m15:13:11.448199 [info ] [MainThread]: 
[0m15:13:11.448665 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:13:11.449065 [info ] [MainThread]: 
[0m15:13:11.449649 [debug] [MainThread]: Acquiring new spark connection 'master'
[0m15:13:11.455703 [debug] [ThreadPool]: Acquiring new spark connection 'list_schemas'
[0m15:13:11.467130 [debug] [ThreadPool]: Using spark connection "list_schemas"
[0m15:13:11.467589 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:13:11.467945 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:13:13.940072 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:13.940625 [debug] [ThreadPool]: SQL status: OK in 2.473 seconds
[0m15:13:14.037331 [debug] [ThreadPool]: On list_schemas: Close
[0m15:13:14.066096 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now list_None_default)
[0m15:13:14.072898 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:14.073439 [debug] [ThreadPool]: Using spark connection "list_None_default"
[0m15:13:14.073901 [debug] [ThreadPool]: On list_None_default: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "connection_name": "list_None_default"} */
show table extended in default like '*'
  
[0m15:13:14.074315 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:13:14.755021 [debug] [ThreadPool]: Spark adapter: Poll status: 2, query complete
[0m15:13:14.755581 [debug] [ThreadPool]: SQL status: OK in 0.681 seconds
[0m15:13:14.763082 [debug] [ThreadPool]: On list_None_default: ROLLBACK
[0m15:13:14.763642 [debug] [ThreadPool]: Spark adapter: NotImplemented: rollback
[0m15:13:14.764064 [debug] [ThreadPool]: On list_None_default: Close
[0m15:13:14.775997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70581261cf20>]}
[0m15:13:14.776683 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:14.777377 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:14.779887 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.test2
[0m15:13:14.780614 [info ] [Thread-1 (]: 1 of 6 START sql incremental model default.test2 ............................... [RUN]
[0m15:13:14.781319 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_None_default, now model.dbt_spark_project.test2)
[0m15:13:14.782280 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.test2
[0m15:13:14.790136 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.test2"
[0m15:13:14.790900 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.test2
[0m15:13:14.842521 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:14.843172 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m15:13:14.843870 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

  
    create or replace temporary view test2__dbt_tmp as
      

SELECT 
    id,
    name,
    value * 10 AS tripled_value
FROM raw_data
  
[0m15:13:14.844393 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:15.157232 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:15.158101 [debug] [Thread-1 (]: SQL status: OK in 0.314 seconds
[0m15:13:15.185521 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m15:13:15.186266 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

      describe extended default.test2
  
[0m15:13:15.319891 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:15.320858 [debug] [Thread-1 (]: SQL status: OK in 0.134 seconds
[0m15:13:15.326787 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.test2"
[0m15:13:15.327902 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.test2"
[0m15:13:15.328682 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.test2"} */

    insert into table default.test2
    select `ID`, `NAME`, `TRIPLED_VALUE` from test2__dbt_tmp


[0m15:13:18.005088 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:18.005692 [debug] [Thread-1 (]: SQL status: OK in 2.676 seconds
[0m15:13:18.022441 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: ROLLBACK
[0m15:13:18.023004 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:18.023468 [debug] [Thread-1 (]: On model.dbt_spark_project.test2: Close
[0m15:13:18.093572 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7058127771d0>]}
[0m15:13:18.094569 [info ] [Thread-1 (]: 1 of 6 OK created sql incremental model default.test2 .......................... [[32mOK[0m in 3.31s]
[0m15:13:18.095441 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.test2
[0m15:13:18.095987 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data
[0m15:13:18.096652 [info ] [Thread-1 (]: 2 of 6 START sql table model default.transformed_data .......................... [RUN]
[0m15:13:18.097189 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.test2, now model.dbt_spark_project.transformed_data)
[0m15:13:18.097664 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data
[0m15:13:18.100247 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data"
[0m15:13:18.100920 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data
[0m15:13:18.118460 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:13:18.118959 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */
drop table if exists default.transformed_data
[0m15:13:18.119365 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:18.692718 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:18.693415 [debug] [Thread-1 (]: SQL status: OK in 0.574 seconds
[0m15:13:18.731500 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data"
[0m15:13:18.732262 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:18.732733 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data"
[0m15:13:18.733220 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data"} */

  
    
        create table default.transformed_data
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 2 as doubled_value
FROM raw_data
  
[0m15:13:19.643101 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:19.644072 [debug] [Thread-1 (]: SQL status: OK in 0.910 seconds
[0m15:13:19.659203 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: ROLLBACK
[0m15:13:19.660054 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:19.660712 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data: Close
[0m15:13:19.667638 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70580dec0170>]}
[0m15:13:19.668831 [info ] [Thread-1 (]: 2 of 6 OK created sql table model default.transformed_data ..................... [[32mOK[0m in 1.57s]
[0m15:13:19.669824 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data
[0m15:13:19.670532 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data2
[0m15:13:19.671291 [info ] [Thread-1 (]: 3 of 6 START sql table model default.transformed_data2 ......................... [RUN]
[0m15:13:19.672118 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data, now model.dbt_spark_project.transformed_data2)
[0m15:13:19.672799 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data2
[0m15:13:19.676569 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data2"
[0m15:13:19.677315 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data2
[0m15:13:19.681045 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:13:19.681465 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */
drop table if exists default.transformed_data2
[0m15:13:19.681835 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:19.853532 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:19.854074 [debug] [Thread-1 (]: SQL status: OK in 0.172 seconds
[0m15:13:19.857153 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data2"
[0m15:13:19.857982 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:19.858439 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data2"
[0m15:13:19.859162 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data2"} */

  
    
        create table default.transformed_data2
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 3 as tripled_value
FROM raw_data
  
[0m15:13:20.345512 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:20.346093 [debug] [Thread-1 (]: SQL status: OK in 0.486 seconds
[0m15:13:20.348739 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: ROLLBACK
[0m15:13:20.349218 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:20.349663 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data2: Close
[0m15:13:20.356507 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70580debed80>]}
[0m15:13:20.357806 [info ] [Thread-1 (]: 3 of 6 OK created sql table model default.transformed_data2 .................... [[32mOK[0m in 0.68s]
[0m15:13:20.358910 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data2
[0m15:13:20.359724 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data3
[0m15:13:20.360756 [info ] [Thread-1 (]: 4 of 6 START sql table model default.transformed_data3 ......................... [RUN]
[0m15:13:20.361629 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data2, now model.dbt_spark_project.transformed_data3)
[0m15:13:20.362356 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data3
[0m15:13:20.366195 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data3"
[0m15:13:20.367213 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data3
[0m15:13:20.373525 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:13:20.374270 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */
drop table if exists default.transformed_data3
[0m15:13:20.374923 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:20.537532 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:20.538176 [debug] [Thread-1 (]: SQL status: OK in 0.163 seconds
[0m15:13:20.542217 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data3"
[0m15:13:20.543634 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:20.544500 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data3"
[0m15:13:20.545218 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data3"} */

  
    
        create table default.transformed_data3
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 4 as quadrupled_value
FROM raw_data
  
[0m15:13:20.994412 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:20.995401 [debug] [Thread-1 (]: SQL status: OK in 0.449 seconds
[0m15:13:20.999266 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: ROLLBACK
[0m15:13:20.999996 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:21.000687 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data3: Close
[0m15:13:21.009251 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7058100404d0>]}
[0m15:13:21.010574 [info ] [Thread-1 (]: 4 of 6 OK created sql table model default.transformed_data3 .................... [[32mOK[0m in 0.65s]
[0m15:13:21.011470 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data3
[0m15:13:21.012080 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data4
[0m15:13:21.012838 [info ] [Thread-1 (]: 5 of 6 START sql table model default.transformed_data4 ......................... [RUN]
[0m15:13:21.013385 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data3, now model.dbt_spark_project.transformed_data4)
[0m15:13:21.013897 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data4
[0m15:13:21.016694 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data4"
[0m15:13:21.017448 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data4
[0m15:13:21.022107 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:13:21.022653 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */
drop table if exists default.transformed_data4
[0m15:13:21.023093 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:21.182632 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:21.183245 [debug] [Thread-1 (]: SQL status: OK in 0.160 seconds
[0m15:13:21.186930 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data4"
[0m15:13:21.187809 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:21.188304 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data4"
[0m15:13:21.188827 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data4"} */

  
    
        create table default.transformed_data4
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 5 as quintupled_value
FROM raw_data
  
[0m15:13:21.582991 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:21.583707 [debug] [Thread-1 (]: SQL status: OK in 0.394 seconds
[0m15:13:21.586768 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: ROLLBACK
[0m15:13:21.587430 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:21.588011 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data4: Close
[0m15:13:21.596065 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705810049310>]}
[0m15:13:21.596980 [info ] [Thread-1 (]: 5 of 6 OK created sql table model default.transformed_data4 .................... [[32mOK[0m in 0.58s]
[0m15:13:21.597919 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data4
[0m15:13:21.598511 [debug] [Thread-1 (]: Began running node model.dbt_spark_project.transformed_data5
[0m15:13:21.599249 [info ] [Thread-1 (]: 6 of 6 START sql table model default.transformed_data5 ......................... [RUN]
[0m15:13:21.599901 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_spark_project.transformed_data4, now model.dbt_spark_project.transformed_data5)
[0m15:13:21.600407 [debug] [Thread-1 (]: Began compiling node model.dbt_spark_project.transformed_data5
[0m15:13:21.603198 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_spark_project.transformed_data5"
[0m15:13:21.603973 [debug] [Thread-1 (]: Began executing node model.dbt_spark_project.transformed_data5
[0m15:13:21.611444 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:13:21.612307 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */
drop table if exists default.transformed_data5
[0m15:13:21.613077 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:13:21.773753 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:21.774339 [debug] [Thread-1 (]: SQL status: OK in 0.161 seconds
[0m15:13:21.778115 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_spark_project.transformed_data5"
[0m15:13:21.779012 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:21.779454 [debug] [Thread-1 (]: Using spark connection "model.dbt_spark_project.transformed_data5"
[0m15:13:21.779903 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: /* {"app": "dbt", "dbt_version": "1.9.3", "profile_name": "dbt_spark_project", "target_name": "dev", "node_id": "model.dbt_spark_project.transformed_data5"} */

  
    
        create table default.transformed_data5
      
      
      
      
      
      
      
      

      as
      

SELECT 
    id, 
    name, 
    value * 6 as sextupled_value
FROM raw_data
  
[0m15:13:22.216372 [debug] [Thread-1 (]: Spark adapter: Poll status: 2, query complete
[0m15:13:22.217069 [debug] [Thread-1 (]: SQL status: OK in 0.437 seconds
[0m15:13:22.220596 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: ROLLBACK
[0m15:13:22.221150 [debug] [Thread-1 (]: Spark adapter: NotImplemented: rollback
[0m15:13:22.221693 [debug] [Thread-1 (]: On model.dbt_spark_project.transformed_data5: Close
[0m15:13:22.229923 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '543d49d6-7fb6-4a29-b6cf-d3486c01e29d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705810049130>]}
[0m15:13:22.230755 [info ] [Thread-1 (]: 6 of 6 OK created sql table model default.transformed_data5 .................... [[32mOK[0m in 0.63s]
[0m15:13:22.231489 [debug] [Thread-1 (]: Finished running node model.dbt_spark_project.transformed_data5
[0m15:13:22.232763 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:22.233226 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:13:22.272285 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:22.272854 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:13:22.273252 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:13:22.273648 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:22.274034 [debug] [MainThread]: Spark adapter: NotImplemented: rollback
[0m15:13:22.274404 [debug] [MainThread]: On master: Close
[0m15:13:22.281517 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:13:22.281992 [debug] [MainThread]: Connection 'model.dbt_spark_project.transformed_data5' was properly closed.
[0m15:13:22.282474 [info ] [MainThread]: 
[0m15:13:22.282923 [info ] [MainThread]: Finished running 1 incremental model, 5 table models in 0 hours 0 minutes and 10.83 seconds (10.83s).
[0m15:13:22.284604 [debug] [MainThread]: Command end result
[0m15:13:22.306579 [debug] [MainThread]: Wrote artifact WritableManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/manifest.json
[0m15:13:22.310634 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/semantic_manifest.json
[0m15:13:22.317431 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/minh/Desktop/dbt-spark/dbt_spark_project/target/run_results.json
[0m15:13:22.317908 [info ] [MainThread]: 
[0m15:13:22.318400 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:13:22.318911 [info ] [MainThread]: 
[0m15:13:22.319399 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m15:13:22.320694 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.483792, "process_in_blocks": "8", "process_kernel_time": 0.168696, "process_mem_max_rss": "109600", "process_out_blocks": "2104", "process_user_time": 2.283886}
[0m15:13:22.321597 [debug] [MainThread]: Command `dbt run` succeeded at 15:13:22.321368 after 11.48 seconds
[0m15:13:22.322199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x705810bae6c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70580dee0710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x70581164ca70>]}
[0m15:13:22.322775 [debug] [MainThread]: Flushing usage events
[0m15:13:23.976687 [debug] [MainThread]: An error was encountered while trying to flush usage events
